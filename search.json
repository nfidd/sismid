[
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "We’ve now developed a good idea of how epidemiological time delays affect our understanding of an evolving outbreak. So far, we’ve been working with individual line-list data. However, we usually want to model how an outbreak evolves through a whole population. The aim of this session is to introduce how delay distributions can be used to model population-level data generating process during an epidemic.\nThis means working with aggregated count data. This creates some new issues in correctly accounting for uncertainty. We handle population-level data using convolutions as a way of combining count data with a distribution of individual probabilities. We will have to adjust our continuous probability distributions to work with this using discretisation. We’ll then need to re-introduce additional uncertainty to account for the observation process at a population level.\n\n\n\nDelay distributions at the population level\n\n\n\n\nIn this session, we’ll focus on the delay from infection to symptom onset at a population level. First, we will introduce the techniques of convolution and discretisation. We’ll apply these to an aggregated time series of infections in order to simulate observed symptom onsets. Then, we’ll use those simulated symptom onsets to try and reconstruct a time series of infections.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, the ggplot2 library for plotting and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#slides",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#slides",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "Delay distributions at the population level",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#objectives",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#objectives",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "In this session, we’ll focus on the delay from infection to symptom onset at a population level. First, we will introduce the techniques of convolution and discretisation. We’ll apply these to an aggregated time series of infections in order to simulate observed symptom onsets. Then, we’ll use those simulated symptom onsets to try and reconstruct a time series of infections.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, the ggplot2 library for plotting and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#source-file",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#source-file",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "The source file of this session is located at sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.qmd.",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#libraries-used",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#libraries-used",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, the ggplot2 library for plotting and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#initialisation",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#initialisation",
    "title": "Using delay distributions to model the data generating process",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#delay-distributions-and-convolutions",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#delay-distributions-and-convolutions",
    "title": "Using delay distributions to model the data generating process",
    "section": "Delay distributions and convolutions",
    "text": "Delay distributions and convolutions\nIn the last session we simulated individual outcomes from a delay distribution, and then re-estimated the corresponding parameters. However, sometimes we do not have data on these individual-level outcomes, either because they are not recorded or because they cannot be shared, for example due to privacy concerns. At the population level, individual-level delays translate into convolutions.\nIf we have a time series of infections \\(I_t\\) (\\(t=1, 2, 3, \\ldots, t_\\mathrm{max}\\)), where \\(t\\) denotes the day on which the infections occur, and observable outcomes occur with a delay given by a delay distribution \\(p_i\\) (\\(i=0, 1, 2, \\dots, p_\\mathrm{max}\\)), where \\(i\\) is the number of days after infection that the observation happens, then the number of observable outcomes \\(C_t\\) on day \\(t\\) is given by\n\\[\nC_t = \\sum_{i=0}^{i=p_\\mathrm{max}} I_{t-i} p_i\n\\]\nIn other words, the number of observable outcomes on day \\(t\\) is given by the sum of infections on all previous days multiplied by the probability that those infections are observed on day \\(t\\). For example, the observable outcomes \\(C_t\\) could be the number of symptom onsets on day \\(t\\) and \\(p_i\\) is the incubation period.\nWe can use the same data as in the session on biases in delay distributions, but this time we first aggregate this into a daily time series of infections. You can do this using:\n\ninf_ts &lt;- make_daily_infections(infection_times)\n\n\n\n\n\n\n\nNote\n\n\n\nAs before you can look at the code of the function by typing make_daily_infections. The second part of this function is used to add days without infections with a zero count. This will make our calculations easier later (as otherwise we would have to try and detect these in any models that used this data which could be complicated).\n\n\nAnd look at the first few rows of the daily aggregated data:\n\nhead(inf_ts)\n\n# A tibble: 6 × 2\n  infection_day infections\n          &lt;dbl&gt;      &lt;int&gt;\n1             0          1\n2             1          0\n3             2          1\n4             3          0\n5             4          2\n6             5          1\n\n\nNow we can convolve the time series with a delay distribution to get a time series of outcomes as suggested above.\n\nDiscretising a delay distribution\nIn our first session, we decided to assume the delay from infection to symptom onset had a gamma distribution. However, if we want to use the gamma distribution with shape 5 and rate 1 as before, we face a familiar issue. The gamma distribution is a continuous distribution, but now our delay data are in days which are discrete entities. We will assume that both events that make up the delay (here infection and symptom onset) are observed as daily counts (e.g. as number of infections/symptom onsets by calendar date). Therefore, both observations are censored (as events are rounded to the nearest date). This means that our distribution is double interval censored which we encountered in the the biases in delay distribution session, so we need to use the same ideas introduced in that session.\n\n\n\n\n\n\nMathematical Definition (optional): Discretising a delay distribution subject to double interval censoring\n\n\n\n\n\nThe cumulative distribution function (CDF) (\\(F(t)\\)) of a distribution that has a daily censored primary event can be expressed as,\n\\[\nF^*(t) = \\int_0^1 F(t - u) du\n\\]\nIn effect, this is saying that the daily censored CDF is the average of the continuous distributions CDF over all possible event times (here between 0 and 1).\nThe probability mass function (PMF) of this distribution when observed as a daily process (i.e. the secondary event is also daily censored) is then\n\\[\nf_t \\propto F^*(t + 1) - F^*(t - 1)\n\\]\nThe important point is that the ultimately observed PMF is a combination of a primary event daily censoring process and a secondary event daily censoring process.\n\n\n\nWe can think about this via simulation. We do so by generating many replicates of the corresponding random delay, taking into account that we have already rounded down our infection times to infection days. This means that discretising a delay in this context is double censoring as we discussed in the the biases in delay distribution session. In the absence of any other information or model, we assume for our simulation that infection occurred at some random time during the day, with each time equally likely. We can then apply the incubation period using a continuous probability distribution, before once again rounding down to get the day of symptom onset (mimicking daily reporting). We repeat this many times to get the probability mass function that allows us to go from infection days to symptom onset days.\nYou can view the function we have written do this using:\n\ncensored_delay_pmf\n\nfunction (rgen, max, n = 1e+06, ...) \n{\n    first &lt;- runif(n, min = 0, max = 1)\n    second &lt;- first + rgen(n, ...)\n    delay &lt;- floor(second)\n    counts &lt;- table(factor(delay, levels = seq(0, max)))\n    pmf &lt;- counts/sum(counts)\n    return(pmf)\n}\n&lt;bytecode: 0x55ba9e98d550&gt;\n&lt;environment: namespace:nfidd&gt;\n\n\n\n\n\n\n\n\nTake 3 minutes\n\n\n\nTry to understand the censored_delay_pmf() function above. Try it with a few different probability distributions and parameters, e.g. for the parameters given above and a maximum delay of 2 weeks (14 days) it would be:\n\ngamma_pmf &lt;- censored_delay_pmf(rgamma, max = 14, shape = 5, rate = 1)\ngamma_pmf\n\n\n           0            1            2            3            4            5 \n0.0006738544 0.0213000084 0.0904146458 0.1637886841 0.1914657786 0.1741648410 \n           6            7            8            9           10           11 \n0.1339778840 0.0919285654 0.0582628773 0.0344727114 0.0194606761 0.0103971836 \n          12           13           14 \n0.0055410260 0.0027965460 0.0013547178 \n\nplot(gamma_pmf)\n\n\n\n\n\n\n\n\n\n\n\n\nApplying a convolution\nNext we apply a convolution with the discretised incubation period distribution to the time series of infections, to generate a time series of symptom onsets. The function we have written to do this is called convolve_with_delay\n\nconvolve_with_delay\n\nfunction (ts, delay_pmf) \n{\n    max_delay &lt;- length(delay_pmf) - 1\n    convolved &lt;- numeric(length(ts))\n    for (i in seq_along(ts)) {\n        first_index &lt;- max(1, i - max_delay)\n        ts_segment &lt;- ts[seq(first_index, i)]\n        pmf &lt;- rev(delay_pmf[seq_len(i - first_index + 1)])\n        ret &lt;- sum(ts_segment * pmf)\n        convolved[i] &lt;- ret\n    }\n    convolved\n}\n&lt;bytecode: 0x55baa06ddbd0&gt;\n&lt;environment: namespace:nfidd&gt;\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nTry to understand the convolve_with_delay() function above. Try it with a few different time series and delay distributions. How would you create the time series of symptom onsets from infections, using the discretised gamma distribution created above (saved in gamma_pmf)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nonsets &lt;- convolve_with_delay(inf_ts$infections, gamma_pmf)\n\nLet’s examine the convolution result to understand how it works. First, we look at the infections and delay distribution:\n\nhead(inf_ts$infections)\n\n[1] 1 0 1 0 2 1\n\nhead(gamma_pmf)\n\n\n           0            1            2            3            4            5 \n0.0006738544 0.0213000084 0.0904146458 0.1637886841 0.1914657786 0.1741648410 \n\n\nNow let’s look at the resulting onsets:\n\nhead(onsets)\n\n[1] 0.0006738544 0.0213000084 0.0910885002 0.1850886925 0.2832281333\n[6] 0.3812273963\n\n\nTo understand what’s happening, let’s work through the first few rows.\nFor day 1, only infections from day 1 can contribute:\n\ninf_ts$infections[1] * gamma_pmf[1]\n\n           0 \n0.0006738544 \n\n\nFor day 2, infections from both day 1 and day 2 can contribute:\n\ninf_ts$infections[1] * gamma_pmf[2] + inf_ts$infections[2] * gamma_pmf[1]\n\n         1 \n0.02130001 \n\n\nFor day 3, infections from days 1, 2, and 3 can contribute:\n\ninf_ts$infections[1] * gamma_pmf[3] + inf_ts$infections[2] * gamma_pmf[2] + inf_ts$infections[3] * gamma_pmf[1]\n\n        2 \n0.0910885 \n\n\nEach day’s onsets are the sum of all previous infections weighted by the probability of observing them on that day.\n\n\n\nWe can plot these symptom onsets:\n\ncombined &lt;- inf_ts |&gt;\n  rename(time = infection_day) |&gt;\n  mutate(onsets = onsets)\nggplot(combined, aes(x = time, y = onsets)) +\n  geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nDo they look similar to the plot of symptom onsets in the session on delay distributions?",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#observation-uncertainty",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#observation-uncertainty",
    "title": "Using delay distributions to model the data generating process",
    "section": "Observation uncertainty",
    "text": "Observation uncertainty\nUsually not all data are perfectly observed. Also, the convolution we applied is a deterministic operation that brushes over the fact that individual delays are random. We should therefore find another way to model the variation these processes introduce.\nGiven that we are now dealing with count data a natural choice is the Poisson distribution. We can use this to generate uncertainty around our convolved data.\n\ncombined &lt;- combined |&gt;\n  mutate(reported_onsets = rpois(n(), onsets))\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nDoes a plot of these observations look more like the plots from the session on delay distributions than the convolution plotted above?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nggplot(combined, aes(x = time, y = reported_onsets)) +\n  geom_bar(stat = \"identity\")",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#challenge",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#challenge",
    "title": "Using delay distributions to model the data generating process",
    "section": "Challenge",
    "text": "Challenge\n\nAbove, we used a Poisson distribution to characterise uncertainty. In the Poisson distribution, the variance is the same as the mean. Another common choice is the negative binomial distribution, which has a more flexible relationship between variance and mean. If you re-did the analysis above with the negative binomial distribution, what would be the difference?\nWe could have used the individual-level model of the previous section to try to estimate the number of infections with a known delay distribution by estimating each individual infection time. How would this look in stan code? Would you expect it to yield a different result?",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#methods-in-practice",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#methods-in-practice",
    "title": "Using delay distributions to model the data generating process",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nThe primarycensored package provides tools for efficiently working with censored and truncated delay distributions. It offers both theoretical background on primary and secondary interval censoring and right truncation, as well as practical functions for estimating and using these distributions. The package implements deterministic methods that are significantly faster than the Monte Carlo approach used in censored_delay_pmf() while providing exact results.",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#references",
    "href": "sessions/using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic.html#references",
    "title": "Using delay distributions to model the data generating process",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Using delay distributions to model the data generating process"
    ]
  },
  {
    "objectID": "sessions/slides/introduction-to-real-world-forecasts.html#your-turn",
    "href": "sessions/slides/introduction-to-real-world-forecasts.html#your-turn",
    "title": "Evaluating real-world forecasts",
    "section": " Your Turn",
    "text": "Your Turn\nUse COVID-19 Forecast Hub forecasts to …\n\ncreate forecast visualizations.\nevaluate multiple forecast models.\ncreate weighted ensembles."
  },
  {
    "objectID": "sessions/slides/introduction-to-joint-estimation-of-nowcasting-and-reporting-delays.html#your-turn",
    "href": "sessions/slides/introduction-to-joint-estimation-of-nowcasting-and-reporting-delays.html#your-turn",
    "title": "Introduction to joint estimation of nowcasting and reporting delays",
    "section": " Your Turn",
    "text": "Your Turn\n\nSimulate data with delayed reporting\nPerform a joint estimation of the delay and nowcast\nUnderstand the limitations of the data generating process\nPerform a joint estimation of the delay, nowcast, and reproduction number"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-disease-progression",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-disease-progression",
    "title": "Introduction to epidemiological delays",
    "section": "Epidemiological events: disease progression",
    "text": "Epidemiological events: disease progression\n\ninfection\nsymptom onset\nbecoming infectious\nhospital admission\ndeath"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-recovery",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-recovery",
    "title": "Introduction to epidemiological delays",
    "section": "Epidemiological events: recovery",
    "text": "Epidemiological events: recovery\n\npathogen clearance\nsymptoms clearance\nend of infectiousness\ndischarge from hospital"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-control",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-control",
    "title": "Introduction to epidemiological delays",
    "section": "Epidemiological events: control",
    "text": "Epidemiological events: control\n\nquarantine\nisolation\ntreatment"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-reporting",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#epidemiological-events-reporting",
    "title": "Introduction to epidemiological delays",
    "section": "Epidemiological events: reporting",
    "text": "Epidemiological events: reporting\n\nspecimen taken\nreport added to database"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\ninfection to symptom onset\n\nIncubation period"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-1",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-1",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\ninfection to becoming infectious\n\nLatent period"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-2",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-2",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\nbecoming infectious to end of infectiousness\n\nInfectious period"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-3",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-3",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\nhospital admission to discharge\n\nLength of stay"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-4",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-4",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\nsymptom onset (person A) to symptom onset (person B, infected by A)\n\nSerial interval"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-5",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#some-delays-have-names-5",
    "title": "Introduction to epidemiological delays",
    "section": "Some delays have names",
    "text": "Some delays have names\ninfection (person A) to infection (person B, infected by A)\n\nGeneration interval"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#why-do-we-want-to-know-these",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#why-do-we-want-to-know-these",
    "title": "Introduction to epidemiological delays",
    "section": "Why do we want to know these?",
    "text": "Why do we want to know these?"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#key-parameters-in-mathematical-models",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#key-parameters-in-mathematical-models",
    "title": "Introduction to epidemiological delays",
    "section": "Key parameters in mathematical models",
    "text": "Key parameters in mathematical models\n\nFerguson et al., 2020"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#key-parameters-in-mathematical-models-1",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#key-parameters-in-mathematical-models-1",
    "title": "Introduction to epidemiological delays",
    "section": "Key parameters in mathematical models",
    "text": "Key parameters in mathematical models\n\nWard et al., 2020"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#key-elements-of-infectious-disease-epidemiology",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#key-elements-of-infectious-disease-epidemiology",
    "title": "Introduction to epidemiological delays",
    "section": "Key elements of infectious disease epidemiology",
    "text": "Key elements of infectious disease epidemiology\n Nishiura et al., 2007"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#why-do-we-want-to-know-these-1",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#why-do-we-want-to-know-these-1",
    "title": "Introduction to epidemiological delays",
    "section": "Why do we want to know these?",
    "text": "Why do we want to know these?\n\nKey elements of infectious disease epidemiology\nIntricate relationship with nowcasting/forecasting"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#quantifying-delays",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#quantifying-delays",
    "title": "Introduction to epidemiological delays",
    "section": "Quantifying delays",
    "text": "Quantifying delays\n\nEpidemiological delays are variable between individuals\nWe can capture their variability using probability distributions"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#what-is-a-probability-distribution",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#what-is-a-probability-distribution",
    "title": "Introduction to epidemiological delays",
    "section": "What is a probability distribution?",
    "text": "What is a probability distribution?\n\nMathematical way to describe variability in delays\nShows how likely different delay values are\nCommon examples for delays:\n\nLog-normal: right-skewed, good for incubation periods\nGamma: flexible shape, alternative for delays"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#warning-two-levels-of-uncertainty",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#warning-two-levels-of-uncertainty",
    "title": "Introduction to epidemiological delays",
    "section": "Warning: Two levels of uncertainty",
    "text": "Warning: Two levels of uncertainty\n\nProbability distributions characterise variability in the delays between individuals\nParameters of the probability distribution can be uncertain\n\n\n\n\\[\n\\alpha \\sim \\mathrm{Normal}(mean = 5, sd = 0.1) \\\\\n\\beta \\sim \\mathrm{Normal}(mean = 1, sd = 0.1) \\\\\n\\]\nShowing probability density functions of lognormal distributions with shape \\(\\alpha\\) and rate \\(\\beta\\)."
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#statistical-inference-for-delays",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#statistical-inference-for-delays",
    "title": "Introduction to epidemiological delays",
    "section": "Statistical inference for delays",
    "text": "Statistical inference for delays\n\nWe observe delay data but don’t know true distribution parameters\nNeed to estimate parameters from incomplete, noisy data\nBayesian approach: prior knowledge + observed data → posterior estimates"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#stan-for-delay-estimation",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#stan-for-delay-estimation",
    "title": "Introduction to epidemiological delays",
    "section": "Stan for delay estimation",
    "text": "Stan for delay estimation\n\nStatistical software for Bayesian inference\nWe specify: data structure, parameters to estimate, model relationships\nStan returns parameter estimates with proper uncertainty quantification"
  },
  {
    "objectID": "sessions/slides/introduction-to-epidemiological-delays.html#your-turn",
    "href": "sessions/slides/introduction-to-epidemiological-delays.html#your-turn",
    "title": "Introduction to epidemiological delays",
    "section": " Your Turn",
    "text": "Your Turn\n\nSimulate epidemiological delays\nEstimate parameters of a delay distribution"
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#biases-in-epidemiological-delays",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#biases-in-epidemiological-delays",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Biases in epidemiological delays",
    "text": "Biases in epidemiological delays\nWhy might our estimates of epidemiological delays be biased?\n\n\n\ndata reliability and representativeness\n\n\n\nintrinsic issues with data collection and recording"
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#issue-1-double-censoring",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#issue-1-double-censoring",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Issue #1: Double censoring",
    "text": "Issue #1: Double censoring\n\nreporting of events usually as a date (not date + precise time)\nfor short delays this can make quite a difference\naccounting for it incorrectly can introduce more bias than doing nothing"
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Double censoring: example",
    "text": "Double censoring: example\nWe are trying to estimate an incubation period. For person A we know exposure happened on day 1 and symptom onset on day 3."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-1",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-1",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Double censoring: example",
    "text": "Double censoring: example\nWe are trying to estimate an incubation period. For person A we know exposure happened on day 1 and symptom onset on day 3."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-2",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-2",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Double censoring: example",
    "text": "Double censoring: example\nWe are trying to estimate an incubation period. For person A we know exposure happened on day 1 and symptom onset on day 3."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-3",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#double-censoring-example-3",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Double censoring: example",
    "text": "Double censoring: example\nWe are trying to estimate an incubation period. For person A we know exposure happened on day 1 and symptom onset on day 3.\n\nThe true incubation period of A could be anywhere between 1 and 3 days (but not all equally likely)."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#understanding-double-censoring-components",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#understanding-double-censoring-components",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Understanding double censoring components",
    "text": "Understanding double censoring components\nDouble interval censoring can be decomposed into:\n\nPrimary event censoring: Uncertainty about when the first event occurred within its reported day\n\n\nSecondary event censoring: Uncertainty about when the second event occurred within its reported day\n\n\n\n\n\n\n\n\nWarning\n\n\nA common mistake is to only account for secondary event censoring while ignoring primary event censoring."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#issue-2-right-truncation",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#issue-2-right-truncation",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Issue #2: right truncation",
    "text": "Issue #2: right truncation\n\nreporting of events can be triggered by the secondary event\nin that case, longer delays might be missing because whilst the primary events have occurred the secondary events have not occurred yet"
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Example: right truncation",
    "text": "Example: right truncation\nWe are trying to estimate an incubation period. Each arrow represents one person with an associated pair of events (infection and symptom onset)."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-1",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-1",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Example: right truncation",
    "text": "Example: right truncation\nWe are trying to estimate an incubation period. Each arrow represents one person with an associated pair of events (infection and symptom onset)."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-2",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-2",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Example: right truncation",
    "text": "Example: right truncation\nWe are trying to estimate an incubation period. Each arrow represents one person with an associated pair of events (infection and symptom onset)."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-3",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-3",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Example: right truncation",
    "text": "Example: right truncation\nWe are trying to estimate an incubation period. Each arrow represents one person with an associated pair of events (infection and symptom onset)\n\nOn the day of analysis we have not observed some onsets yet. The delay from infection to onset for these delays tended to be longer. This is made worse during periods of exponential growth."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-4",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#example-right-truncation-4",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Example: right truncation",
    "text": "Example: right truncation\nWe are trying to estimate an incubation period. Each arrow represents one person with an associated pair of events (infection and symptom onset)\n\nWe need to account for infections with longer delays that we haven’t yet observed. We can best do this by using a lognormal distribution for the delays that we do have data on."
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#censoring-and-right-truncation",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#censoring-and-right-truncation",
    "title": "Introduction to biases in epidemiological delays",
    "section": "Censoring and right truncation",
    "text": "Censoring and right truncation\n\nWhen analysing data from an outbreak in real time, we are likely to have double censoring and right truncation, making things worse\nIn the practical we will only look at the two separately to keep things simple"
  },
  {
    "objectID": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#your-turn",
    "href": "sessions/slides/introduction-to-biases-in-epidemiological-delays.html#your-turn",
    "title": "Introduction to biases in epidemiological delays",
    "section": " Your Turn",
    "text": "Your Turn\n\nSimulate epidemiological delays with biases\nEstimate parameters of a delay distribution, correcting for biases"
  },
  {
    "objectID": "sessions/slides/hub-playground.html#your-turn",
    "href": "sessions/slides/hub-playground.html#your-turn",
    "title": "Forecasting practice and local hubs",
    "section": " Your Turn",
    "text": "Your Turn\n\nClone the Sandbox Hub.\nBuild and visualize forecasts from models that you are interested in.\nValidate and test forecasts.\nSubmit forecasts to the Sandbox Hub."
  },
  {
    "objectID": "sessions/slides/forecasting-nowcasting.html#your-turn",
    "href": "sessions/slides/forecasting-nowcasting.html#your-turn",
    "title": "Combining nowcasting and forecasting",
    "section": " Your Turn",
    "text": "Your Turn\n\nGenerate example data with realistic truncation\nImplement 4 forecasting approaches (complete data, pipeline, joint)\nCompare performance quantitatively\nDiscuss trade-offs and applications"
  },
  {
    "objectID": "sessions/slides/convolutions.html#individual-delay-distributions",
    "href": "sessions/slides/convolutions.html#individual-delay-distributions",
    "title": "Delay distributions at the population level",
    "section": "Individual delay distributions",
    "text": "Individual delay distributions\nLet us consider an epidemiological process that is characterised by a discrete distribution \\(f_d\\).\n\nExample:\n\\(f_d\\): Probability of having incubation period of \\(d\\) days."
  },
  {
    "objectID": "sessions/slides/convolutions.html#individual-to-population-a-simple-example",
    "href": "sessions/slides/convolutions.html#individual-to-population-a-simple-example",
    "title": "Delay distributions at the population level",
    "section": "Individual to population: A simple example",
    "text": "Individual to population: A simple example\nIf we know individual events, we can add a draw from the delay distribution and sum up the resulting delays."
  },
  {
    "objectID": "sessions/slides/convolutions.html#why-not-use-individual-delays",
    "href": "sessions/slides/convolutions.html#why-not-use-individual-delays",
    "title": "Delay distributions at the population level",
    "section": "Why not use individual delays?",
    "text": "Why not use individual delays?\n\nwe don’t always have individual data available\nwe often model other processes at the population level (such as transmission) and so being able to model delays on the same scale is useful\ndoing the computation at the population level requires fewer calculations (i.e. is faster)\n\n\n\nhowever, a downside is that we won’t have realistic uncertainty, especially if the number of individuals is small"
  },
  {
    "objectID": "sessions/slides/convolutions.html#convolution-from-individual-to-population",
    "href": "sessions/slides/convolutions.html#convolution-from-individual-to-population",
    "title": "Delay distributions at the population level",
    "section": "Convolution: From individual to population",
    "text": "Convolution: From individual to population"
  },
  {
    "objectID": "sessions/slides/convolutions.html#population-level-counts-the-mathematics",
    "href": "sessions/slides/convolutions.html#population-level-counts-the-mathematics",
    "title": "Delay distributions at the population level",
    "section": "Population level counts: The mathematics",
    "text": "Population level counts: The mathematics\nIf the number of individuals \\(P_{t'}\\) that have their primary event at time \\(t'\\) then we can rewrite this as\n\\[\nS_t = \\sum_{t'} P_{t'} f_{t - t'}\n\\]\nThis operation is called a (discrete) convolution of \\(P\\) with \\(f_d\\).\nWe can use convolutions with the delay distribution that applies at the individual level to determine population-level counts."
  },
  {
    "objectID": "sessions/slides/convolutions.html#what-if-f-is-continuous",
    "href": "sessions/slides/convolutions.html#what-if-f-is-continuous",
    "title": "Delay distributions at the population level",
    "section": "What if \\(f\\) is continuous?",
    "text": "What if \\(f\\) is continuous?\nHaving moved to the population level, we can’t estimate individual-level event times any more.\nInstead, we discretise the distribution (remembering that it is double censored - as both events are censored).\nThis can be solved mathematically but in the session we will use simulation."
  },
  {
    "objectID": "sessions/slides/convolutions.html#your-turn",
    "href": "sessions/slides/convolutions.html#your-turn",
    "title": "Delay distributions at the population level",
    "section": " Your Turn",
    "text": "Your Turn\n\nSimulate convolutions with infection counts\nDiscretise continuous distributions\nEstimate parameters numbers of infections from number of symptom onsets, using a convolution model"
  },
  {
    "objectID": "sessions/real-world-forecasts.html",
    "href": "sessions/real-world-forecasts.html",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "",
    "text": "So far in this course we have focused on building, visualising, evaluating, and combining “toy” forecast models in somewhat synthetic settings. In this session you will work with real forecasts from an existing modeling hub. This will expose many of the challenges involved with real-time forecasting, as well as the benefits of coordinated modeling efforts.\n\n\n\nReal-world forecasts\n\n\n\n\nThe aim of this session is to develop advanced skills in working with real forecasts and an appreciation for the challenges of real-time forecasting.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/real-world-forecasts.qmd.\n\n\n\nIn this session we will use the nfidd package to access some stored datasets, the dplyr package for data wrangling, the ggplot2 library for plotting, and the following hubverse packages: hubData, hubUtils, hubEvals and hubEnsembles.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"hubData\")\nlibrary(\"hubUtils\")\nlibrary(\"hubEvals\")\nlibrary(\"hubEnsembles\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(5050) # for Shohei Ohtani!",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#slides",
    "href": "sessions/real-world-forecasts.html#slides",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "",
    "text": "Real-world forecasts",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#objectives",
    "href": "sessions/real-world-forecasts.html#objectives",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "",
    "text": "The aim of this session is to develop advanced skills in working with real forecasts and an appreciation for the challenges of real-time forecasting.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/real-world-forecasts.qmd.\n\n\n\nIn this session we will use the nfidd package to access some stored datasets, the dplyr package for data wrangling, the ggplot2 library for plotting, and the following hubverse packages: hubData, hubUtils, hubEvals and hubEnsembles.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"hubData\")\nlibrary(\"hubUtils\")\nlibrary(\"hubEvals\")\nlibrary(\"hubEnsembles\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(5050) # for Shohei Ohtani!",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#source-file",
    "href": "sessions/real-world-forecasts.html#source-file",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "",
    "text": "The source file of this session is located at sessions/real-world-forecasts.qmd.",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#libraries-used",
    "href": "sessions/real-world-forecasts.html#libraries-used",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "",
    "text": "In this session we will use the nfidd package to access some stored datasets, the dplyr package for data wrangling, the ggplot2 library for plotting, and the following hubverse packages: hubData, hubUtils, hubEvals and hubEnsembles.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"hubData\")\nlibrary(\"hubUtils\")\nlibrary(\"hubEvals\")\nlibrary(\"hubEnsembles\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#initialisation",
    "href": "sessions/real-world-forecasts.html#initialisation",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(5050) # for Shohei Ohtani!",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#forecast-dimensions",
    "href": "sessions/real-world-forecasts.html#forecast-dimensions",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Forecast dimensions",
    "text": "Forecast dimensions\nHere are some details about the structure of the COVID-19 Forecast Hub project.\n\nForecasts are collected every week, with submissions due on Wednesdays.\nTeams make predictions of -1 through 3 week ahead counts of new hospital admissions due to COVID-19.\nThe ground truth data (“target data” in hubverse lingo) is taken from the Weekly Hospital Respiratory Data (HRD) Metrics by Jurisdiction, National Healthcare Safety Network (NHSN) dataset.\nTeams may forecast for up to 53 locations, including the US national level, all 50 states, plus Puerto Rico and Washington DC.\nForecasts are solicited in a quantile format.\n\nFurther details about the hub are available at its GitHub README page.",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#accessing-forecast-data-from-the-cloud",
    "href": "sessions/real-world-forecasts.html#accessing-forecast-data-from-the-cloud",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Accessing forecast data from the cloud",
    "text": "Accessing forecast data from the cloud\nThe following code can be used to retrieve covid forecasts from the cloud. However, there are over 2 million rows of data, and it can take some bandwidth to download. To make these results more reproducible and to provide a snapshot of the data, we also provide the forecasts as a data object that can be loaded directly from the nfidd package.\n\nhub_path_cloud &lt;- hubData::s3_bucket(\"s3://covid19-forecast-hub\")\nhub_con_cloud &lt;- hubData::connect_hub(hub_path_cloud, skip_checks = TRUE)\n\ndata(covid_locations)\n\ncovid_forecasts &lt;- hub_con_cloud |&gt; \n  filter(\n    output_type == \"quantile\",\n    target == \"wk inc covid hosp\",\n    horizon &gt;= 0\n    ) |&gt; \n  collect() |&gt; \n  left_join(covid_locations)\n\nThis line of code will load all the forecasts saved in the package into your R session.\n\ndata(covid_forecasts)",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#accessing-target-data-from-the-cloud",
    "href": "sessions/real-world-forecasts.html#accessing-target-data-from-the-cloud",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Accessing target data from the cloud",
    "text": "Accessing target data from the cloud\nHere is code to query the time-series data from the cloud-based hub.\n\ncovid_time_series &lt;- connect_target_timeseries(hub_path_cloud) |&gt; \n  filter(target == \"wk inc covid hosp\") |&gt; \n  collect() |&gt; \n  left_join(covid_locations)\n\nHowever, to ensure reproducibility (since this is a live dataset), we have downloaded this object for you already as of July 9, 2025, and it is available to load in from the course R package directly.\n\ndata(covid_time_series)",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#exploratory-data-analysis-and-visualization",
    "href": "sessions/real-world-forecasts.html#exploratory-data-analysis-and-visualization",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Exploratory data analysis and visualization",
    "text": "Exploratory data analysis and visualization\nNow that we’ve downloaded the forecast and target data, let’s just do a few basic explorations to make sure we understand the dimensions of our data. Ideally, storing data in the hubverse ensures that the data is “clean”, no typos in targets or locations, no missing quantile levels, no negative predictions, etc… But let’s start by just getting a sense of how many unique values we have for location, horizon, target_end_date, etc…\n\nunique_per_column &lt;- covid_forecasts  |&gt; \n  select(-value) |&gt; \n  purrr::map(~ sort(unique(.x)))\nunique_per_column\n\n$reference_date\n [1] \"2024-11-23\" \"2024-11-30\" \"2024-12-07\" \"2024-12-14\" \"2024-12-21\"\n [6] \"2024-12-28\" \"2025-01-04\" \"2025-01-11\" \"2025-01-18\" \"2025-01-25\"\n[11] \"2025-02-01\" \"2025-02-08\" \"2025-02-15\" \"2025-02-22\" \"2025-03-01\"\n[16] \"2025-03-08\" \"2025-03-15\" \"2025-03-22\" \"2025-03-29\" \"2025-04-05\"\n[21] \"2025-04-12\" \"2025-04-19\" \"2025-04-26\" \"2025-05-03\" \"2025-05-10\"\n[26] \"2025-05-17\" \"2025-05-24\" \"2025-05-31\" \"2025-06-07\" \"2025-06-14\"\n[31] \"2025-06-21\" \"2025-06-28\" \"2025-07-05\" \"2025-07-12\"\n\n$location\n [1] \"01\" \"02\" \"04\" \"05\" \"06\" \"08\" \"09\" \"10\" \"11\" \"12\" \"13\" \"15\" \"16\" \"17\" \"18\"\n[16] \"19\" \"20\" \"21\" \"22\" \"23\" \"24\" \"25\" \"26\" \"27\" \"28\" \"29\" \"30\" \"31\" \"32\" \"33\"\n[31] \"34\" \"35\" \"36\" \"37\" \"38\" \"39\" \"40\" \"41\" \"42\" \"44\" \"45\" \"46\" \"47\" \"48\" \"49\"\n[46] \"50\" \"51\" \"53\" \"54\" \"55\" \"56\" \"72\" \"US\"\n\n$horizon\n[1] 0 1 2 3\n\n$target_end_date\n [1] \"2024-11-23\" \"2024-11-30\" \"2024-12-07\" \"2024-12-14\" \"2024-12-21\"\n [6] \"2024-12-28\" \"2025-01-04\" \"2025-01-11\" \"2025-01-18\" \"2025-01-25\"\n[11] \"2025-02-01\" \"2025-02-08\" \"2025-02-15\" \"2025-02-22\" \"2025-03-01\"\n[16] \"2025-03-08\" \"2025-03-15\" \"2025-03-22\" \"2025-03-29\" \"2025-04-05\"\n[21] \"2025-04-12\" \"2025-04-19\" \"2025-04-26\" \"2025-05-03\" \"2025-05-10\"\n[26] \"2025-05-17\" \"2025-05-24\" \"2025-05-31\" \"2025-06-07\" \"2025-06-14\"\n[31] \"2025-06-21\" \"2025-06-28\" \"2025-07-05\" \"2025-07-12\" \"2025-07-19\"\n[36] \"2025-07-26\" \"2025-08-02\"\n\n$target\n[1] \"wk inc covid hosp\"\n\n$output_type\n[1] \"quantile\"\n\n$output_type_id\n [1] \"0.01\"  \"0.025\" \"0.05\"  \"0.1\"   \"0.15\"  \"0.2\"   \"0.25\"  \"0.3\"   \"0.35\" \n[10] \"0.4\"   \"0.45\"  \"0.5\"   \"0.55\"  \"0.6\"   \"0.65\"  \"0.7\"   \"0.75\"  \"0.8\"  \n[19] \"0.85\"  \"0.9\"   \"0.95\"  \"0.975\" \"0.99\" \n\n$model_id\n [1] \"CEPH-Rtrend_covid\"            \"CFA_Pyrenew-Pyrenew_H_COVID\" \n [3] \"CFA_Pyrenew-Pyrenew_HE_COVID\" \"CFA_Pyrenew-Pyrenew_HW_COVID\"\n [5] \"CFA_Pyrenew-PyrenewHEW_COVID\" \"CMU-climate_baseline\"        \n [7] \"CMU-TimeSeries\"               \"CovidHub-baseline\"           \n [9] \"CovidHub-ensemble\"            \"Google_SAI-Ensemble\"         \n[11] \"JHU_CSSE-CSSE_Ensemble\"       \"Metaculus-cp\"                \n[13] \"MOBS-GLEAM_COVID\"             \"NEU_ISI-AdaptiveEnsemble\"    \n[15] \"OHT_JHU-nbxd\"                 \"UGA_flucast-INFLAenza\"       \n[17] \"UM-DeepOutbreak\"              \"UMass-ar6_pooled\"            \n[19] \"UMass-gbqr\"                  \n\n$abbreviation\n [1] \"AK\" \"AL\" \"AR\" \"AZ\" \"CA\" \"CO\" \"CT\" \"DC\" \"DE\" \"FL\" \"GA\" \"HI\" \"IA\" \"ID\" \"IL\"\n[16] \"IN\" \"KS\" \"KY\" \"LA\" \"MA\" \"MD\" \"ME\" \"MI\" \"MN\" \"MO\" \"MS\" \"MT\" \"NC\" \"ND\" \"NE\"\n[31] \"NH\" \"NJ\" \"NM\" \"NV\" \"NY\" \"OH\" \"OK\" \"OR\" \"PA\" \"PR\" \"RI\" \"SC\" \"SD\" \"TN\" \"TX\"\n[46] \"US\" \"UT\" \"VA\" \"VT\" \"WA\" \"WI\" \"WV\" \"WY\"\n\n$location_name\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"District of Columbia\"\n[10] \"Florida\"              \"Georgia\"              \"Hawaii\"              \n[13] \"Idaho\"                \"Illinois\"             \"Indiana\"             \n[16] \"Iowa\"                 \"Kansas\"               \"Kentucky\"            \n[19] \"Louisiana\"            \"Maine\"                \"Maryland\"            \n[22] \"Massachusetts\"        \"Michigan\"             \"Minnesota\"           \n[25] \"Mississippi\"          \"Missouri\"             \"Montana\"             \n[28] \"Nebraska\"             \"Nevada\"               \"New Hampshire\"       \n[31] \"New Jersey\"           \"New Mexico\"           \"New York\"            \n[34] \"North Carolina\"       \"North Dakota\"         \"Ohio\"                \n[37] \"Oklahoma\"             \"Oregon\"               \"Pennsylvania\"        \n[40] \"Puerto Rico\"          \"Rhode Island\"         \"South Carolina\"      \n[43] \"South Dakota\"         \"Tennessee\"            \"Texas\"               \n[46] \"US\"                   \"Utah\"                 \"Vermont\"             \n[49] \"Virginia\"             \"Washington\"           \"West Virginia\"       \n[52] \"Wisconsin\"            \"Wyoming\"             \n\n$population\n [1]    578759    623989    705749    731545    762062    884659    973764\n [8]   1059361   1068778   1344212   1359711   1415872   1787065   1792147\n[15]   1934408   2096829   2913314   2976149   3017804   3080156   3155070\n[22]   3205958   3565287   3754939   3956971   4217737   4467673   4648794\n[29]   4903185   5148714   5639632   5758736   5822434   6045680   6626371\n[36]   6732219   6829174   6892503   7278717   7614893   8535519   8882190\n[43]   9986857  10488084  10617423  11689100  12671821  12801989  19453561\n[50]  21477737  28995881  39512223 332875137\n\n\nFrom the above, we can see that\n\nthe forecasts were made every week from 2024-11-23 until 2025-07-12,\nthere is just one target, the weekly counts of hospital admissions due to COVID-19, and we only have quantile values for it,\nwe have forecasts for horizons 0 to 3 weeks,\nthere are 23 quantile levels present in the data,\nthere are 19 models.\n\nLet’s just do a quick visualization of forecasts from one week to make sure we understand the structure of what one date and location’s worth of forecasts look like.\n\ncovid_forecasts |&gt; \n  filter(reference_date == \"2025-02-15\", abbreviation == \"GA\") |&gt; \n  hubVis::plot_step_ahead_model_output(\n    target_data = covid_time_series |&gt; \n      filter(as_of == as.Date(\"2025-07-09\"),\n             abbreviation == \"GA\"),\n    use_median_as_point = TRUE,\n    x_col_name = \"target_end_date\",\n    x_target_col_name = \"date\",\n    pal_color = \"Set3\", \n    title = \"Weekly hospitalizations due to COVID-19 (data and forecasts)\"\n  )\n\nWarning: ! `model_out_tbl` must be a `model_out_tbl`. Class applied by default\n\n\nWarning: ! `model_out_tbl` contains more than 5 models, the plot will be reduced to show\n  only one interval (the maximum interval value): \"0.95\"\n\n\nWarning: ! `output_type_id` column must be a numeric. Converting to numeric.\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTake a moment to explore the forecasts interactively in the above plot. We suggest starting by double-clicking on the “target” line in the legend. This will remove all models from the plot. Then add the models one at a time to compare them.",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#who-submitted-when",
    "href": "sessions/real-world-forecasts.html#who-submitted-when",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Who submitted when?",
    "text": "Who submitted when?\nOne thing that can foul up evaluation and ensembles of models is when not all models submit at the same times.\nDo all models in our dataset of forecasts have the same number of predictions? Let’s look at this a few different ways.\nHere is a tabular summary showing some summary stats about how often and how much each model submitted:\n\ncovid_forecasts |&gt; \n  group_by(model_id) |&gt; \n  summarise(\n    n_submissions = n_distinct(reference_date),\n    n_rows = n(),\n    n_horizons = n_distinct(horizon),\n    n_locations = n_distinct(location)\n  ) |&gt; \n  arrange(-n_submissions) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nmodel_id\nn_submissions\nn_rows\nn_horizons\nn_locations\n\n\n\n\nCEPH-Rtrend_covid\n33\n160908\n4\n53\n\n\nCovidHub-baseline\n33\n160908\n4\n53\n\n\nCovidHub-ensemble\n33\n160908\n4\n53\n\n\nCMU-TimeSeries\n32\n156032\n4\n53\n\n\nCMU-climate_baseline\n32\n156032\n4\n53\n\n\nUMass-ar6_pooled\n32\n155112\n4\n53\n\n\nUMass-gbqr\n32\n155112\n4\n53\n\n\nOHT_JHU-nbxd\n31\n151156\n4\n53\n\n\nMOBS-GLEAM_COVID\n29\n136068\n4\n52\n\n\nUM-DeepOutbreak\n27\n131652\n4\n53\n\n\nCFA_Pyrenew-Pyrenew_H_COVID\n26\n59202\n2\n51\n\n\nNEU_ISI-AdaptiveEnsemble\n26\n104995\n4\n53\n\n\nMetaculus-cp\n22\n966\n4\n1\n\n\nCFA_Pyrenew-Pyrenew_HE_COVID\n19\n43746\n2\n51\n\n\nJHU_CSSE-CSSE_Ensemble\n19\n82340\n4\n52\n\n\nCFA_Pyrenew-Pyrenew_HW_COVID\n18\n31694\n2\n48\n\n\nUGA_flucast-INFLAenza\n16\n78016\n4\n53\n\n\nGoogle_SAI-Ensemble\n12\n58144\n4\n53\n\n\nCFA_Pyrenew-PyrenewHEW_COVID\n11\n19642\n2\n47\n\n\n\n\n\nAnd here is a visual showing more details about submissions each week by model, sorted with the models with the most forecasts at the top. Each tile represents a week, with lighter blue colors in a given model-week indicating a higher number of locations being submitted for.\n\ncovid_forecasts |&gt; \n  group_by(model_id, reference_date) |&gt; \n  summarise(\n    n_rows = n(),\n    n_locations = n_distinct(location)\n  ) |&gt; \n  ungroup() |&gt; \n  mutate(model_id = reorder(model_id, n_rows, FUN = sum)) |&gt; \n  ggplot() +\n  geom_tile(aes(x = reference_date, y = model_id, fill = n_locations))\n\n`summarise()` has grouped output by 'model_id'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nWhat are some reasons you can imagine for a model not being submitted in a given week?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere are some real examples of why models have not been submitted in a given week:\n\nThe target dataset is not released (as was the case one week in January 2025).\nThe modeler who runs the model every week is sick or on vacation.\nA model may have joined mid-season because it was still in development.\nA model have have gone offline because a team stopped participating.\nThe modeling team may be unhappy with their forecast and decide not to submit (for evaluation of the model vs the modeling team this can be a big problem).\n… (there are likely other reasons too)",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#what-are-the-models",
    "href": "sessions/real-world-forecasts.html#what-are-the-models",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "What are the models?",
    "text": "What are the models?\n\n\n\n\n\n\nWhat is a baseline model?\n\n\n\nBaseline models generate forecasts that are created to serve as benchmarks. They are designed to be naive, not overly “optimized” approaches that make forecasts. In evaluation, baseline models may occasionally do well by chance (for example, a flat line model may do well when dynamics are, well, flat). But in general, their forecasts should be “beatable” by models that are able to learn and make predictions based on dynamics in the data. The Random Walk (rw) model from the Forecast Evaluation session is a good example of a baseline model. Other models that might serve as slightly more sophisticated forecast baselines could be a simple ARIMA model, or a model that smooths the recent data and predicts the a continuation of a recent trend in the data.\n\n\nYou can read about each of the models in the model-metadata folder of the forecast hub. Model metadata can also be programmatically accessed using hubData::load_model_metadata(). There are a wide variety of models submitted to this hub.\n\n\n\n\n\n\nTip\n\n\n\nHere are a few models that we will highlight with their provided descriptions before going further:\nThis is the hub-generated ensemble forecast:\n\nCovidHub-ensemble: “Median-based ensemble of quantile forecasts submissions.” Note this is similar to the ensembles we introduced in the ensembles session.\n\nHere are two models that have been explicitly designed as “baselines” or benchmarks:\n\nCovidHub-baseline: Flat baseline model. The most observed value from the target dataset is the median forward projection. Prospective uncertainty is based on the preceding timeseries.” (This is kind of like the rw model from previous sessions.)\nCMU-climate_baseline: “Using data from 2022 onwards, the climatological model uses samples from the 7 weeks centered around the target week and reference week to form the quantiles for the target week, as one might use climate information to form a meteorological forecast. To get more variation at some potential issue of generalization, one can form quantiles after aggregating across geographic values as well as years (after converting to a rate based case count). This model uses a simple average of the geo-specific quantiles and the geo-aggregated quantiles.” (This is kind of like our fourier model from previous sessions.)\n\nHere are two models built by InsightNet-funded teams:\n\nUMass-ar6_pooled: “AR(6) model after fourth root data transform. AR coefficients are shared across all locations. A separate variance parameter is estimated for each location.”\nMOBS-GLEAM_COVID: “Metapopulation, age structured SLIR model. … The GLEAM framework is based on a metapopulation approach in which the US is divided into geographical subpopulations. Human mobility between subpopulations is represented on a network. … Superimposed on the US population and mobility layers is an compartmental epidemic model that defines the infection and population dynamics.”\n\n\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nBrowse through some of the other model descriptions. Which ones look interesting to you and why? Discuss with your neighbor.",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#visual-evaluation",
    "href": "sessions/real-world-forecasts.html#visual-evaluation",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Visual evaluation",
    "text": "Visual evaluation\nEven though we have probabilistic forecasts, sometimes it is just easier to look at the point forecasts when trying to look at a lot of forecasts. Here is a plot showing all the point forecasts (the median from the quantile forecasts) for one state. The forecasts are in red and the black line shows the observed data.\n\ncovid_forecasts |&gt; \n  filter(output_type_id == 0.5,\n         abbreviation == \"GA\") |&gt; \n  ggplot() +\n  geom_line(aes(x = target_end_date, \n                y = value, \n                group = interaction(reference_date, model_id)),\n            alpha = 0.5, \n            color = \"red\") +\n  geom_line(data = filter(covid_time_series, \n                          abbreviation == \"GA\",\n                          as_of == \"2025-07-09\"), \n            aes(x = date,\n                y = observation))+\n  ggtitle(\"Forecasts and data for Georgia\") +\n  ylab(\"incident hospital admissions\") +\n  xlab(NULL)\n\n\n\n\n\n\n\n\n\ncovid_forecasts |&gt; \n  filter(output_type_id == 0.5,\n         abbreviation == \"VA\") |&gt; \n  ggplot() +\n  geom_line(aes(x = target_end_date, \n                y = value, \n                group = interaction(reference_date, model_id)),\n            alpha = 0.5, \n            color = \"red\") +\n  geom_line(data = filter(covid_time_series, \n                          abbreviation == \"VA\",\n                          as_of == \"2025-07-09\"), \n            aes(x = date,\n                y = observation))+\n  ggtitle(\"Forecasts and data for Virginia\") +\n  ylab(\"incident hospital admissions\") +\n  xlab(NULL)\n\n\n\n\n\n\n\n\nThe forecasts can be interactively explored at the COVID-19 Forecast Hub Dashboard.",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#overall-model-comparison",
    "href": "sessions/real-world-forecasts.html#overall-model-comparison",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Overall model comparison",
    "text": "Overall model comparison\nWe will start our quantitative analysis with an overall evaluation of the models, using, as we did in earlier sessions, hubEvals. We will use the Weighted Interval Score (WIS) as our primary metric of interest as we did in the session on Forecast Ensembles.\n\nscores &lt;- score_model_out(\n  covid_forecasts, \n  covid_oracle_output\n  )\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\nscores |&gt; \n  arrange(wis) |&gt; \n  knitr::kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_id\nwis\noverprediction\nunderprediction\ndispersion\nbias\ninterval_coverage_50\ninterval_coverage_90\nae_median\n\n\n\n\nCFA_Pyrenew-PyrenewHEW_COVID\n7.566\n2.740\n1.690\n3.136\n0.069\n0.503\n0.875\n11.987\n\n\nCFA_Pyrenew-Pyrenew_HW_COVID\n11.988\n1.551\n4.163\n6.275\n-0.127\n0.575\n0.932\n17.033\n\n\nCFA_Pyrenew-Pyrenew_HE_COVID\n15.643\n3.957\n4.749\n6.937\n-0.033\n0.511\n0.887\n23.709\n\n\nUGA_flucast-INFLAenza\n19.607\n4.729\n5.098\n9.779\n0.084\n0.531\n0.910\n30.682\n\n\nGoogle_SAI-Ensemble\n25.710\n7.096\n12.936\n5.678\n-0.078\n0.432\n0.850\n35.452\n\n\nCFA_Pyrenew-Pyrenew_H_COVID\n40.732\n8.706\n15.243\n16.782\n-0.205\n0.530\n0.898\n62.741\n\n\nCovidHub-ensemble\n41.166\n6.813\n14.184\n20.169\n-0.100\n0.561\n0.909\n62.543\n\n\nUMass-ar6_pooled\n47.129\n11.030\n16.047\n20.051\n0.023\n0.576\n0.916\n70.351\n\n\nUMass-gbqr\n51.002\n6.195\n30.430\n14.377\n-0.100\n0.459\n0.836\n74.142\n\n\nCEPH-Rtrend_covid\n52.582\n7.454\n21.806\n23.322\n-0.231\n0.463\n0.825\n81.899\n\n\nOHT_JHU-nbxd\n57.426\n6.205\n36.073\n15.148\n-0.212\n0.349\n0.685\n86.601\n\n\nCovidHub-baseline\n59.678\n11.839\n27.006\n20.833\n-0.070\n0.627\n0.848\n80.823\n\n\nNEU_ISI-AdaptiveEnsemble\n59.836\n20.368\n19.107\n20.361\n-0.079\n0.357\n0.700\n92.284\n\n\nCMU-TimeSeries\n63.432\n23.031\n9.323\n31.078\n-0.052\n0.508\n0.900\n92.438\n\n\nJHU_CSSE-CSSE_Ensemble\n64.317\n19.601\n18.537\n26.179\n-0.068\n0.485\n0.811\n91.388\n\n\nMOBS-GLEAM_COVID\n64.494\n13.450\n35.783\n15.261\n-0.349\n0.303\n0.638\n96.427\n\n\nUM-DeepOutbreak\n71.665\n15.181\n13.954\n42.530\n0.008\n0.639\n0.940\n96.881\n\n\nCMU-climate_baseline\n130.380\n61.793\n3.076\n65.511\n0.455\n0.492\n0.934\n229.489\n\n\nMetaculus-cp\n1315.161\n77.497\n281.803\n955.862\n-0.190\n0.690\n1.000\n1657.442\n\n\n\n\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nShould we trust the results in this table to give us a reliable ranking for all of the forecasts? Why or why not?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBecause there are so many missing forecasts, this single summary is not reliable. We will need a more careful comparison to determine relative rankings of models.\nRecall, here is the summary of which models submitted when. There is a lot of variation in how much each model submitted!\n\ncovid_forecasts |&gt; \n  group_by(model_id) |&gt; \n  summarise(\n    n_submissions = n_distinct(reference_date),\n    n_rows = n(),\n    n_horizons = n_distinct(horizon),\n    n_locations = n_distinct(location)\n  ) |&gt; \n  arrange(-n_submissions) |&gt; \n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nmodel_id\nn_submissions\nn_rows\nn_horizons\nn_locations\n\n\n\n\nCEPH-Rtrend_covid\n33\n160908\n4\n53\n\n\nCovidHub-baseline\n33\n160908\n4\n53\n\n\nCovidHub-ensemble\n33\n160908\n4\n53\n\n\nCMU-TimeSeries\n32\n156032\n4\n53\n\n\nCMU-climate_baseline\n32\n156032\n4\n53\n\n\nUMass-ar6_pooled\n32\n155112\n4\n53\n\n\nUMass-gbqr\n32\n155112\n4\n53\n\n\nOHT_JHU-nbxd\n31\n151156\n4\n53\n\n\nMOBS-GLEAM_COVID\n29\n136068\n4\n52\n\n\nUM-DeepOutbreak\n27\n131652\n4\n53\n\n\nCFA_Pyrenew-Pyrenew_H_COVID\n26\n59202\n2\n51\n\n\nNEU_ISI-AdaptiveEnsemble\n26\n104995\n4\n53\n\n\nMetaculus-cp\n22\n966\n4\n1\n\n\nCFA_Pyrenew-Pyrenew_HE_COVID\n19\n43746\n2\n51\n\n\nJHU_CSSE-CSSE_Ensemble\n19\n82340\n4\n52\n\n\nCFA_Pyrenew-Pyrenew_HW_COVID\n18\n31694\n2\n48\n\n\nUGA_flucast-INFLAenza\n16\n78016\n4\n53\n\n\nGoogle_SAI-Ensemble\n12\n58144\n4\n53\n\n\nCFA_Pyrenew-PyrenewHEW_COVID\n11\n19642\n2\n47\n\n\n\n\n\nIn real-world forecasting hubs, it is very common to have missing forecasts, and it probably isn’t safe to assume that these forecasts are “missing at random”. For example, some teams may choose not to submit forecasts when their model’s algorithm fails to converge, signaling a particularly difficult phase of the outbreak to predict—while others, consciously or not, submit during these periods and receive lower scores.",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#additional-metrics",
    "href": "sessions/real-world-forecasts.html#additional-metrics",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Additional metrics",
    "text": "Additional metrics\nBefore we dig further into the evaluation, let’s introduce a few additional evaluation metrics and ideas.\n\nRelative scores\nRelative skill scores are a way to create “head-to-head” comparisons of models that to some extent are able to adjust for the difficulty of the predictions made by each model. Here is a figure showing how the relative scores are calculated, based on aggregated pairwise comparisons between each pair of models.\n\n\n\nSchematic of relative skill score computation.(Bosse et al. 2024)\n\n\n\n\n\n\n\n\nInterpreting relative skill\n\n\n\nThe interpretation of the relative skill metric is the factor by which the score for a given model is more or less accurate than the average model, adjusting for the difficulty of the forecasts made by that particular model. Relative skill scores that are lower than 1 indicate that the model is more accurate on the whole than the average model evaluated. For example, a 0.9 relative WIS skill for model A suggests that model A was 10% more accurate on average than other models, adjusting for the difficulty level of forecasts made by model A.\n\n\n\nExample of relative skill.\n\n\n\n\nThis metric (or a version of it) has been used in practice in a number of large-scale forecast evaluation research papers.(Cramer et al. 2022; Meakin et al. 2022; Sherratt et al. 2023; Wolffram et al. 2023) The general rule of thumb is that it does a reasonable job comparing models where there is never less than, say, 50% overlap in the targets predicted by any pair of models.\nHere is a table of relative skill scores for WIS.\n\nscore_model_out(\n  covid_forecasts, \n  covid_oracle_output,\n  metrics = \"wis\",\n  relative_metrics = \"wis\"\n  ) |&gt; \n  arrange(wis_relative_skill)\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\n\nWarning in wilcox.test.default(values_x, values_y, paired = TRUE): cannot\ncompute exact p-value with ties\n\n\nKey: &lt;model_id&gt;\n                        model_id         wis wis_relative_skill\n                          &lt;char&gt;       &lt;num&gt;              &lt;num&gt;\n 1: CFA_Pyrenew-Pyrenew_HE_COVID   15.643224          0.6617223\n 2:        UGA_flucast-INFLAenza   19.606728          0.6992593\n 3:            CovidHub-ensemble   41.165853          0.7351364\n 4: CFA_Pyrenew-PyrenewHEW_COVID    7.565851          0.7683668\n 5:             UMass-ar6_pooled   47.128999          0.8615963\n 6:                   UMass-gbqr   51.001652          0.9039621\n 7:            CEPH-Rtrend_covid   52.582203          0.9575737\n 8:     NEU_ISI-AdaptiveEnsemble   59.836005          0.9580200\n 9:                 OHT_JHU-nbxd   57.425689          0.9616374\n10:            CovidHub-baseline   59.678144          1.0116854\n11:               CMU-TimeSeries   63.431791          1.0140982\n12: CFA_Pyrenew-Pyrenew_HW_COVID   11.988136          1.0340812\n13:                 Metaculus-cp 1315.160817          1.0370798\n14:       JHU_CSSE-CSSE_Ensemble   64.317330          1.0621682\n15:          Google_SAI-Ensemble   25.710362          1.0696303\n16:  CFA_Pyrenew-Pyrenew_H_COVID   40.731989          1.1823656\n17:             MOBS-GLEAM_COVID   64.493753          1.2589497\n18:              UM-DeepOutbreak   71.664888          1.2608713\n19:         CMU-climate_baseline  130.379998          2.3539375\n\n\nNote that the ordering of the wis column does not align perfectly with the wis_relative_skill column. This is due to the “adjustment” of the relative skill based on which forecasts were made by each model.\nTo make a more fair comparison, let’s subset to only include models that have submitted at least 60% of the maximum possible number of predictions for the season.\n\n## 33 dates, 4 horizons, 53 locations, 60%\nthreshold_60pct &lt;- 33 * 4 * 53 * 0.6\nmodel_subset &lt;- covid_forecasts |&gt; \n  filter(output_type_id == 0.5) |&gt; \n  group_by(model_id) |&gt; \n  summarize(targets = n()) |&gt; \n  filter(targets &gt; threshold_60pct) |&gt; \n  pull(model_id)\n\ncovid_forecasts |&gt; \n  filter(model_id %in% model_subset) |&gt; \n  score_model_out(\n    covid_oracle_output,\n    metrics = \"wis\",\n    relative_metrics = \"wis\"\n  ) |&gt; \n  arrange(wis_relative_skill)\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\n\nKey: &lt;model_id&gt;\n                    model_id       wis wis_relative_skill\n                      &lt;char&gt;     &lt;num&gt;              &lt;num&gt;\n 1:        CovidHub-ensemble  41.16585          0.6952576\n 2:         UMass-ar6_pooled  47.12900          0.8067576\n 3:               UMass-gbqr  51.00165          0.8734806\n 4:        CEPH-Rtrend_covid  52.58220          0.8871788\n 5: NEU_ISI-AdaptiveEnsemble  59.83600          0.8967471\n 6:             OHT_JHU-nbxd  57.42569          0.9010345\n 7:        CovidHub-baseline  59.67814          0.9715371\n 8:           CMU-TimeSeries  63.43179          1.0355758\n 9:          UM-DeepOutbreak  71.66489          1.0776110\n10:         MOBS-GLEAM_COVID  64.49375          1.2025614\n11:     CMU-climate_baseline 130.38000          2.1838601\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nLooking at the relative and absolute WIS scores from the table above, and the number of total forecasts for each model, what are some of your take-aways from the analysis above?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe ensemble forecast is the most accurate model overall, by fairly large margin, close to 20% in terms of absolute WIS and 10% in terms of relative skill.\nThe CovidHub-baseline has basically “average” performance, with a relative WIS score of 0.96.\nFive individual models have better performance than the baseline and four have worse performance than the baseline.\nThe worst-performing model is the CMU-climate_baseline. This is designed to be a “seasonal average” model, and covid isn’t that seasonal at the moment, so it’s not super surprising that this is a bad model.\nTying back to approaches from the Renewal equation session:\n\nThe CEPH-Rtrend_covid model is “A renewal equation method based on Bayesian estimation of Rt from hospitalization data.” It wasn’t as good as pure statistical and ML approaches, but was better than the baseline.\nThe MOBS-GLEAM_COVID model is a mechanistic model (see description above) and was worse than the baseline.\n\n\n\n\n\nSince the counts at the national level are likely to dominate the absolute scale of average WIS scores, you could make an argument to subset to only the states and evaluate based on that. Here is the analysis above, on that subset:\n\ncovid_forecasts |&gt; \n  filter(model_id %in% model_subset,\n         location != \"US\") |&gt; \n  score_model_out(\n    covid_oracle_output,\n    metrics = \"wis\",\n    relative_metrics = \"wis\"\n  ) |&gt; \n  arrange(wis_relative_skill)\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\n\nKey: &lt;model_id&gt;\n                    model_id      wis wis_relative_skill\n                      &lt;char&gt;    &lt;num&gt;              &lt;num&gt;\n 1:        CovidHub-ensemble 25.64862          0.7289393\n 2:         UMass-ar6_pooled 28.95864          0.8191001\n 3:               UMass-gbqr 29.05540          0.8218274\n 4:        CEPH-Rtrend_covid 31.83277          0.9039557\n 5:        CovidHub-baseline 34.29734          0.9441114\n 6:           CMU-TimeSeries 33.94338          0.9589511\n 7:             OHT_JHU-nbxd 36.27410          0.9644199\n 8: NEU_ISI-AdaptiveEnsemble 39.92454          1.0457292\n 9:          UM-DeepOutbreak 41.43688          1.0521645\n10:         MOBS-GLEAM_COVID 36.18229          1.1470630\n11:     CMU-climate_baseline 71.84392          2.0458225\n\n\nThe results above haven’t changed much, although the CMU-TimeSeries and OHT_JHU-nbxd swapped places in the final rankings.\n\n\nLog-scale scores\nWe can also score on the logarithmic scale. This can be useful if we are interested in the relative performance of the model at different scales of the data, for example if we are interested in the model’s performance at capturing the exponential growth phase of the epidemic.(Bosse et al. 2023) In some sense, scoring in this way can be an approximation of scoring the effective reproduction number estimates. Doing this directly can be difficult as the effective reproduction number is a latent variable and so we cannot directly score it. Additionally, it can make sense to score a target (and make it easier to compare scores) when the variance of the scores is stabilized.\nTo implement the log-scale scoring, we can simply log-transform the observations and forecasted values (with an offset, to account for zeroes in the data).\n\ncovid_forecasts |&gt; \n  filter(model_id %in% model_subset) |&gt; \n  mutate(value = log(value+1)) |&gt; \n  score_model_out(\n    covid_oracle_output |&gt; \n      mutate(oracle_value = log(oracle_value+1)),\n    metrics = \"wis\",\n    relative_metrics = \"wis\"\n  ) |&gt; \n  arrange(wis_relative_skill)\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\n\nKey: &lt;model_id&gt;\n                    model_id       wis wis_relative_skill\n                      &lt;char&gt;     &lt;num&gt;              &lt;num&gt;\n 1:        CovidHub-ensemble 0.1935312          0.7189940\n 2:               UMass-gbqr 0.2188057          0.8143009\n 3:           CMU-TimeSeries 0.2228692          0.8209956\n 4:         UMass-ar6_pooled 0.2285401          0.8474079\n 5:        CEPH-Rtrend_covid 0.2588151          0.9592896\n 6:             OHT_JHU-nbxd 0.2679237          0.9827550\n 7: NEU_ISI-AdaptiveEnsemble 0.2619506          1.0265103\n 8:        CovidHub-baseline 0.2922600          1.0798208\n 9:         MOBS-GLEAM_COVID 0.2953681          1.1490552\n10:          UM-DeepOutbreak 0.3609916          1.3719314\n11:     CMU-climate_baseline 0.4036584          1.4902983\n\n\nNote that this rescaling of the data prior to scoring results in a more substantial change in the evaluation. While the ensemble remains the most accurate model, now five models are better than the baseline, and the UMass-gbqr model is the most accurate individual model (just barely edging out the CMU-TimeSeries model.)\n\n\nPrediction interval coverage\nThe above comparisons focus on WIS and relative WIS, but in our collaborations with federal, state, and local epidemiologists, we have often found that the metric of prediction interval coverage rates is a useful tool for communicating the reliability of a set of forecasts. Prediction interval coverage measures the fraction of prediction intervals at a given levels that cover the truth. For example, if a model is well-calibrated, 50% PIs should cover the truth around 50% of the time, 90% PIs should cover the truth 90% of the time, etc… This is a key part of the forecasting paradigm mentioned in the Forecast Evaluation session is that we want to maximize sharpness subject to calibration. Prediction interval coverage rates help assess calibration. Prediction interval widths can help assess sharpness.\n\n\n\n\n\n\nTip\n\n\n\nPrediction interval coverage is not a proper score. For example, to get perfect 90% PI coverage, you could always make 9 out of every 10 of your intervals infinitely wide and the last one infinitely small. Et voila! 90% PI coverage achieved.\n\n\nBut, assuming that modelers aren’t trying to game the scores, it can generally be a useful and interpretable metric in practice. Let’s add some PI coverage metrics to our (unscaled) results from before.\n\ncovid_forecasts |&gt; \n  filter(model_id %in% model_subset) |&gt; \n  score_model_out(\n    covid_oracle_output,\n    metrics = c(\"wis\", \"interval_coverage_50\", \"interval_coverage_90\")\n  ) |&gt; \n  arrange(interval_coverage_90) |&gt; \n  knitr::kable(digits = 3)\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\n\n\n\n\n\n\n\n\n\n\nmodel_id\nwis\ninterval_coverage_50\ninterval_coverage_90\n\n\n\n\nMOBS-GLEAM_COVID\n64.494\n0.303\n0.638\n\n\nOHT_JHU-nbxd\n57.426\n0.349\n0.685\n\n\nNEU_ISI-AdaptiveEnsemble\n59.836\n0.357\n0.700\n\n\nCEPH-Rtrend_covid\n52.582\n0.463\n0.825\n\n\nUMass-gbqr\n51.002\n0.459\n0.836\n\n\nCovidHub-baseline\n59.678\n0.627\n0.848\n\n\nCMU-TimeSeries\n63.432\n0.508\n0.900\n\n\nCovidHub-ensemble\n41.166\n0.561\n0.909\n\n\nUMass-ar6_pooled\n47.129\n0.576\n0.916\n\n\nCMU-climate_baseline\n130.380\n0.492\n0.934\n\n\nUM-DeepOutbreak\n71.665\n0.639\n0.940\n\n\n\n\n\nThese results are sorted by the 90% PI coverage, but note that lower is not necessarily “better” here. What is good is having interval_coverage_90 scores that are close to 0.90. A few models (CMU-TimeSeries and CMU-climate_baseline) all have coverage rates within 5% of the nominal level for both 50% and 90% PIs.\n\n\n\n\n\n\nTip\n\n\n\nNote, this is a nice example of a model (CMU-climate_baseline) being well calibrated (near-nominal PI coverage) but not having good accuracy (bad/high WIS on average).",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#model-comparison-by-horizon-and-forecast-date",
    "href": "sessions/real-world-forecasts.html#model-comparison-by-horizon-and-forecast-date",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Model comparison by horizon and forecast date",
    "text": "Model comparison by horizon and forecast date\nAs we discussed in the session on evaluating forecasts, overall comparisons can miss details about ways that models perform across different forecast dimensions. Let’s run an analysis looking at the performance of model by horizon and then by forecast date. For now, we will keep the focus on the 10 models that made at least 60% of possible predictions, and we will only evaluate state-level forecasts.\n\nscores_by_horizon &lt;- covid_forecasts |&gt; \n  filter(model_id %in% model_subset,\n         location != \"US\") |&gt; \n  score_model_out(\n    covid_oracle_output,\n    metrics = \"wis\",\n    relative_metrics = \"wis\",\n    by = c(\"model_id\", \"horizon\")\n  ) \n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\np &lt;- ggplot(scores_by_horizon)+\n  geom_line(aes(x = horizon, y = wis, color = model_id))\nplotly::ggplotly(p)\n\n\n\n\n\nFor looking by reference_date, we will also plot the relative WIS.\n\nscores_by_reference_date &lt;- covid_forecasts |&gt; \n  filter(model_id %in% model_subset,\n         location != \"US\") |&gt; \n  score_model_out(\n    covid_oracle_output,\n    metrics = \"wis\",\n    relative_metrics = \"wis\",\n    by = c(\"model_id\", \"reference_date\")\n  ) |&gt; \n  tidyr::pivot_longer(cols = c(wis, wis_relative_skill), \n                      names_to = \"metric\",\n                      values_to = \"score\")\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\np &lt;- ggplot(scores_by_reference_date)+\n  geom_line(aes(x = reference_date, y = score, color = model_id)) +\n  facet_grid(metric~., scales = \"free_y\")\nplotly::ggplotly(p)\n\n\n\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nBy horizon and reference date, how variable are the models in their accuracy?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn general, the models seem fairly consistent in their performance. There are few “crossings” in the by horizon plot, suggesting that by and large models that are accurate at short-term horizons are also accurate at the longer horizons. There is much more variability by date, although it still seems that with a few exceptions, models aren’t jumping around too dramatically in their relative (or absolute level) WIS.",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#build-unweighted-lop-ensembles",
    "href": "sessions/real-world-forecasts.html#build-unweighted-lop-ensembles",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Build unweighted LOP ensembles",
    "text": "Build unweighted LOP ensembles\nLet’s start with the “easy” part, building the unweighted ensembles.\n\n\n\n\n\n\nLOP details\n\n\n\n\n\nNote that when we build LOPs with quantile based forecasts, the algorithm needs to approximate a full distribution (the tails are not well-defined in a quantile representation). There are some assumptions made under the hood about how this is approximated. For the really gory details, you can check out the hubEnsembles documentation\n\n\n\nNote that trying to build all of the forecasts in one call to linear_pool() exceeded the memory on your instructors laptop, so we will do a purrr::map_dfr() “loop” to do this one reference_date at a time. (This code takes a few minutes to run.)\n\n# Get a list of unique reference dates, removing the last one\nreference_dates &lt;- covid_forecasts |&gt; \n  filter(reference_date != as.Date(\"2025-07-12\")) |&gt; \n  pull(reference_date) |&gt; \n  unique()\n\n# Map over each date and apply linear_pool separately\nlop_unweighted_all &lt;- purrr::map_dfr(\n  reference_dates, \n  function(date) {\n    covid_forecasts |&gt; \n      filter(model_id != \"CovidHub-ensemble\", \n             reference_date == date) |&gt; \n      hubEnsembles::linear_pool(model_id = \"lop_unweighted_all\")\n})\n\n\nlop_unweighted_select &lt;- purrr::map_dfr(\n  reference_dates, \n  function(date) {\n    covid_forecasts |&gt; \n      filter(model_id %in% model_subset,\n             model_id != \"CovidHub-ensemble\", \n             reference_date == date) |&gt; \n      hubEnsembles::linear_pool(model_id = \"lop_unweighted_select\")\n})",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#build-weighted-ensemble",
    "href": "sessions/real-world-forecasts.html#build-weighted-ensemble",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Build weighted ensemble",
    "text": "Build weighted ensemble\nInverse WIS weighting is a simple method that weights the forecasts by the inverse of their WIS over some period. Note that identifying what this period should be in order to produce the best forecasts is not straightforward as predictive performance may vary over time if, say, some models are better at making predictions during different parts of an outbreak. For example, some models might be good at predicting exponential growth at the beginning of an outbreak but not as good at seeing the slowdown.\nThe main benefit of WIS weighting over other methods is that it is simple to understand and implement. However, it does not optimise the weights directly to produce the best forecasts. It relies on the hope that giving more weight to better performing models yields a better ensemble.\nWe will use the WIS scores that we have computed above to generate weights for our ensembles. The weights for one reference_date will be based on the inverse of the cumulative average WIS achieved by the model up to the week prior to the current reference_date.\n\n\n\n\n\n\nCaution\n\n\n\nNote that since our scores are computed based on data available at the end of the season, this is kind of “cheating”, since we’re using information that would not have been available to us in real time. If we were to do this entirely correctly, we would, for each week, compute the scores based on the available data at that time.\n\n\nLet’s start by calculating WIS values by model and target_end_date. This will allow us to see what scores are available for a model’s forecasts that it had for dates prior to the current forecast date.\n\nscores_by_target_date &lt;- covid_forecasts |&gt; \n  filter(model_id %in% model_subset,\n         location != \"US\") |&gt; \n  score_model_out(\n    covid_oracle_output,\n    metrics = \"wis\",\n    by = c(\"model_id\", \"target_end_date\")\n  )\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\nscores_by_target_date\n\n              model_id target_end_date       wis\n                &lt;char&gt;          &lt;Date&gt;     &lt;num&gt;\n  1: CEPH-Rtrend_covid      2024-11-23 26.908679\n  2: CEPH-Rtrend_covid      2024-11-30 28.499946\n  3: CEPH-Rtrend_covid      2024-12-07 30.215181\n  4: CEPH-Rtrend_covid      2024-12-14 31.887136\n  5: CEPH-Rtrend_covid      2024-12-21 36.592749\n ---                                            \n352:        UMass-gbqr      2025-06-07 11.916627\n353:        UMass-gbqr      2025-06-14 12.991709\n354:        UMass-gbqr      2025-06-21 12.173811\n355:        UMass-gbqr      2025-06-28 10.808055\n356:        UMass-gbqr      2025-07-05  9.590668\n\n\nNow, we will compute a cumulative average WIS up to a given date for all models except the ensemble, which we are not going to include in our ensemble.\n\ncum_wis &lt;- scores_by_target_date |&gt;\n  filter(model_id != \"CovidHub-ensemble\") |&gt; \n  arrange(model_id, target_end_date) |&gt; \n  group_by(model_id) |&gt; \n  mutate(\n    cumulative_mean_wis = cummean(wis), ## cumulative mean WIS including current week\n    cumulative_mean_wis_lag1 = lag(cumulative_mean_wis)\n    ) |&gt; \n  ungroup()\ncum_wis\n\n# A tibble: 323 × 5\n   model_id     target_end_date   wis cumulative_mean_wis cumulative_mean_wis_…¹\n   &lt;chr&gt;        &lt;date&gt;          &lt;dbl&gt;               &lt;dbl&gt;                  &lt;dbl&gt;\n 1 CEPH-Rtrend… 2024-11-23       26.9                26.9                   NA  \n 2 CEPH-Rtrend… 2024-11-30       28.5                27.7                   26.9\n 3 CEPH-Rtrend… 2024-12-07       30.2                28.5                   27.7\n 4 CEPH-Rtrend… 2024-12-14       31.9                29.4                   28.5\n 5 CEPH-Rtrend… 2024-12-21       36.6                30.8                   29.4\n 6 CEPH-Rtrend… 2024-12-28       61.6                36.0                   30.8\n 7 CEPH-Rtrend… 2025-01-04       89.2                43.6                   36.0\n 8 CEPH-Rtrend… 2025-01-11       52.2                44.6                   43.6\n 9 CEPH-Rtrend… 2025-01-18       65.7                47.0                   44.6\n10 CEPH-Rtrend… 2025-01-25       91.7                51.4                   47.0\n# ℹ 313 more rows\n# ℹ abbreviated name: ¹​cumulative_mean_wis_lag1\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that we also compute a cumulative_mean_wis_lag1, which is the mean up through the previous week. This is to inject a bit of realism: how could we know in real-time how our forecast did for the current week?\nSo the cumulative_mean_wis_lag1 is the cumulative mean WIS that we could have observed at the target_end_date. (This all still assumes that whatever data we observed at the target_end_date was complete and never revised.)\n\n\nWe will proceed by computing the inverse mean wis and using those to derive the weights.\n\nweights_per_model &lt;- cum_wis |&gt; \n  mutate(inv_wis = 1 / cumulative_mean_wis_lag1) |&gt;\n  group_by(target_end_date) |&gt; \n  mutate(inv_wis_total_by_date = sum(inv_wis, na.rm = TRUE)) |&gt;\n  ungroup() |&gt; \n  mutate(weight = inv_wis / inv_wis_total_by_date) |&gt; ## this normalises the weights to sum to 1\n  select(model_id, target_end_date, weight)\n\nAnd we will put in place a few small fixes to the weights:\n\nassign equal weights for the first two weeks. (There are missing forecasts here, and we haven’t learned much about the model performance yet anyways.)\nchange target_end_date to reference_date because now we want the weights computed up to that target_end_date to be applied on that reference_date.\nset to 0 a missing weight for the NEU_ISI-AdaptiveEnsemble.\n\n\n## assign equal weights for the first week\nfirst_week_idx &lt;- weights_per_model$target_end_date == as.Date(\"2024-11-23\")\nweights_per_model[first_week_idx, \"weight\"] &lt;- 1/sum(first_week_idx)\n\nsecond_week_idx &lt;- weights_per_model$target_end_date == as.Date(\"2024-11-30\")\nweights_per_model[second_week_idx, \"weight\"] &lt;- 1/sum(second_week_idx)\n\nweights_per_model &lt;- rename(weights_per_model, reference_date = target_end_date)\n\nweights_per_model$weight[is.na(weights_per_model$weight)] &lt;- 0\n\nAs always, it is good to plot your data as a sanity check. Here are the model weights plotted over time. For each week (x-axis), the weights assigned to each model are shown in a stacked bar plot, with the height of the bar equal to the weight of the model.\n\nplot_weights &lt;- ggplot(weights_per_model) +\n  geom_col(aes(x = reference_date, y = weight, fill = model_id))\nplotly::ggplotly(plot_weights)\n\n\n\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nLooking at the above plot, what is the “story” that the weights tell when plotted over time? Which models did the ensemble like and dislike over time and how did it change? (Remember, in the first two weeks, all models received equal weight.)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting in the third week, three models were assigned less weight than others: CMU-climate-baseline, CMU-TimeSeries and COVIDHub-baseline. Over time, CMU-TimeSeries and COVIDHub-baseline earned back some “trust” with higher weights, but CMU-climate-baseline stayed with a low weight.\nIn general, the weights did not change much after the first few weeks (except in the last week when UM-DeepOutbreak wasn’t submitted.)\n\n\n\nNow that we have the weights and we’ve checked them, let’s start building some ensembles! First, we will build the median ensembles, both weighted and unweighted..\n\n## build the weighted ensemble\nmedian_inv_wis_weighted_select &lt;- covid_forecasts |&gt; \n  filter(model_id %in% model_subset,\n         model_id != \"CovidHub-ensemble\") |&gt; \n  simple_ensemble(weights = weights_per_model, \n                  agg_fun = median,\n                  model_id = \"median_inv_wis_weighted_select\")\n\n## build unweighted select median ensemble\nmedian_unweighted_select &lt;- covid_forecasts |&gt; \n  filter(model_id %in% model_subset,\n         model_id != \"CovidHub-ensemble\") |&gt; \n  simple_ensemble(agg_fun = median,\n                  model_id = \"median_unweighted_select\")\n\nAnd then we will build the inverse WIS weighted LOP ensemble.\n\n\n\n\n\n\nCaution\n\n\n\nIt takes several minutes for the LOP ensembles to be built if you are running the code on your machine.\n\n\n\nlop_inv_wis_weighted_select &lt;- purrr::map_dfr(\n  reference_dates, \n  function(date) {\n    covid_forecasts |&gt; \n      filter(model_id %in% model_subset,\n             model_id != \"CovidHub-ensemble\", \n             reference_date == date) |&gt; \n      linear_pool(model_id = \"lop_inv_wis_weighted_select\",\n                  weights = weights_per_model)\n})",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#compare-all-ensembles",
    "href": "sessions/real-world-forecasts.html#compare-all-ensembles",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Compare all ensembles",
    "text": "Compare all ensembles\nWe will compare the performance of all ensembles just at the state level, since the scores by reference date were only computed at the state level.\n\ncovid_forecasts_w_ensembles &lt;- bind_rows(\n  covid_forecasts,\n  lop_unweighted_all,\n  lop_unweighted_select,\n  lop_inv_wis_weighted_select,\n  median_inv_wis_weighted_select,\n  median_unweighted_select\n)\n\nmodel_subset_ens &lt;- c(\"CovidHub-ensemble\", \n                      \"lop_unweighted_all\",\n                      \"lop_unweighted_select\",\n                      \"median_inv_wis_weighted_select\",\n                      \"median_unweighted_select\",\n                      \"lop_inv_wis_weighted_select\")\n\ncovid_forecasts_w_ensembles |&gt; \n  filter(model_id %in% model_subset_ens,\n         location != \"US\") |&gt; \n  score_model_out(\n    covid_oracle_output\n  ) |&gt; \n  arrange(wis) |&gt; \n  knitr::kable(digits = 2)\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_id\nwis\noverprediction\nunderprediction\ndispersion\nbias\ninterval_coverage_50\ninterval_coverage_90\nae_median\n\n\n\n\nCovidHub-ensemble\n25.65\n4.67\n9.96\n11.02\n-0.10\n0.56\n0.91\n38.71\n\n\nmedian_unweighted_select\n25.77\n4.62\n9.08\n12.08\n-0.06\n0.60\n0.92\n38.47\n\n\nmedian_inv_wis_weighted_select\n26.85\n5.09\n9.88\n11.87\n-0.07\n0.59\n0.91\n40.34\n\n\nlop_unweighted_all\n27.15\n3.38\n7.76\n16.00\n-0.06\n0.68\n0.99\n38.06\n\n\nlop_inv_wis_weighted_select\n27.21\n3.38\n9.01\n14.82\n-0.09\n0.65\n0.97\n38.93\n\n\nlop_unweighted_select\n27.32\n3.34\n7.84\n16.14\n-0.06\n0.68\n0.99\n38.22\n\n\n\n\n\nRemember, in the above table, the CovidHub-ensemble is kind of like median-unweighted-all because it uses a median ensemble of all models available in every week without weights. Overall, the ensembles have fairly similar average WIS values across all dates and states. Using a median unweighted ensemble of frequently submitting models marginally outperformed the actual CovidHub-ensemble.\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nWhy do you think weighting does not improve the forecasts more?\nSeeing these patterns, and the plots of the weights above, is there anything you might change if you were given the opportunity to compute the weights in a different way?\nWrite down your thoughts about these questions and compare answers with a neighbor.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWhy aren’t weighted ensembles better?\nWhen model performance varies a lot from one week to the next, it may be hard to estimate/anticipate/predict which model should get more weight in a given week.\n\n\nWhat might your consider changing about the weights?\nThere isn’t one right answer here, but some things that you might consider to do differently are:\n\nFiguring out a way to encourage the model to stay closer to equal weights early on. The weights are being perhaps overly influenced by small sample sizes there.\nThinking about using a rolling window of mean WIS to compute weights rather than overall cumulative. This could be good, but also assumes that recent performance will predict future performance, which is not always true!\n\n\n\n\n\nIt does not seem like weighting adds a lot of value to these forecasts, although perhaps with additional data on model performance the weights could be improved.\nWe can also look at how the ensemble performance varied over time.\n\nensemble_scores_by_reference_date &lt;- covid_forecasts_w_ensembles |&gt; \n  filter(model_id %in% model_subset_ens,\n         location != \"US\") |&gt; \n  score_model_out(\n    covid_oracle_output,\n    metrics = \"wis\",\n    by = c(\"model_id\", \"reference_date\")\n  )\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\np &lt;- ggplot(ensemble_scores_by_reference_date) +\n  geom_line(aes(x = reference_date, y = wis, color = model_id))\nplotly::ggplotly(p)\n\n\n\n\n\nIt is interesting to note how while the LOP ensembles did slightly worse overall, they actually had slightly more accurate forecasts near the peak of the season, especially in the weeks where the scores were the highest.",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#challenge",
    "href": "sessions/real-world-forecasts.html#challenge",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Challenge",
    "text": "Challenge\n\nOnce you start looking at lots of plots of forecast scores, it is easy to fall into the trap of not looking at the actual forecasts. Looking at the scores of the ensemble forecasts by date, it looks like there was one week when the CovidHub-ensemble did much worse than other forecasts. Investigate this week in particular. What did the forecasts look like that week and what was anomalous about it?\nIn the evaluation of the real-world models, we subset to only models that had submitted a large fraction of forecasts. But this omits some interesting models, including the model from Metaculus, a human-judgment forecast aggregation company, and some models from the CDC Center for Forecasting and Outbreak Analytics (CFA). Adapt the code above to conduct a targeted and fair evaluation of the forecasts made by those models.\nLook at the list of hubverse-style hubs. Using the code above, try to replicate some of the analyses in this session for a different hub. What challenges and obstacles do you run into?",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/real-world-forecasts.html#methods-in-practice",
    "href": "sessions/real-world-forecasts.html#methods-in-practice",
    "title": "Evaluating real-world outbreak forecasts",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nRay et al. (2023) evaluates the performance of ensemble forecasts (including some weighted ensembles) in predicting COVID-19 cases, hospitalization and deaths, both with weighted and unweighted ensembles.\nSherratt et al. (2023) investigates the performance of different ensembles in the European COVID-19 Forecast Hub.",
    "crumbs": [
      "Evaluating real-world outbreak forecasts"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html",
    "href": "sessions/joint-nowcasting.html",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "In the last session we introduced the idea of nowcasting using a simple model. However, this approach had problems: we didn’t fully account for uncertainty, or for example observation error in the primary events, and it’s not a fully generative model of the data reporting process. And as we saw, if we get the delay distribution wrong, we can get the nowcast very wrong.\nA better approach is to jointly estimate the delay distribution together with the nowcast. We can do this by using information from multiple snapshots of the data as it changes over time (using a data structure called the “reporting triangle”). In this session, we’ll introduce this approach to joint estimation in nowcasting. At the end we’ll then demonstrate a way to combine this with our previous work estimating the reproduction number, steadily improving our real time outbreak model.\n\n\n\nIntroduction to joint estimation of delays and nowcasts\n\n\n\n\nThis session aims to introduce how to do nowcasting if the reporting delay distribution is unknown.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/joint-nowcasting.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models. Finally, we set an option to not warn about the partial definition of initial conditions.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)\noptions(cmdstanr_warn_inits = FALSE)",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#slides",
    "href": "sessions/joint-nowcasting.html#slides",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "Introduction to joint estimation of delays and nowcasts",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#objectives",
    "href": "sessions/joint-nowcasting.html#objectives",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "This session aims to introduce how to do nowcasting if the reporting delay distribution is unknown.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/joint-nowcasting.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models. Finally, we set an option to not warn about the partial definition of initial conditions.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)\noptions(cmdstanr_warn_inits = FALSE)",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#source-file",
    "href": "sessions/joint-nowcasting.html#source-file",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "The source file of this session is located at sessions/joint-nowcasting.qmd.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#libraries-used",
    "href": "sessions/joint-nowcasting.html#libraries-used",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#initialisation",
    "href": "sessions/joint-nowcasting.html#initialisation",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models. Finally, we set an option to not warn about the partial definition of initial conditions.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)\noptions(cmdstanr_warn_inits = FALSE)",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#motivation",
    "href": "sessions/joint-nowcasting.html#motivation",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Motivation",
    "text": "Motivation\nSo far we have assumed that the delay distribution is known. In practice, this is often not the case and we need to estimate it from the data. As we discussed in the session on biases in delay distributions, this can be done using individual data and then passing this estimate to a simple nowcasting model. However, this has the disadvantage that the nowcasting model does not take into account the uncertainty in the delay distribution or observation error of the primary events (and potentially some other issues we will cover in this session).\nIn the nowcasting concepts session we also saw that getting the delay distribution wrong can lead to very poor nowcasts.\nA better approach is to jointly estimate the delay distribution together with the nowcast. This builds on the data simulation approach we used in the concepts session, but now we model the entire process at the population level with observation error.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#extending-our-data-simulation-approach",
    "href": "sessions/joint-nowcasting.html#extending-our-data-simulation-approach",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Extending our data simulation approach",
    "text": "Extending our data simulation approach\nRecall from the nowcasting concepts session that we simulated individual reporting delays and then aggregated them. Now we’ll simulate the same process but at the population level with observation error - for the same reasons we made this change in the convolutions session.\nFirst, let’s generate our simulated onset dataset as before:\n\ngen_time_pmf &lt;- make_gen_time_pmf()\nip_pmf &lt;- make_ip_pmf()\nonset_df &lt;- simulate_onsets(\n  make_daily_infections(infection_times), gen_time_pmf, ip_pmf\n)\nhead(onset_df)\n\n# A tibble: 6 × 3\n    day onsets infections\n  &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;\n1     1      0          0\n2     2      0          1\n3     3      0          0\n4     4      1          2\n5     5      0          1\n6     6      0          1\n\ncutoff &lt;- 71\n\nWe then need to simulate the reporting delays:\n\nreporting_delay_pmf &lt;- censored_delay_pmf(\n  rlnorm, max = 15, meanlog = 1, sdlog = 0.5\n)\nplot(reporting_delay_pmf)\n\n\n\n\n\n\n\n\nWe can then simulate the data by day, onset day, and reporting day by applying the reporting delay distribution to the onsets (notice how this is nearly identical to the convolution we saw in the convolutions session except that we are not applying the sum):\n\nreporting_triangle &lt;- onset_df |&gt;\n  filter(day &lt; cutoff) |&gt;\n  mutate(\n    reporting_delay = list(\n      tibble(d = 0:15, reporting_delay = reporting_delay_pmf)\n    )\n  ) |&gt;\n  unnest(reporting_delay) |&gt;\n  mutate(\n    reported_onsets = rpois(n(), onsets * reporting_delay)\n  ) |&gt;\n  mutate(reported_day = day + d)\n\ntail(reporting_triangle)\n\n# A tibble: 6 × 7\n    day onsets infections     d reporting_delay reported_onsets reported_day\n  &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;table[1d]&gt;               &lt;int&gt;        &lt;dbl&gt;\n1    70     76        115    10 0.0028236523                  0           80\n2    70     76        115    11 0.0014853431                  0           81\n3    70     76        115    12 0.0007961839                  0           82\n4    70     76        115    13 0.0004601063                  0           83\n5    70     76        115    14 0.0002700624                  0           84\n6    70     76        115    15 0.0001920444                  0           85\n\n\nWe also apply the Poisson observation error to the reported onsets to capture our uncertainty in the reporting process (remember we have lost uncertainty in the onsets as we are not individually simulating the reporting delays).\nTo recover the onsets onsets by day we group by day and then sum reported onsets across report days.\n\nnoisy_onsets_df &lt;- reporting_triangle |&gt;\n  summarise(noisy_onsets = sum(reported_onsets), .by = day)\n\ntail(noisy_onsets_df)\n\n# A tibble: 6 × 2\n    day noisy_onsets\n  &lt;dbl&gt;        &lt;int&gt;\n1    65           66\n2    66           77\n3    67           71\n4    68           84\n5    69           84\n6    70           81\n\n\nAs we only observed reported cases up to the current day we need to filter it to only include the data we have observed:\n\nfiltered_reporting_triangle &lt;- reporting_triangle |&gt;\n  filter(reported_day &lt;= max(day))\n\ntail(noisy_onsets_df)\n\n# A tibble: 6 × 2\n    day noisy_onsets\n  &lt;dbl&gt;        &lt;int&gt;\n1    65           66\n2    66           77\n3    67           71\n4    68           84\n5    69           84\n6    70           81\n\n\nFinally, we sum this to get the counts we actually observe. This is the same as the date we corrected for right truncation in the nowcasting concepts session.\n\navailable_onsets &lt;- filtered_reporting_triangle |&gt;\n  summarise(available_onsets = sum(reported_onsets), .by = day)\n\ntail(available_onsets)\n\n# A tibble: 6 × 2\n    day available_onsets\n  &lt;dbl&gt;            &lt;int&gt;\n1    65               61\n2    66               67\n3    67               44\n4    68               32\n5    69               14\n6    70                0",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#understanding-the-data-structure",
    "href": "sessions/joint-nowcasting.html#understanding-the-data-structure",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Understanding the data structure",
    "text": "Understanding the data structure\nNotice how this simulation creates a similar, but different, data structure than in the concepts session.\n\n\n\n\n\n\nThree-dimensional epidemiological data\n\n\n\nSo far we’ve worked with data that has two dimensions. Such as:\n\nOnset day: when symptoms began\nOnset counts: the number of cases that occurred on each day\n\nNow we are introducing a third dimension:\n\nReport day: when the case enters our surveillance system\n\nThis means that to recover the true onset counts we need to sum across the third dimension and we may not be able to do this if all the data has not been reported yet (i.e. we have right truncation in our data). This is just a reformulation of the nowcasting problem we saw in the nowcasting concepts session.\n\n\nThis richer data structure contains information about both the delay distribution and the final expected counts so we can use it to jointly estimate both which was not possible with the simpler data structure we used in the nowcasting concepts session.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#mathematical-formulation",
    "href": "sessions/joint-nowcasting.html#mathematical-formulation",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Mathematical formulation",
    "text": "Mathematical formulation\nWe can formalise this process mathematically. The total number of onsets on day \\(t\\) is: \\[\nN_{t} = \\sum_{d=0}^{D} n_{t,d}\n\\]\nwhere \\(n_{t,d}\\) is the number of onsets on day \\(t\\) that are reported on day \\(t+d\\), and \\(D\\) is the maximum delay.\nWe model each component as: \\[\n  n_{t,d} \\mid \\lambda_{t},p_{t,d} \\sim \\text{Poisson} \\left(\\lambda_{t} \\times p_{t,d} \\right),\\ t=1,...,T.\n\\]\nwhere:\n\n\\(\\lambda_{t}\\) is the expected number of onsets on day \\(t\\)\n\\(p_{t,d}\\) is the probability that an onset on day \\(t\\) is reported on day \\(t+d\\)\n\nThis approach jointly estimates the delay distribution (\\(p_{t,d}\\)) and the underlying onset counts (\\(\\lambda_{t}\\)) from the observed data.\n\n\n\n\n\n\nModelling options\n\n\n\nWe now have two main modelling options:\n\nHow we model the expected number of onsets \\(\\lambda_{t}\\)\nHow we model the probability of reporting \\(p_{t,d}\\)\n\nWe will explore these in the next section.\n\n\nA key insight is that we can split each \\(n_{t,d}\\) into observed and unobserved components:\n\\[n_{t,d} = n_{t,d}^{obs} + n_{t,d}^{miss}\\]\nwhere:\n\n\\(n_{t,d}^{obs}\\) is what we observe (when \\(t+d \\leq\\) current day)\n\\(n_{t,d}^{miss}\\) is what we need to estimate (when \\(t+d &gt;\\) current day)\n\nThe joint model uses the observed data to estimate both the delay distribution and the underlying onset counts, which then allows us to predict the missing entries.\n\n\n\n\n\n\nThe reporting triangle\n\n\n\n\n\nThis data structure is sometimes called the “reporting triangle” in the literature because when visualised as a matrix (with onset days as rows and reporting days as columns), the observed data (\\(n_{t,d}^{obs}\\)) creates a triangular shape.\n\n\n\nReporting triangle visualisation (by Johannes Bracher)\n\n\nNowcasting aims to complete this triangle by estimating the missing entries (\\(n_{t,d}^{miss}\\)).\n\n\n\nCompleted reporting triangle (by Johannes Bracher)\n\n\nOnce completed, we sum across the rows to get our nowcast of the true onset counts.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#fitting-the-joint-model",
    "href": "sessions/joint-nowcasting.html#fitting-the-joint-model",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Fitting the joint model",
    "text": "Fitting the joint model\nFor fitting the joint model specification follows our data simulation approach but we have to make choices about how to model the expected number of onsets \\(\\lambda_{t}\\) and the probability of reporting \\(p_{t,d}\\).\nAs usual, we start by loading the model:\n\njoint_mod &lt;- nfidd_cmdstan_model(\"joint-nowcast\")\njoint_mod\n\n 1: functions {\n 2:   #include \"functions/geometric_random_walk.stan\"\n 3:   #include \"functions/observe_onsets_with_delay.stan\"\n 4:   #include \"functions/combine_obs_with_predicted_obs_rng.stan\"\n 5: }\n 6: \n 7: data {\n 8:   int n;                // number of days\n 9:   int m;                // number of reports\n10:   array[n] int p;       // number of observations per day\n11:   array[m] int obs;     // observed symptom onsets\n12:   int d;                // number of reporting delays\n13: }\n14: \n15: transformed data{\n16:   array[n] int P = to_int(cumulative_sum(p));\n17:   array[n] int D = to_int(cumulative_sum(rep_array(d, n)));\n18: }\n19: \n20: parameters {\n21:   real&lt;lower=0&gt; init_onsets;\n22:   array[n-1] real rw_noise;\n23:   real&lt;lower=0&gt; rw_sd;\n24:   simplex[d] reporting_delay; // reporting delay distribution\n25: }\n26: \n27: transformed parameters {\n28:   array[n] real onsets = geometric_random_walk(init_onsets, rw_noise, rw_sd);\n29:   array[m] real onsets_by_report = observe_onsets_with_delay(onsets, reporting_delay, P, p);\n30: }\n31: \n32: model {\n33:   // Prior\n34:   init_onsets ~ normal(1, 5) T[0,];\n35:   rw_noise ~ std_normal();\n36:   rw_sd ~ normal(0, 0.05) T[0,];\n37:   reporting_delay ~ dirichlet(rep_vector(1, d));\n38:   // Likelihood\n39:   obs ~ poisson(onsets_by_report);\n40: }\n41: \n42: generated quantities {\n43:   array[d*n] real complete_onsets_by_report = observe_onsets_with_delay(onsets, reporting_delay, D, rep_array(d, n));\n44:   array[n] int nowcast = combine_obs_with_predicted_obs_rng(obs, complete_onsets_by_report, P, p, d, D);\n45: }\n46: \n\n\n\n\n\n\n\n\nModel details\n\n\n\n\n\nThis time we won’t go into details of the model. For now, it is important that you understand the concept, but as the models get more complex, we hope that you trust us that the model does what we describe above.\nOne thing to note is that we are now fitting the initial number of symptom onsets (init_onsets). This is different from earlier when we had to pass the initial number of infections (I0) as data. In most situations, this number would be unknown, so what we do here is closer to what one would do in the real world.\n\n\n\n\n\n\n\n\n\nTake two minutes\n\n\n\nWhat are the models we have picked for the onsets (\\(\\lambda_{t}\\)) and the reporting delay distribution (\\(p_{t,d}\\))?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\\(\\lambda_{t}\\) is modelled using a geometric random walk.\n\\(p_{t,d}\\) is modelled using a Dirichlet distribution (i.e. a multinomial distribution with a constraint that the sum of the probabilities is 1 - this allows the delay distribution to be flexible).\n\n\n\n\nWe then fit it to data:\n\njoint_data &lt;- list(\n  n = length(unique(filtered_reporting_triangle$day)), # number of days\n  m = nrow(filtered_reporting_triangle),               # number of reports\n  p = filtered_reporting_triangle |&gt;\n   group_by(day) |&gt;\n   filter(d == max(d)) |&gt;\n   mutate(d = d + 1) |&gt;\n   pull(d),            # number of observations per day\n  obs = filtered_reporting_triangle$reported_onsets,   # observed symptom onsets\n  d = 16               # number of reporting delays\n)\njoint_nowcast_fit &lt;- nfidd_sample(joint_mod, data = joint_data)\n\n\njoint_nowcast_fit\n\n    variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail\n lp__        527.20 527.41 6.89 6.62 515.24 538.13 1.00      924     1324\n init_onsets   0.76   0.71 0.32 0.29   0.34   1.34 1.00     2332     1329\n rw_noise[1]  -0.05  -0.04 0.97 0.94  -1.68   1.56 1.00     3224     1594\n rw_noise[2]   0.12   0.12 1.00 0.98  -1.51   1.78 1.00     2817     1480\n rw_noise[3]   0.23   0.22 0.93 0.93  -1.31   1.78 1.00     2885     1452\n rw_noise[4]   0.19   0.19 0.99 1.02  -1.34   1.80 1.00     3512     1419\n rw_noise[5]   0.32   0.31 0.98 0.97  -1.33   1.92 1.00     2631     1449\n rw_noise[6]   0.47   0.49 0.95 0.91  -1.09   2.08 1.00     3727     1736\n rw_noise[7]   0.10   0.09 0.97 1.03  -1.50   1.70 1.00     2856     1393\n rw_noise[8]  -0.26  -0.26 0.96 0.97  -1.79   1.35 1.00     2933     1594\n\n # showing 10 of 2348 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nOne benefit of this model is that, because we have decomposed the data into the reporting triangle, we can make a nowcast that uses the data we have available, augmented with predictions from the model. This should give us more accurate uncertainty estimates than the simple nowcasting models above (see stan/functions/combine_obs_with_predicted_obs_rng.stan but note the code is fairly involved).\n\n\nWe now extract this nowcast:\n\njoint_nowcast_onsets &lt;- joint_nowcast_fit |&gt;\n  gather_draws(nowcast[day]) |&gt;\n  ungroup() |&gt;\n  filter(.draw %in% sample(.draw, 100))\n\nFinally, we can plot the nowcast alongside the observed data:\n\nggplot(joint_nowcast_onsets, aes(x = day)) +\n  geom_col(\n    data = noisy_onsets_df, mapping = aes(y = noisy_onsets), alpha = 0.6\n  ) +\n  geom_line(mapping = aes(y = .value, group = .draw), alpha = 0.1) +\n  geom_point(data = available_onsets, mapping = aes(y = available_onsets))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nReminder: The points in this plot represent the data available when the nowcast was made (and so are truncated) whilst the bars represent the finally reported data (a perfect nowcast would exactly reproduce these).\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nLook back at the last three nowcasts. How do they compare? What are the advantages and disadvantages of each? Could we improve the nowcasts further?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe simple nowcast struggled to capture the generative process of the data and so produced poor nowcasts. The nowcast with the geometric random walk was better but still struggled to capture the generative process of the data. The joint nowcast was the best of the three as it properly handled the uncertainty and allowed us to fit the delay distribution versus relying on known delays.\nHowever, the joint nowcast is still quite simple (in the sense that no detailed mechanism or reporting process is being modelled) and so may struggle to capture more complex patterns in the data. In particular, the prior model for the geometric random walk assumes that onsets are the same as the previous day with some statistical noise. This may not be a good assumption in a rapidly changing epidemic (where the reproduction number is not near 1).\nIn addition, whilst we say it is “quite simple” as should be clear from the code, it is quite complex and computationally intensive. This is because we are fitting a model to the reporting triangle, which is a much larger data set and so the model is relatively quite slow to fit.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#challenge",
    "href": "sessions/joint-nowcasting.html#challenge",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Challenge",
    "text": "Challenge\n\nThe simple nowcast models we showed here assumed perfect knowledge of the delay distribution. What happens when you instead use an estimate of the delay distribution from the data? Try and do this using methods from session on biases in delay distributions and see how it affects the simple nowcast models.\nDespite being fairly involved, the joint nowcast model we used here is still quite simple and may struggle to capture more complex patterns in the data.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/joint-nowcasting.html#methods-in-practice",
    "href": "sessions/joint-nowcasting.html#methods-in-practice",
    "title": "Nowcasting with an unknown reporting delay",
    "section": "Methods in practice",
    "text": "Methods in practice\nIn practice, more complex methods are often needed to account for structure in the reporting process, time-varying delay distributions, or delays that vary by other factors (such as the age of cases). Consider how nowcasting approaches might differ in non-outbreak settings: What additional factors might you need to account for when applying these methods to routine surveillance data? Think about seasonal patterns, long-term trends, or other cyclical behaviours that might influence case reporting patterns.\n\nThis session focused on the role of the generative process in nowcasting. This is an area of active research but (Lison et al. 2024) gives a good overview of the current state of the art.\nThe epinowcast package implements a more complex version of the model we have used here. It is designed to be highly flexible and so can be used to model a wide range of different datasets.",
    "crumbs": [
      "Nowcasting with an unknown reporting delay"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html",
    "href": "sessions/hub-playground.html",
    "title": "Local hub playground",
    "section": "",
    "text": "In this session we will try to take some of the lessons on building and evaluating forecasts in practice from the earlier sessions and and apply them. Parts of this session will be more open-ended than other sessions, enabling you to spend time doing some hands-on modeling of your own, or spending time reviewing some of the best practices and key concepts that we have covered in the course so far.\n\n\n\nWhy use a local hub?\n\n\n\n\nThe aims of this session are\n\nto introduce the use of modeling hubs and hubverse-style tools for local model development and collaborative modeling projects, and\nto practice building forecasting models using real data.\n\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/hub-playground.qmd.\n\n\n\nIn this session we will use the dplyr package for data wrangling, the ggplot2 library for plotting, the fable package for building forecasting models, and the epidatr package for downloading epidemiological surveillance data.\nAdditionally, we will use some hubverse packages such as hubEvals, hubUtils, hubData, hubVis, hubEnsembles` packages for building ensembles.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\nlibrary(\"fable\")\nlibrary(\"hubUtils\")\nlibrary(\"hubEvals\")\nlibrary(\"hubVis\")\nlibrary(\"hubData\")\nlibrary(\"hubEnsembles\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(406) # for Ted Williams",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html#slides",
    "href": "sessions/hub-playground.html#slides",
    "title": "Local hub playground",
    "section": "",
    "text": "Why use a local hub?",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html#objectives",
    "href": "sessions/hub-playground.html#objectives",
    "title": "Local hub playground",
    "section": "",
    "text": "The aims of this session are\n\nto introduce the use of modeling hubs and hubverse-style tools for local model development and collaborative modeling projects, and\nto practice building forecasting models using real data.\n\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/hub-playground.qmd.\n\n\n\nIn this session we will use the dplyr package for data wrangling, the ggplot2 library for plotting, the fable package for building forecasting models, and the epidatr package for downloading epidemiological surveillance data.\nAdditionally, we will use some hubverse packages such as hubEvals, hubUtils, hubData, hubVis, hubEnsembles` packages for building ensembles.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\nlibrary(\"fable\")\nlibrary(\"hubUtils\")\nlibrary(\"hubEvals\")\nlibrary(\"hubVis\")\nlibrary(\"hubData\")\nlibrary(\"hubEnsembles\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(406) # for Ted Williams",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html#source-file",
    "href": "sessions/hub-playground.html#source-file",
    "title": "Local hub playground",
    "section": "",
    "text": "The source file of this session is located at sessions/hub-playground.qmd.",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html#libraries-used",
    "href": "sessions/hub-playground.html#libraries-used",
    "title": "Local hub playground",
    "section": "",
    "text": "In this session we will use the dplyr package for data wrangling, the ggplot2 library for plotting, the fable package for building forecasting models, and the epidatr package for downloading epidemiological surveillance data.\nAdditionally, we will use some hubverse packages such as hubEvals, hubUtils, hubData, hubVis, hubEnsembles` packages for building ensembles.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\nlibrary(\"fable\")\nlibrary(\"hubUtils\")\nlibrary(\"hubEvals\")\nlibrary(\"hubVis\")\nlibrary(\"hubData\")\nlibrary(\"hubEnsembles\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html#initialisation",
    "href": "sessions/hub-playground.html#initialisation",
    "title": "Local hub playground",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(406) # for Ted Williams",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html#ili-forecasting-at-hhs-region-level",
    "href": "sessions/hub-playground.html#ili-forecasting-at-hhs-region-level",
    "title": "Local hub playground",
    "section": "ILI Forecasting at HHS Region level",
    "text": "ILI Forecasting at HHS Region level\nWe have done some modeling on ILI data in the earlier sessions. Our sandbox hub is set up to accept forecasts for the US level and the 10 Health and Human Services (HHS) Regions.\n\n\n\nA map showing the 10 HHS Regions.\n\n\nA forecast will produce quantile predictions of 1- through 4-week ahead forecasts of the estimated percentage of outpatient doctors office visits that are for IL. This target is given the label “ili perc” in the hub. Forecasts can be submitted for any week during the 2015/2016, 2016/2017, 2018/2019 or 2019/2020 influenza seasons.",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html#details-of-this-hub",
    "href": "sessions/hub-playground.html#details-of-this-hub",
    "title": "Local hub playground",
    "section": "Details of this hub",
    "text": "Details of this hub\nThere are six models that already have made forecasts that live in the hub: hist-avg, delphi-epicast, protea-cheetah, lanl-dbmplus, kot-kot, and neu-gleam.\nWe will assume that you have the hub repository downloaded to your local machine as described in the section above.\n\nhub_path &lt;- here::here(\"sismid-ili-forecasting-sandbox\")\nhub_con &lt;- connect_hub(hub_path)\n\nWe can look at one forecast from the hub to get a sense of what a forecast should look like:\n\nhub_con |&gt; \n  filter(model_id == \"delphi-epicast\",\n         origin_date == \"2015-11-14\") |&gt; \n  collect_hub()\n\n# A tibble: 1,012 × 9\n   model_id     location origin_date horizon target_end_date target output_type output_type_id value\n * &lt;chr&gt;        &lt;chr&gt;    &lt;date&gt;        &lt;int&gt; &lt;date&gt;          &lt;chr&gt;  &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;\n 1 delphi-epic… HHS Reg… 2015-11-14        1 2015-11-21      ili p… quantile             0.01  0.504\n 2 delphi-epic… HHS Reg… 2015-11-14        1 2015-11-21      ili p… quantile             0.025 0.512\n 3 delphi-epic… HHS Reg… 2015-11-14        1 2015-11-21      ili p… quantile             0.05  0.525\n 4 delphi-epic… HHS Reg… 2015-11-14        1 2015-11-21      ili p… quantile             0.1   0.551\n 5 delphi-epic… HHS Reg… 2015-11-14        1 2015-11-21      ili p… quantile             0.15  0.578\n 6 delphi-epic… HHS Reg… 2015-11-14        1 2015-11-21      ili p… quantile             0.2   0.604\n 7 delphi-epic… HHS Reg… 2015-11-14        1 2015-11-21      ili p… quantile             0.25  0.630\n 8 delphi-epic… HHS Reg… 2015-11-14        1 2015-11-21      ili p… quantile             0.3   0.657\n 9 delphi-epic… HHS Reg… 2015-11-14        1 2015-11-21      ili p… quantile             0.35  0.683\n10 delphi-epic… HHS Reg… 2015-11-14        1 2015-11-21      ili p… quantile             0.4   0.709\n# ℹ 1,002 more rows",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html#building-a-simple-forecast-for-multiple-locations",
    "href": "sessions/hub-playground.html#building-a-simple-forecast-for-multiple-locations",
    "title": "Local hub playground",
    "section": "Building a simple forecast for multiple locations",
    "text": "Building a simple forecast for multiple locations\nWe’re going to start by building a simple auto-regressive forecast without putting too much thought into which exact model specification we choose. Let’s try, for starters, an ARIMA(2,1,0) with a fourth-root transformation. Recall, that this means we are using two auto-regressive terms with one set of differencing of the data.\n\nfourth_root &lt;- function(x) x^0.25\ninv_fourth_root &lt;- function(x) x^4\nmy_fourth_root &lt;- new_transformation(fourth_root, inv_fourth_root)\n\nfit_arima210 &lt;- flu_data_hhs |&gt;\n  filter(origin_date &lt;= \"2015-10-24\") |&gt; \n  model(ARIMA(my_fourth_root(wili) ~ pdq(2,1,0)))\nforecast(fit_arima210, h=4) |&gt;\n  autoplot(flu_data_hhs |&gt; \n             filter(\n               origin_date &lt;= \"2015-10-24\",\n               origin_date &gt;= \"2014-09-01\"\n             )) +\n  facet_wrap(.~location, scales = \"free_y\") +\n  labs(y = \"% of visits due to ILI\",\n       x = NULL)\n\n\n\n\n\n\n\n\nNote that if you set up your tsibble object correctly, with the \"location\" column as a key variable, then fable will fit and forecast 11 separate versions of the ARIMA(2,1,0) model for you directly, one for each of the locations in the dataset.\nEach model can be inspected as before\n\nfit_arima210 |&gt; \n  filter(location == \"HHS Region 1\") |&gt; \n  report(model)\n\nSeries: wili \nModel: ARIMA(2,1,0) \nTransformation: my_fourth_root(wili) \n\nCoefficients:\n         ar1     ar2\n      0.0302  0.0354\ns.e.  0.0397  0.0403\n\nsigma^2 estimated as 0.004699:  log likelihood=800.67\nAIC=-1595.33   AICc=-1595.29   BIC=-1581.98\n\nfit_arima210 |&gt; \n  filter(location == \"HHS Region 9\") |&gt; \n  report(model)\n\nSeries: wili \nModel: ARIMA(2,1,0) \nTransformation: my_fourth_root(wili) \n\nCoefficients:\n          ar1      ar2\n      -0.0938  -0.0118\ns.e.   0.0402   0.0402\n\nsigma^2 estimated as 0.004732:  log likelihood=798.43\nAIC=-1590.86   AICc=-1590.82   BIC=-1577.51",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html#ideas-for-more-complex-models",
    "href": "sessions/hub-playground.html#ideas-for-more-complex-models",
    "title": "Local hub playground",
    "section": "Ideas for more complex models",
    "text": "Ideas for more complex models\nBelow, you will be encouraged to try fitting your own models. Here are some ideas of other models you could try:\n\nuse the fable::VAR() model to build a multivariate vector auto-regressive model.\nuse the fable::NNETAR() model to build a neural network model.\nadd forecasts from the renewal model (this may take some time to run, but it would be interesting!)\nadd forecasts from other variants of the fable::ARIMA() models\ntry out other specifications of transformations (e.g. a log transformation instead of a fourth-root) or the number of fourier terms.\nincorporate another model of your choice.\nbuild some ensembles of all (or a subset) of the models you created.",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html#validation-phase-forecasts",
    "href": "sessions/hub-playground.html#validation-phase-forecasts",
    "title": "Local hub playground",
    "section": "Validation-phase forecasts",
    "text": "Validation-phase forecasts\nWe’ll follow the same time-series cross-validation recipe that we used in the session on forecast evaluation to run multiple forecasts at once for our validation phase.\nYou could adapt the code below to run validation-phase forecasts.\nHere are all the valid origin_dates that we could submit forecasts for, read directly from the hub:\n\norigin_dates &lt;- hub_path |&gt; \n  read_config(\"tasks\") |&gt; \n  get_round_ids()\norigin_dates\n\n  [1] \"2015-10-17\" \"2015-10-24\" \"2015-10-31\" \"2015-11-07\" \"2015-11-14\" \"2015-11-21\" \"2015-11-28\"\n  [8] \"2015-12-05\" \"2015-12-12\" \"2015-12-19\" \"2015-12-26\" \"2016-01-02\" \"2016-01-09\" \"2016-01-16\"\n [15] \"2016-01-23\" \"2016-01-30\" \"2016-02-06\" \"2016-02-13\" \"2016-02-20\" \"2016-02-27\" \"2016-03-05\"\n [22] \"2016-03-12\" \"2016-03-19\" \"2016-03-26\" \"2016-04-02\" \"2016-04-09\" \"2016-04-16\" \"2016-04-23\"\n [29] \"2016-04-30\" \"2016-05-07\" \"2016-10-22\" \"2016-10-29\" \"2016-11-05\" \"2016-11-12\" \"2016-11-19\"\n [36] \"2016-11-26\" \"2016-12-03\" \"2016-12-10\" \"2016-12-17\" \"2016-12-24\" \"2016-12-31\" \"2017-01-07\"\n [43] \"2017-01-14\" \"2017-01-21\" \"2017-01-28\" \"2017-02-04\" \"2017-02-11\" \"2017-02-18\" \"2017-02-25\"\n [50] \"2017-03-04\" \"2017-03-11\" \"2017-03-18\" \"2017-03-25\" \"2017-04-01\" \"2017-04-08\" \"2017-04-15\"\n [57] \"2017-04-22\" \"2017-04-29\" \"2017-05-06\" \"2017-10-21\" \"2017-10-28\" \"2017-11-04\" \"2017-11-11\"\n [64] \"2017-11-18\" \"2017-11-25\" \"2017-12-02\" \"2017-12-09\" \"2017-12-16\" \"2017-12-23\" \"2017-12-30\"\n [71] \"2018-01-06\" \"2018-01-13\" \"2018-01-20\" \"2018-01-27\" \"2018-02-03\" \"2018-02-10\" \"2018-02-17\"\n [78] \"2018-02-24\" \"2018-03-03\" \"2018-03-10\" \"2018-03-17\" \"2018-03-24\" \"2018-03-31\" \"2018-04-07\"\n [85] \"2018-04-14\" \"2018-04-21\" \"2018-04-28\" \"2018-05-05\" \"2018-10-13\" \"2018-10-20\" \"2018-10-27\"\n [92] \"2018-11-03\" \"2018-11-10\" \"2018-11-17\" \"2018-11-24\" \"2018-12-01\" \"2018-12-08\" \"2018-12-15\"\n [99] \"2018-12-22\" \"2018-12-29\" \"2019-01-05\" \"2019-01-12\" \"2019-01-19\" \"2019-01-26\" \"2019-02-02\"\n[106] \"2019-02-09\" \"2019-02-16\" \"2019-02-23\" \"2019-03-02\" \"2019-03-09\" \"2019-03-16\" \"2019-03-23\"\n[113] \"2019-03-30\" \"2019-04-06\" \"2019-04-13\" \"2019-04-20\" \"2019-04-27\" \"2019-05-04\" \"2019-10-12\"\n[120] \"2019-10-19\" \"2019-10-26\" \"2019-11-02\" \"2019-11-09\" \"2019-11-16\" \"2019-11-23\" \"2019-11-30\"\n[127] \"2019-12-07\" \"2019-12-14\" \"2019-12-21\" \"2019-12-28\" \"2020-01-04\" \"2020-01-11\" \"2020-01-18\"\n[134] \"2020-01-25\" \"2020-02-01\" \"2020-02-08\" \"2020-02-15\" \"2020-02-22\" \"2020-02-29\"\n\n\nWe will define time-series cross-validation datasets for each season separately, as we want to make sure that we start and end each season at the correct time.\n\nMake season 1 forecasts\nHere, we define a time-series cross-validation dataset for the first season\n\nflu_data_hhs_tscv_season1 &lt;- flu_data_hhs |&gt; \n  filter(\n    origin_date &lt;= \"2016-05-07\"  ## last 2015/2016 date\n    ) |&gt; \n  tsibble::stretch_tsibble(\n    .init = 634, ## flu_data_hhs |&gt; filter(location == \"HHS Region 1\", origin_date &lt;= \"2015-10-17\") |&gt; nrow()\n    .step = 1,\n    .id = \".split\"\n    )\nflu_data_hhs_tscv_season1\n\n# A tsibble: 214,005 x 4 [7D]\n# Key:       .split, location [330]\n   location     origin_date   wili .split\n   &lt;chr&gt;        &lt;date&gt;       &lt;dbl&gt;  &lt;int&gt;\n 1 HHS Region 1 2003-08-30  0.559       1\n 2 HHS Region 1 2003-09-06  0.423       1\n 3 HHS Region 1 2003-09-13  0.0677      1\n 4 HHS Region 1 2003-09-20  0.0694      1\n 5 HHS Region 1 2003-09-27  0.594       1\n 6 HHS Region 1 2003-10-04  0.572       1\n 7 HHS Region 1 2003-10-11  0.449       1\n 8 HHS Region 1 2003-10-18  0.614       1\n 9 HHS Region 1 2003-10-25  0.661       1\n10 HHS Region 1 2003-11-01  0.761       1\n# ℹ 213,995 more rows\n\n\nAnd now we will run all of the forecasts and make 1 through 4 week forecasts, including the generate() function to generate predictive samples and then extracting the quantiles. This code also reformats the fable-style forecasts into the required hubverse structure for this hub.\n\nquantile_levels &lt;- c(0.01, 0.025, seq(0.05, 0.95, 0.05), 0.975, 0.99)\n\ncv_forecasts_season1 &lt;- \n  flu_data_hhs_tscv_season1 |&gt; \n  model(\n    arima210 = ARIMA(my_fourth_root(wili) ~ pdq(2,1,0))\n  ) |&gt; \n  generate(h = 4, times = 100, bootstrap = TRUE) |&gt; \n  ## the following 3 lines of code ensure that there is a horizon variable in the forecast data\n  group_by(.split, .rep, location, .model) |&gt; \n  mutate(horizon = row_number()) |&gt; \n  ungroup() |&gt; \n  as_tibble() |&gt; \n  ## make hubverse-friendly names\n  rename(\n    target_end_date = origin_date,\n    value = .sim,\n    model_id = .model\n  ) |&gt; \n  left_join(loc_df) |&gt; \n  mutate(origin_date = target_end_date - horizon * 7L) |&gt; \n  ## compute the quantiles\n  group_by(model_id, location, origin_date, horizon, target_end_date) |&gt; \n  reframe(tibble::enframe(quantile(value, quantile_levels), \"quantile\", \"value\")) |&gt; \n  mutate(output_type_id = as.numeric(stringr::str_remove(quantile, \"%\"))/100, .keep = \"unused\",\n         target = \"ili perc\",\n         output_type = \"quantile\",\n         model_id = \"sismid-arima210\")\n\nJoining with `by = join_by(location)`\n\n\n\n\n\n\n\n\nTip\n\n\n\nNote that you need to choose a model_id for your model. It is required by our hub to have the format of [team abbreviation]-[model abbreviation]. When you are making a local hub, you should pick a team abbreviation that is something short and specific to you. For now, we’re going to use sismid as the team abbreviation. (See last line of code above for where I create the name.)\n\n\nThe above table has 30360 rows, which makes sense because there are\n\n30 origin dates\n11 locations\n4 horizons\n23 quantiles\n\nAnd \\(30 \\cdot 11 \\cdot 4 \\cdot 23 = 30,360\\).\nLet’s plot one set of forecasts as a visual sanity check to make sure our reformatting has worked.\n\ncv_forecasts_season1 |&gt; \n  filter(origin_date == \"2015-12-19\") |&gt; \n  plot_step_ahead_model_output(\n    flu_data_hhs |&gt; \n      filter(origin_date &gt;= \"2015-10-01\", \n             origin_date &lt;= \"2015-12-19\") |&gt; \n      rename(observation = wili),\n    x_target_col_name = \"origin_date\",\n    x_col_name = \"target_end_date\",\n    use_median_as_point = TRUE,\n    facet = \"location\",\n    facet_scales = \"free_y\",\n    facet_nrow=3\n  )\n\nWarning: ! `model_out_tbl` must be a `model_out_tbl`. Class applied by default\n\n\n\n\n\n\n\n\nMake season 2 forecasts\nHere, we define a time-series cross-validation dataset for the second season.\n\nfirst_idx_season2 &lt;- flu_data_hhs |&gt; \n  filter(location == \"HHS Region 1\", \n         origin_date &lt;= \"2016-10-22\") |&gt; ## first 2016/2017 date\n  nrow()\nflu_data_hhs_tscv_season2 &lt;- flu_data_hhs |&gt; \n  filter(\n    origin_date &lt;= \"2017-05-06\"  ## last 2016/2017 date\n    ) |&gt; \n  tsibble::stretch_tsibble(\n    .init = first_idx_season2,\n    .step = 1,\n    .id = \".split\"\n    )\n\nGenerate and reformat season 2 forecasts:\n\ncv_forecasts_season2 &lt;- \n  flu_data_hhs_tscv_season2 |&gt; \n  model(\n    arima210 = ARIMA(my_fourth_root(wili) ~ pdq(2,1,0))\n  ) |&gt; \n  generate(h = 4, times = 100, bootstrap = TRUE) |&gt; \n  ## the following 3 lines of code ensure that there is a horizon variable in the forecast data\n  group_by(.split, .rep, location, .model) |&gt; \n  mutate(horizon = row_number()) |&gt; \n  ungroup() |&gt; \n  as_tibble() |&gt; \n  ## make hubverse-friendly names\n  rename(\n    target_end_date = origin_date,\n    value = .sim,\n    model_id = .model\n  ) |&gt; \n  left_join(loc_df) |&gt; \n  mutate(origin_date = target_end_date - horizon * 7L) |&gt; \n  ## compute the quantiles\n  group_by(model_id, location, origin_date, horizon, target_end_date) |&gt; \n  reframe(tibble::enframe(quantile(value, quantile_levels), \"quantile\", \"value\")) |&gt; \n  mutate(output_type_id = as.numeric(stringr::str_remove(quantile, \"%\"))/100, .keep = \"unused\",\n         target = \"ili perc\",\n         output_type = \"quantile\",\n         model_id = \"sismid-arima210\")\n\nJoining with `by = join_by(location)`\n\n\n\n\nSave the forecasts locally\nNow let’s save the validation forecasts locally so that we can incorporate these forecasts with the existing forecasts in the hub.\nFirst, we need to create a model metadata file (formatted as a YAML file) and store it in the hub. This can be something very minimal, but there are a few required pieces. Here is some simple code to write out a minimal model metadata file.\n\nthis_model_id &lt;- \"sismid-arima210\"\n\nmetadata_filepath &lt;- file.path(\n  hub_path,\n  \"model-metadata\", \n  paste0(this_model_id, \".yml\"))\n\nmy_text &lt;- c(\"team_abbr: \\\"sismid\\\"\", \n             \"model_abbr: \\\"arima210\\\"\", \n             \"designated_model: true\")\n\nwriteLines(my_text, metadata_filepath)\n\nAnd now we can write the files out, one for each origin_date.\n\n# Group the forecasts by task id variables\ngroups &lt;- bind_rows(cv_forecasts_season1, cv_forecasts_season2) |&gt; \n  group_by(model_id, target, origin_date) |&gt; \n  group_split()\n\n# Save each group as a separate CSV\nfor (i in seq_along(groups)) {\n  group_df &lt;- groups[[i]]\n  this_model_id &lt;- group_df$model_id[1]\n  this_origin_date &lt;- group_df$origin_date[1]\n  \n  ## remove model_id from saved data, as it is implied from filepath\n  group_df &lt;- select(group_df, -model_id)\n  \n  ## path to the file from the working directory of the instructional repo\n  model_folder &lt;- file.path(\n    hub_path,\n    \"model-output\", \n    this_model_id)\n\n  ## just the filename, no path\n  filename &lt;- paste0(this_origin_date, \"-\", this_model_id, \".csv\")\n  \n  ## path to the file\n  results_path &lt;- file.path(\n    model_folder, \n    filename)\n  \n  ## if this model's model-out directory doesn't exist yet, make it\n  if (!file.exists(model_folder)) {\n    dir.create(model_folder, recursive = TRUE)\n  }\n  \n  write.csv(group_df, file = results_path, row.names = FALSE)\n  \n  ### if you run into errors, the code below could help trouble-shoot validations\n  # hubValidations::validate_submission(\n  #   hub_path = hub_path,\n  #   file_path = file.path(this_model_id, filename)\n  # )\n}\n\n\n\nLocal hub evaluation\nNow that you have your model’s forecasts in the local hub, you should be able to run an evaluation to compare the model’s forecasts to the model in the hub.\nWe first collect the forecasts from the hub (including our new model)\n\nvalidation_origin_dates &lt;- origin_dates[which(as.Date(origin_dates) &lt;= as.Date(\"2017-05-06\"))]\n\nnew_hub_con &lt;- connect_hub(hub_path)\n\nvalidation_forecasts &lt;- new_hub_con |&gt; \n  filter(origin_date %in% validation_origin_dates) |&gt; \n  collect_hub()\n\nand then compare them to the oracle output.\n\noracle_output &lt;- connect_target_oracle_output(hub_path) |&gt; \n  collect()\n\nhubEvals::score_model_out(\n  validation_forecasts,\n  oracle_output\n) |&gt; \n  arrange(wis) |&gt; \n  knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_id\nwis\noverprediction\nunderprediction\ndispersion\nbias\ninterval_coverage_50\ninterval_coverage_90\nae_median\n\n\n\n\nsismid-nnetar\n0.19\n0.08\n0.06\n0.05\n0.08\n0.33\n0.78\n0.29\n\n\nkzank-arima120_fourier1\n0.19\n0.03\n0.07\n0.08\n-0.07\n0.54\n0.94\n0.29\n\n\nsismid-kznn1\n0.19\n0.08\n0.06\n0.05\n0.06\n0.30\n0.75\n0.30\n\n\nsismid_mb-arima200_fourier\n0.20\n0.03\n0.10\n0.08\n-0.12\n0.60\n0.89\n0.31\n\n\nsismid-NNETARdugqbc2\n0.21\n0.09\n0.06\n0.05\n0.14\n0.24\n0.75\n0.32\n\n\nKS-ar_auto\n0.21\n0.03\n0.10\n0.08\n-0.13\n0.56\n0.92\n0.33\n\n\nAEM-fourier_ar210\n0.22\n0.04\n0.09\n0.09\n-0.03\n0.49\n0.91\n0.34\n\n\nsismid-arima210a\n0.22\n0.04\n0.09\n0.08\n-0.04\n0.50\n0.90\n0.34\n\n\nsismid-arima210log10\n0.22\n0.05\n0.09\n0.08\n-0.04\n0.47\n0.89\n0.35\n\n\nnm-nnetar\n0.29\n0.09\n0.12\n0.08\n0.00\n0.37\n0.74\n0.43\n\n\nsismid-mueda\n0.31\n0.04\n0.13\n0.13\n-0.03\n0.62\n0.90\n0.47\n\n\ndelphi-epicast\n0.31\n0.10\n0.07\n0.14\n0.08\n0.47\n0.90\n0.44\n\n\nsismid-arima210\n0.32\n0.09\n0.09\n0.14\n-0.01\n0.49\n0.94\n0.52\n\n\njoy-arima210\n0.33\n0.10\n0.09\n0.14\n0.03\n0.45\n0.93\n0.53\n\n\nsismid-var2sqrt\n0.34\n0.12\n0.11\n0.11\n0.02\n0.46\n0.85\n0.53\n\n\nsismid-ensemblev2\n0.35\n0.13\n0.10\n0.11\n0.06\n0.47\n0.85\n0.53\n\n\nsismid-arima011\n0.43\n0.16\n0.13\n0.14\n0.03\n0.26\n0.86\n0.72\n\n\nhist-avg\n0.45\n0.06\n0.16\n0.23\n-0.13\n0.56\n0.95\n0.67\n\n\nkot-kot\n2.50\n1.44\n0.00\n1.06\n0.72\n0.13\n0.97\n4.49\n\n\n\n\n\nHere we can see that our new sismid-arima210 model has a bit lower probabilistic accuracy (higher WIS) than the delphi-epicast model, and has better accuracy than the hist-avg model. Also, it shows less bias than either of the other models, although a bit more dispersion (it is less sharp) than the delphi-epicast model.\n\n\n\n\n\n\nCaution\n\n\n\nNote that the forecasts that we make are using the finalized version of the data, whereas the pre-existing models that are in the hub were generated using data that was available in real-time. This is potentially a considerable disadvantage, if data ended up being substantially revised, therefore these evaluations against the “real” models should be interpreted with caution.",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/hub-playground.html#process-of-submitting-to-an-online-hub",
    "href": "sessions/hub-playground.html#process-of-submitting-to-an-online-hub",
    "title": "Local hub playground",
    "section": "Process of submitting to an online hub",
    "text": "Process of submitting to an online hub\nThe best way to submit to an online hub is via a pull request initiated through GitHub.\n\nDrag and drop\nIf you’re not that familiar with GitHub and you have forecasts that you’d like to upload, work with an instructor or another participant who does have access to the hub. They will be able to create a subfolder of the model-output directory of the hub for you to upload forecasts to via a Drag and Drop.\nYou still will need a GitHub account (sign up here if you don’t have one).\n\n\nCloning repo, submitting via PR\n\n\n\n\n\n\nCaution\n\n\n\nThe following assumes that you have a GitHub account and some basic working knowledge of GitHub interactions and terminology.\n\n\n\nFork the sismid-ili-forecasting-sandbox hub so you have a copy of this repository owned by your GitHub user account.\nClone your fork of the repo to your laptop (ideally, it should live at the top level within the sismid directory with these course materials).\nPut whatever model-metadata and forecasts you want to submit into the correct locations in the hub.\nCommit these changes to your local clone and push those changes to your fork on GitHub.\nOpen a Pull Request (PR) back to the primary repository with your changes.\nOnce the PR is open, watch the validations, and inspect/fix any errors that appear.\nIf your forecasts pass validations, then an instructor will merge them in.",
    "crumbs": [
      "Local hub playground"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html",
    "href": "sessions/forecasting-models.html",
    "title": "Improving forecasting models",
    "section": "",
    "text": "Learning how to look at time-series data and identify features that might make a particular dataset easier or harder to predict is a skill that takes time to get good at. It is a common pitfall to just start throwing models at data, without really looking at the data themselves, or making detailed comparisons of forecasts to observations.\n\n\n\nImproving forecasting models\n\n\n\n\nThe goal of this session is to start to introduce you in a bit more depth to the ARIMA modeling framework, and to some important forecasting practices. We will focus on learning how to look at data, transformations and summaries of time-series data, and forecasts, to infer what kinds of model structures will help improve predictions.\n\n\n\n\n\n\nCaution\n\n\n\nNone of the models introduced in this section are designed for real-world use!\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecasting-models.qmd.\n\n\n\nIn this session we will use nfidd package for some flu data, the fable package to fit and forecast ARIMA models, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the epidatr package for loading epidemiological data.\n\nlibrary(\"nfidd\")\nlibrary(\"fable\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do.\n\nset.seed(123)",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#slides",
    "href": "sessions/forecasting-models.html#slides",
    "title": "Improving forecasting models",
    "section": "",
    "text": "Improving forecasting models",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#objectives",
    "href": "sessions/forecasting-models.html#objectives",
    "title": "Improving forecasting models",
    "section": "",
    "text": "The goal of this session is to start to introduce you in a bit more depth to the ARIMA modeling framework, and to some important forecasting practices. We will focus on learning how to look at data, transformations and summaries of time-series data, and forecasts, to infer what kinds of model structures will help improve predictions.\n\n\n\n\n\n\nCaution\n\n\n\nNone of the models introduced in this section are designed for real-world use!\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecasting-models.qmd.\n\n\n\nIn this session we will use nfidd package for some flu data, the fable package to fit and forecast ARIMA models, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the epidatr package for loading epidemiological data.\n\nlibrary(\"nfidd\")\nlibrary(\"fable\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do.\n\nset.seed(123)",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#source-file",
    "href": "sessions/forecasting-models.html#source-file",
    "title": "Improving forecasting models",
    "section": "",
    "text": "The source file of this session is located at sessions/forecasting-models.qmd.",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#libraries-used",
    "href": "sessions/forecasting-models.html#libraries-used",
    "title": "Improving forecasting models",
    "section": "",
    "text": "In this session we will use nfidd package for some flu data, the fable package to fit and forecast ARIMA models, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the epidatr package for loading epidemiological data.\n\nlibrary(\"nfidd\")\nlibrary(\"fable\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#initialisation",
    "href": "sessions/forecasting-models.html#initialisation",
    "title": "Improving forecasting models",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do.\n\nset.seed(123)",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#acf-plots",
    "href": "sessions/forecasting-models.html#acf-plots",
    "title": "Improving forecasting models",
    "section": "ACF plots",
    "text": "ACF plots\nWe will now use the function gg_tsdisplay() from the feasts R package to give a unique view into our dataset.\n\nfeasts::gg_tsdisplay(flu_data, y = wili, plot_type='partial', lag_max = 104)\n\n\n\n\n\n\n\n\nIn the above plot, the top panel shows the time-series. The lower left panel plots the auto-correlation function plotted for up to lag 104. It is commonly referred to as an ACF plot. Along the x-axis is the lag value and the y-axis is the correlation between \\(y_t\\) and \\(y_{t-k}\\) where \\(k\\) is the lag value. So for example, the vertical bar at x=1 is just less than one, which means that when we compute the correlation between \\(y_t\\) and \\(y_{t-1}\\) across our whole dataset, it is very high. We can do this explicitly here for comparison for lags 1 and 12:\n\ncor(flu_data$wili, dplyr::lag(flu_data$wili, 1), use = \"complete.obs\")\n\n[1] 0.9633374\n\ncor(flu_data$wili, dplyr::lag(flu_data$wili, 12), use = \"complete.obs\")\n\n[1] -0.01203389\n\n\nBy comparing these values computed directly here to the ACF plot above at lags of 1 and 12, you can see that the values are the same.",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#pacf-plots",
    "href": "sessions/forecasting-models.html#pacf-plots",
    "title": "Improving forecasting models",
    "section": "PACF plots",
    "text": "PACF plots\nThe lower right hand panel shows the partial auto-correlation function (PACF). The partial auto-correlation at lag \\(k\\) estimates the correlation between \\(y_t\\) and \\(y_{t-k}\\) after adjusting for the effects of lags 1, …, k-1. There are a lot of additional examples and explanation about interpreting ACF and PCF plots in FPP3, Chapter 9.5.\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nWhat can you take away from the ACF and PCF plots of the data? (Same plot as above is reproduced here.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe ACF plots show that there is a lot of correlation between lagged observations. For the first 10 or so lags, observations tend to be positively correlated and then after that, the correlations are periodic, with strong negative lags around every 25-26 weeks (~ 6 months) and strong positive lags every ~52 weeks/1 year. The correlations with the most recent 5-7 weeks of data are stronger than the correlations with the observations at the same time of year in the previous season.\nThe PCF plots show that once you adjust for the other timepoints, much of the correlation disappears. There do appear to be significant signals (both positive and negative) at lags 1 and 2 and at lags around 52 weeks. This suggests that we might want to consider a model that can see very recent lags as well as some annual periodicity or seasonality.",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#arima-model-as-a-regression",
    "href": "sessions/forecasting-models.html#arima-model-as-a-regression",
    "title": "Improving forecasting models",
    "section": "ARIMA model as a regression",
    "text": "ARIMA model as a regression\nAn ARIMA model is specified as ARIMA(\\(p\\),\\(d\\),\\(q\\)), where, as defined above, the \\(p\\), \\(d\\), and \\(q\\) refer to the order of auto-regressive, differencing and moving-average terms. An ARIMA is a kind of regression model, with an outcome being predicted by a set of independent variables. We are going to fit a few overly simple ARIMA models below just to give examples of the math, the fitted values, and what the forecasts look like.\n\nARIMA(1,0,0)\nHere is the equation for an ARIMA(1,0,0) model: \\[ y_t = \\mu_t + \\phi_1 y_{t-1} + \\varepsilon_t \\] where \\(\\mu_t\\) is a constant (like an intercept term in the regression), \\(\\phi_1\\) is a coefficient for the auto-regressive term, and \\(\\varepsilon_t\\) is a random error term with mean zero and constant variance. Note the constant variance assumption! This is one of the places where the stationarity assumption is comes into play: if the time-series is stationary then constant variance is a reasonable assumption.\nHere is code to fit and forecast from an ARIMA(1,0,0) model (this is sometimes also referred to as an AR(1) model):\n\nfit_arima100 &lt;- flu_data |&gt;\n  model(ARIMA(wili ~ pdq(1,0,0)))\nreport(fit_arima100)\n\nSeries: wili \nModel: ARIMA(1,0,0) w/ mean \n\nCoefficients:\n         ar1  constant\n      0.9633    0.0638\ns.e.  0.0094    0.0121\n\nsigma^2 estimated as 0.1219:  log likelihood=-286.2\nAIC=578.41   AICc=578.44   BIC=592.38\n\nforecast(fit_arima100, h=40) |&gt;\n  autoplot(flu_data |&gt; filter(epiweek &gt;= as.Date(\"2015-09-01\"))) +\n  labs(title = \"WILI, US level\",\n       y=\"% of visits\")\n\n\n\n\n\n\n\n\n\n\nARIMA(1,1,0)\nHere is an equation for an ARIMA(1,1,0) model: \\[ y_t - y_{t-1} = \\phi_1 (y_{t-1} - y_{t-2}) + \\varepsilon_t.\\] Another way to think about this is that we’ve just replaced the dependent outcome variable \\(y_t\\) with \\(z_t\\) as we defined above as the difference. Remember that the (1,1,0) part means that we have one auto-regressive term, one differencing and no moving average terms. Here is code to fit and forecast from this model:\n\nfit_arima110 &lt;- flu_data |&gt;\n  model(ARIMA(wili ~ pdq(1,1,0)))\nreport(fit_arima110)\n\nSeries: wili \nModel: ARIMA(1,1,0) \n\nCoefficients:\n         ar1\n      0.5349\ns.e.  0.0302\n\nsigma^2 estimated as 0.08845:  log likelihood=-160.35\nAIC=324.7   AICc=324.72   BIC=334.02\n\nforecast(fit_arima110, h=40) |&gt;\n  autoplot(flu_data |&gt; filter(epiweek &gt;= as.Date(\"2015-09-01\"))) +\n  labs(title = \"WILI, US level\",\n       y=\"% of visits\")\n\n\n\n\n\n\n\n\nNote that by default a model with \\(d=1\\) doesn’t include a constant term in the model.\n\n\nARIMA(0,0,1)\nThe auto-regressive and differencing knobs of ARIMA models are fairly easy to conceptualize, but the moving average part is a little bit trickier. Basically, a moving average model uses residuals from the previous timepoints as predictors in the model. This is actually a common technique in more modern-day machine learning approaches too: learn from your past errors/residuals to improve your model. Here is an example of an ARIMA(0,0,1) model in math terms:\n\\[ y_t = \\mu + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}.\\] The key thing to note here is that \\(\\varepsilon_t\\) is still the same error term as in the earlier equations but the \\(\\varepsilon_{t-1}\\) term is included as a predictor in the model, with it’s own coefficient \\(\\theta_1\\).\n\n\n\n\n\n\nDon’t get confused by “moving average” terminology!\n\n\n\nIt is important to not get the “moving average” part of an ARIMA model confused with a models that smooth noisy data (like loess), which can sometimes also be referred to as “moving averages”. These are very different things!\n\n\nHere is code to fit and forecast from the ARIMA(0,0,1 model):\n\nfit_arima001 &lt;- flu_data |&gt;\n  model(ARIMA(wili ~ pdq(0,0,1)))\nreport(fit_arima001)\n\nSeries: wili \nModel: ARIMA(0,0,1) w/ mean \n\nCoefficients:\n         ma1  constant\n      0.9380    1.8181\ns.e.  0.0095    0.0491\n\nsigma^2 estimated as 0.5024:  log likelihood=-838.4\nAIC=1682.81   AICc=1682.84   BIC=1696.78\n\nforecast(fit_arima001, h=40) |&gt;\n  autoplot(flu_data |&gt; filter(epiweek &gt;= as.Date(\"2015-09-01\"))) +\n  labs(title = \"WILI, US level\",\n       y=\"% of visits\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nUsing the partial auto-correlation function plots as a guide, which lags might you consider including in the AR part of an ARIMA model?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRecall that the interpretation of the height of the line at \\(x = k\\) in the PACF plot is the correlation between \\(y_t\\) and \\(y_{t-k}\\) after adjusting for all other lags less than \\(k\\). This interpretation is similar to that of a multiple regression model, where a coefficient in a model captures the relationship between it and the outcome adjusting for other predictor variables. Therefore, the PACF plots can help identify which auto-regressive terms that might be appropriate for a model. In particular, we see strong PACF values for lags 1 and 2, a slightly “significant” but much smaller PACF value at lag 3 and then also some significant lags at around 52 weeks (exactly one year later). We will talk about adjusting for seasonalily later, but for now, it looks like including 2-3 auto-regressive lags in the model might be appropriate.",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#using-data-inspection-to-inform-model-choices",
    "href": "sessions/forecasting-models.html#using-data-inspection-to-inform-model-choices",
    "title": "Improving forecasting models",
    "section": "Using data inspection to inform model choices",
    "text": "Using data inspection to inform model choices\nLet’s look at our data again.\n\nfeasts::gg_tsdisplay(flu_data, y = wili, plot_type='partial', lag_max = 104)\n\n\n\n\n\n\n\n\nThe time-series is clearly non-stationary, with a clear periodicity that depends on the week of the season. Let’s try first differencing and looking at the data again.\n\nfeasts::gg_tsdisplay(flu_data, y = difference(wili), plot_type='partial', lag_max = 104)\n\n\n\n\n\n\n\n\nIt’s not stationary (there still are seasonal trends, we will do more to deal with this soon), but it’s at least a little bit better.\nBased on this plot, let’s put on our list of candidate models to fit an ARIMA(1,1,0), because with one difference in the data we can see that the PACF shows a significant correlation at only a lag of 1 (and at some higher lags that we will ignore for now). Recall that in the last session we fit an ARIMA(2,0,0). We’re going to come up with a list of a few candidate models to fit and then fit them all at once.\nOne clear feature of the original (not differenced) data is that the variance is small during the summer months (the flu off-season) and very high during the peaks of the season. Let’s implement a transformation to try to get something closer to constant variance. Through a lot of experimentation, we’ve found that a fourth-root transformation often does a reasonable job at stabilizing variance.(Ray et al. 2025) And, it has the added benefit of making it so that predictions will never be negative. (Remember that problem from last session?) Other transformations might be reasonable too (there is a whole section on them in the FPP3 textbook), and this is something often worth doing a bit of exploration for with your particular dataset.\nWe’re going to use fable-specified standards to define two transformation functions\n\nfourth_root &lt;- function(x) x^0.25\ninv_fourth_root &lt;- function(x) x^4\n\nmy_fourth_root &lt;- new_transformation(fourth_root, inv_fourth_root)\n\nAnd then we can plot the data using this transformation:\n\nautoplot(flu_data, .vars = fourth_root(wili))\n\n\n\n\n\n\n\n\nIt’s not perfect, but it’s better. We will include some fourth-root transformations in our next round of model fitting.",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#fit-models",
    "href": "sessions/forecasting-models.html#fit-models",
    "title": "Improving forecasting models",
    "section": "Fit models",
    "text": "Fit models\nLet’s fit four models total: ARIMA(2,0,0) and ARIMA(1,1,0), each with and without the fourth_root transformation.\n\nfits &lt;- flu_data |&gt;\n  model(\n    arima200 = ARIMA(wili ~ pdq(2,0,0)),\n    arima110 = ARIMA(wili ~ pdq(1,1,0)),\n    arima200_trans = ARIMA(my_fourth_root(wili) ~ pdq(2,0,0)),\n    arima110_trans = ARIMA(my_fourth_root(wili) ~ pdq(1,1,0))\n  )\nforecast(fits, h=40) |&gt;\n  autoplot(flu_data |&gt; filter(epiweek &gt;= as.Date(\"2015-09-01\"))) +\n  facet_wrap(~.model) +\n  labs(title = \"WILI, US level\",\n       y=\"% of visits\")\n\n\n\n\n\n\n\n\nThe above plots show 40-week ahead forecasts from each of the four models.\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nNone of the forecasts above look great. But which of these models do you think is doing the most reasonable (or least unreasonable) things?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nOnce again, there is not exactly a “right” answer here, but here are a few things to consider:\n\nThe fact that both of the _trans forecasts (the ones with the fourth-root transformation) stay positive seems pretty critical. The other forecasts routinely predict impossible values!\nThe arima110_trans model has quite high uncertainty at longer horizons in a way that seems reasonable.\n\nAgain, all of these forecasts are not very good, but they are progress.",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecasting-models.html#references",
    "href": "sessions/forecasting-models.html#references",
    "title": "Improving forecasting models",
    "section": "References",
    "text": "References\n\n\nRay, Evan L., Yijin Wang, Russell D. Wolfinger, and Nicholas G. Reich. 2025. “Flusion: Integrating Multiple Data Sources for Accurate Influenza Predictions.” Epidemics 50 (March): 100810. https://doi.org/10.1016/j.epidem.2024.100810.",
    "crumbs": [
      "Improving forecasting models"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html",
    "href": "sessions/forecast-evaluation.html",
    "title": "Forecast evaluation",
    "section": "",
    "text": "So far we have focused on building and visualisation simple forecasts. In this session you will get to know several ways of assessing different aspects of forecast performance. One of these approaches involves visualising forecasts in the context of eventually observed data. Another approach involves summarising performance quantitatively.\n\n\n\nForecast evaluation\n\n\n\n\nThe aim of this session is to introduce the concept of forecast evaluation from a qualitative and quantitative perspective.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecasting-evaluation.qmd.\n\n\n\nIn this session we will use the fable package for fitting and evaluating simple forecasting models, the dplyr package for data wrangling, the ggplot2 library for plotting and the epidatr package for accessing and downloading versions of epidemiological surveillance data from the Delphi EpiData API.\n\nlibrary(\"nfidd\")\nlibrary(\"fable\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(42) # for Jackie Robinson!",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#slides",
    "href": "sessions/forecast-evaluation.html#slides",
    "title": "Forecast evaluation",
    "section": "",
    "text": "Forecast evaluation",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#objectives",
    "href": "sessions/forecast-evaluation.html#objectives",
    "title": "Forecast evaluation",
    "section": "",
    "text": "The aim of this session is to introduce the concept of forecast evaluation from a qualitative and quantitative perspective.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecasting-evaluation.qmd.\n\n\n\nIn this session we will use the fable package for fitting and evaluating simple forecasting models, the dplyr package for data wrangling, the ggplot2 library for plotting and the epidatr package for accessing and downloading versions of epidemiological surveillance data from the Delphi EpiData API.\n\nlibrary(\"nfidd\")\nlibrary(\"fable\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(42) # for Jackie Robinson!",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#source-file",
    "href": "sessions/forecast-evaluation.html#source-file",
    "title": "Forecast evaluation",
    "section": "",
    "text": "The source file of this session is located at sessions/forecasting-evaluation.qmd.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#libraries-used",
    "href": "sessions/forecast-evaluation.html#libraries-used",
    "title": "Forecast evaluation",
    "section": "",
    "text": "In this session we will use the fable package for fitting and evaluating simple forecasting models, the dplyr package for data wrangling, the ggplot2 library for plotting and the epidatr package for accessing and downloading versions of epidemiological surveillance data from the Delphi EpiData API.\n\nlibrary(\"nfidd\")\nlibrary(\"fable\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#initialisation",
    "href": "sessions/forecast-evaluation.html#initialisation",
    "title": "Forecast evaluation",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(42) # for Jackie Robinson!",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#the-forecasting-paradigm",
    "href": "sessions/forecast-evaluation.html#the-forecasting-paradigm",
    "title": "Forecast evaluation",
    "section": "The forecasting paradigm",
    "text": "The forecasting paradigm\nThe general principle underlying forecast evaluation is to maximise sharpness subject to calibration (Gneiting and Raftery 2007). This means that statements about the future should be correct (calibration) and should aim to have narrow uncertainty (sharpness).",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#mean-absolute-error",
    "href": "sessions/forecast-evaluation.html#mean-absolute-error",
    "title": "Forecast evaluation",
    "section": "Mean absolute error",
    "text": "Mean absolute error\n\nWhat is the Mean Absolute Error (MAE)?\nFor point forecasts (single value predictions), forecast accuracy is often measured using the Mean Absolute Error (MAE), which calculates the average (mean) absolute difference between predicted and observed values. Mathematically, this can be expressed simply as \\[\nMAE(\\hat y, y) = \\frac{1}{T} \\sum_{t=1}^T |\\hat y_t - y_t|\n\\] where \\(y_t\\) is the observed value at time \\(t\\), \\(\\hat y_t\\) is the point prediction, and \\(T\\) is the total number of predictions.\n\n\n\n\n\n\nKey things to note about the MAE\n\n\n\n\nSmall values are better.\nAs it is an absolute scoring rule it can be difficult to use to compare forecasts where the observed values are on different scales. For example, if you are comparing forecasts for counts in location A, where the counts tend to be over ten thousand, to counts from location B, where counts tend to be under 100.\nThe metric is on the scale of the data. The number MAE returns can be thought of as the average absolute distance between your prediction and the eventually observed value.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#mean-squared-error",
    "href": "sessions/forecast-evaluation.html#mean-squared-error",
    "title": "Forecast evaluation",
    "section": "Mean squared error",
    "text": "Mean squared error\n\nWhat is the Root Mean Squared Error (RMSE)?\nAnother common point forecast accuracy measure is the Root Mean Squared Error (RMSE), which calculates the square root of the average (mean) squared difference between predicted and observed values. Mathematically, this can be expressed simply as \\[\nRMSE(\\hat y, y) = \\sqrt{\\frac{1}{T} \\sum_{t=1}^T (\\hat y_t - y_t)^2}\n\\] where \\(y_t\\) is the observed value at time \\(t\\), \\(\\hat y_t\\) is the point prediction, and \\(T\\) is the total number of predictions.\n\n\n\n\n\n\nKey things to note about the RMSE\n\n\n\n\nSmall values are better.\nAs it is an absolute scoring rule it can be difficult to use to compare forecasts where the observed values are on different scales (same as MAE).\nIt tends to penalize forecasts that have large misses, because those distances will be squared and have an outsize impact on the mean.\nThe metric is on the scale of the data.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#continuous-ranked-probability-score",
    "href": "sessions/forecast-evaluation.html#continuous-ranked-probability-score",
    "title": "Forecast evaluation",
    "section": "Continuous ranked probability score",
    "text": "Continuous ranked probability score\n\nWhat is the Continuous Ranked Probability Score (CRPS)?\nFor probabilistic forecasts, where the forecast is a distribution rather than a single point estimate, we can use the Continuous Ranked Probability Score (CRPS). The CRPS is a proper scoring rule that generalises MAE to probabilistic forecasts. Note that for deterministic forecasts, CRPS reduces to MAE.\nThe CRPS can be thought about as the combination of two key aspects of forecasting: 1. The accuracy of the forecast in terms of how close the predicted values are to the observed value. 2. The confidence of the forecast in terms of the spread of the predicted values.\nBy balancing these two aspects, the CRPS provides a comprehensive measure of the quality of probabilistic forecasts.\n\n\n\n\n\n\nKey things to note about the CRPS\n\n\n\n\nSmall values are better\nAs it is an absolute scoring rule it can be difficult to use to compare forecasts across scales.\nThe metric is on the scale of the data. It can be thought of, in a heuristic kind of way, as a measure of distance between the predicted distribution and the observed value.\n\n\n\n\n\n\n\n\n\nMathematical Definition (optional)\n\n\n\n\n\nThe CRPS for a predictive distribution characterised by a cumulative distribution function \\(F\\) and observed value \\(x\\) is calculated as\n\\[\nCRPS(F, y) = \\int_{-\\infty}^{+\\infty} \\left( F(x) - \\mathbb{1} ({x \\geq y}) \\right)^2 dx.\n\\]\nFor distributions with a finite first moment (a mean exists and it is finite), the CRPS can be expressed as:\n\\[\nCRPS(F, y) = \\mathbb{E}_{X \\sim F}[|X - y|] - \\frac{1}{2} \\mathbb{E}_{X, X' \\sim F}[|X - X'|]\n\\]\nwhere \\(X\\) and \\(X'\\) are independent random variables sampled from the distribution \\(F\\). To calculate this we simply replace \\(X\\) and \\(X'\\) by samples from our posterior distribution and sum over all possible combinations.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#looking-at-the-scores-by-horizon",
    "href": "sessions/forecast-evaluation.html#looking-at-the-scores-by-horizon",
    "title": "Forecast evaluation",
    "section": "Looking at the scores by horizon",
    "text": "Looking at the scores by horizon\nWhilst the metrics described above are very useful they can be difficult to interpret in isolation. For example, it is often useful to compare the CRPS of different models or to compare the CRPS of the same model under different conditions. Let’s compare the all of the metrics across different forecast horizons.\n\nmetrics_fourier_ar2 &lt;- forecast_fourier_ar2 |&gt; \n  mutate(h = row_number()) |&gt; \n  accuracy(data = flu_data, \n           measures = list(mae = MAE, rmse = RMSE, crps = CRPS),\n           by = c(\".model\", \"h\"))\nmetrics_fourier_ar2\n\n# A tibble: 40 × 6\n   .model               h .type    mae   rmse  crps\n   &lt;chr&gt;            &lt;int&gt; &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 arima200_fourier     1 Test  0.0160 0.0160 0.192\n 2 arima200_fourier     2 Test  0.105  0.105  0.336\n 3 arima200_fourier     3 Test  0.200  0.200  0.468\n 4 arima200_fourier     4 Test  0.211  0.211  0.553\n 5 arima200_fourier     5 Test  0.237  0.237  0.633\n 6 arima200_fourier     6 Test  0.301  0.301  0.719\n 7 arima200_fourier     7 Test  0.365  0.365  0.799\n 8 arima200_fourier     8 Test  0.285  0.285  0.816\n 9 arima200_fourier     9 Test  0.147  0.147  0.834\n10 arima200_fourier    10 Test  0.0963 0.0963 0.875\n# ℹ 30 more rows\n\n\nAbove you can see the top few rows of errors. And we could plot these metrics by horizon.\n\nmetrics_fourier_ar2 |&gt; \n  tidyr::pivot_longer(\n    cols = c(\"mae\", \"rmse\", \"crps\"),\n    names_to = \"metric\"\n    ) |&gt; \n  ggplot() +\n  geom_line(aes(x = h, y = value, color = metric)) +\n  facet_wrap(.~metric)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nHow do the scores change with forecast horizon? How similar to or different from each other are the metrics? Which of the additional dimensions mentioned at the top of the lesson would you like to examine more closely for forecast accuracy?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAll of the metrics increase for horizons where true incidence is higher.\nAll of the metrics follow nearly the same patterns, indicating that they are measuring very similar things about the forecasts.\nThe forecast errors are strongly associated with the seasonality of the data.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#time-series-cross-validation",
    "href": "sessions/forecast-evaluation.html#time-series-cross-validation",
    "title": "Forecast evaluation",
    "section": "Time-series cross-validation",
    "text": "Time-series cross-validation\nThis kind of experimental set-up is called “cross-validation”. Running an experiment in this way mimics what happens during real-time forecasting, when forecasts are made each week using available data. And because we are doing this on time-series data, it is referred to as “time-series cross-validation”. (If you are interested, you can read a little bit more about time-series cross-validation in the FPP3 book.)\n\n\n\n\n\n\nReal-time forecasting realism\n\n\n\n\n\nOne thing about forecasting in real-time that is not replicated with the above experiment is that data in real-time are often incompletely reported (similar to data issues discussed in the first half of the course). In the experiment below, we are only subsetting the complete and final version of the data and assuming that this is what the model would have seen in real time. Knowing what we know about public health surveillance datasets, this is an optimistic assumption. To increase realism in an experiment like this, we could use epidatr to extract specific versions of data as they were available in real-time and forecast from those datasets.\n\n\n\nThe image below (generated with some code extracted from ChatGPT, with just a few very small tweaks) shows the experimental set-up. Each row of tiles corresponds to one “split” of the data, or one week where we are making a forecast. Blue tiles correspond to “training set” weeks (weeks of data the model is able train on). There are actually many more weeks of training data, but they are omitted so we can focus on the red weeks. Red tiles show the weeks that forecasts are made for.\n\n\n\n\n\n\n\n\n\nThe code below shows how to use the tsibble::stretch_tsibble() function to set up the cross-validation experiment. We looked at the dataset itself and saw that we wanted the first split to have 732 training weeks of data (that brings us to the start of the 2017 season), so that is the .init argument. We want to make forecasts every 4 weeks, so we set .step = 4. And, we want the resulting dataset to have a column called .split which will define each separate dataset.\n\nflu_data_tscv &lt;- flu_data |&gt; \n  filter(epiweek &lt;= as.Date(\"2018-06-01\")) |&gt; \n  tsibble::stretch_tsibble(\n    .init = 732, \n    .step = 4,\n    .id = \".split\"\n    )\nflu_data_tscv\n\n# A tsibble: 7,500 x 4 [7D]\n# Key:       .split, region [10]\n   region epiweek     wili .split\n   &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt;  &lt;int&gt;\n 1 nat    2003-08-24 0.490      1\n 2 nat    2003-08-31 0.736      1\n 3 nat    2003-09-07 0.582      1\n 4 nat    2003-09-14 0.654      1\n 5 nat    2003-09-21 0.750      1\n 6 nat    2003-09-28 0.884      1\n 7 nat    2003-10-05 1.03       1\n 8 nat    2003-10-12 1.28       1\n 9 nat    2003-10-19 1.33       1\n10 nat    2003-10-26 1.77       1\n# ℹ 7,490 more rows\n\n\nThe resulting dataset (the top of which is shown above), now has many more rows than the initial dataset, since it basically is creating duplicate versions of the dataset, one for each unique value in the .split column.\nFor this experiment, let’s define four models:\n\na random walk: this is what it sounds like. Each forecast takes a random walk from the last observed value. This is equivalent to an ARIMA(0,1,0) model.\nthe AR(2) model that we fit earlier.\na model that just has fourier terms and no ARIMA terms (you have to specify pdq(0,0,0) otherwise fable will insert some auto-selected terms in there for you).\na model that has both the AR(2) and fourier terms.\n\nThe following code generates the forecasts for each cross-validation split and for all horizons.\n\ncv_forecasts &lt;- flu_data_tscv |&gt; \n  model(\n    rw = RW(my_fourth_root(wili)),\n    ar2 = ARIMA(my_fourth_root(wili) ~ pdq(2,0,0)),\n    fourier = ARIMA(my_fourth_root(wili) ~ pdq(0,0,0) + fourier(period = \"year\", K=3)),\n    fourier_ar2 = ARIMA(my_fourth_root(wili) ~ pdq(2,0,0) + fourier(period = \"year\", K=3))\n  ) |&gt; \n  forecast(h = 8) |&gt; \n  ## the following 3 lines of code ensure that there is a horizon variable in the forecast data\n  group_by(.split, .model) |&gt; \n  mutate(h = row_number()) |&gt; \n  ungroup() |&gt; \n  ## this ensures that the output is a table object\n  as_fable(response = \"wili\", distribution = wili)\n\nA common mistake in forecasting is to make forecasts and then just skip to looking at the metrics! Let’s pause here to look at a few of the forecasts.\n\ncv_forecasts |&gt;\n  filter(.split == 1) |&gt; \n  tsibble::update_tsibble(key=c(.model, region)) |&gt;  \n  as_fable(response = \"wili\", distribution = wili) |&gt; \n  autoplot(flu_data |&gt; filter(epiweek &gt;= as.Date(\"2016-09-01\"))) +\n  facet_wrap(~.model) +\n  labs(title = \"WILI, US level\",\n       y=\"% of visits\")\n\n\n\n\n\n\n\n\n\ncv_forecasts |&gt;\n  filter(.split == 4) |&gt; \n  tsibble::update_tsibble(key=c(.model, region)) |&gt;  \n  as_fable(response = \"wili\", distribution = wili) |&gt; \n  autoplot(flu_data |&gt; filter(epiweek &gt;= as.Date(\"2016-09-01\"))) +\n  facet_wrap(~.model) +\n  labs(title = \"WILI, US level\",\n       y=\"% of visits\")\n\n\n\n\n\n\n\n\n\ncv_forecasts |&gt;\n  filter(.split == 6) |&gt; \n  tsibble::update_tsibble(key=c(.model, region)) |&gt;  \n  as_fable(response = \"wili\", distribution = wili) |&gt; \n  autoplot(flu_data |&gt; filter(epiweek &gt;= as.Date(\"2016-09-01\"))) +\n  facet_wrap(~.model) +\n  labs(title = \"WILI, US level\",\n       y=\"% of visits\")\n\n\n\n\n\n\n\n\n\ncv_forecasts |&gt;\n  filter(.split == 8) |&gt; \n  tsibble::update_tsibble(key=c(.model, region)) |&gt;  \n  as_fable(response = \"wili\", distribution = wili) |&gt; \n  autoplot(flu_data |&gt; filter(epiweek &gt;= as.Date(\"2016-09-01\"))) +\n  facet_wrap(~.model) +\n  labs(title = \"WILI, US level\",\n       y=\"% of visits\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nBased on the above plots (and you could make a few more at different splits if you wanted to), which forecast model(s) do you expect to have the best forecast accuracy metrics? Why?",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#evaluating-the-experimental-forecasts",
    "href": "sessions/forecast-evaluation.html#evaluating-the-experimental-forecasts",
    "title": "Forecast evaluation",
    "section": "Evaluating the experimental forecasts",
    "text": "Evaluating the experimental forecasts\nLet’s start by looking at an overall tabular summary of forecast accuracy.\n\ncv_forecasts |&gt; \n  accuracy(\n    flu_data, \n    measures = list(mae = MAE, rmse = RMSE, crps=CRPS)\n  ) |&gt; \n  arrange(crps)\n\n# A tibble: 4 × 6\n  .model      region .type   mae  rmse  crps\n  &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 fourier_ar2 nat    Test  0.595  1.00  1.09\n2 ar2         nat    Test  0.929  1.38  1.53\n3 fourier     nat    Test  0.939  1.63  1.57\n4 rw          nat    Test  1.30   1.85  1.76\n\n\nAccording to each metric, the fourier_ar2 model is the most accurate overall.\nOften, results show differences across some of the experimental dimensions. Here, re-calculate the forecast errors and we plot the CRPS values as a function of horizon\n\ncv_forecasts |&gt; \n  accuracy(\n    flu_data, \n    by = c(\"h\", \".model\"), \n    measures = list(mae = MAE, rmse = RMSE, crps=CRPS)\n  ) |&gt; \n  ggplot(aes(x = h, y = crps, color = .model)) +\n  geom_point() +\n  geom_line()\n\n\n\n\n\n\n\n\nAnd here we compute the error based on the week the forecast was made and compare it to the observed data.\n\nscores_to_plot &lt;- cv_forecasts |&gt; \n  mutate(forecast_date = epiweek - h*7L) |&gt; \n  accuracy(\n    flu_data, \n    by = c(\"forecast_date\", \".model\"), \n    measures = list(mae = MAE, rmse = RMSE, crps=CRPS)\n  ) \n\np1 &lt;- scores_to_plot |&gt; \n  ggplot(aes(x = forecast_date, y = crps, color = .model)) +\n  geom_point() +\n  geom_line() +\n  theme(legend.position = \"bottom\")\n\ndate_range &lt;- range(scores_to_plot$forecast_date)\np2 &lt;- flu_data |&gt; \n  filter(epiweek &gt;= date_range[1],\n         epiweek &lt;= date_range[2]) |&gt; \n  autoplot(wili)\n\ngridExtra::grid.arrange(p1, p2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nHow do the CRPS scores change based on horizon? How do the CRPS scores change based on forecast date? What do these results this tell you about the models?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBy horizon\n\nNotably, the accuracy declines (the CRPS increases) for all models across horizons except for the fourier model. This is because the fourier model is predicting the same thing every week: seasonal average. The forecast does not align itself with recent data observations, it just predicts the historical seasonal average.\nAt horizons 1 and 2 the errors are similar for the ar2, fourier_ar2 and rw models, although the rw model has moderately higher CRPS.\nAt larger horizons, there is a clearer distinction between the models, with the fourier and fourier_ar2 models lower than the others, and fourier_ar2 showing the best accuracy of any of the models.\nIt is interesting to see that around 8 weeks the fourier_ar2 and fourier models converge in terms of accuracy. It seems that the auto-regressive components of the model seem to “add benefit” to the forecast accuracy for around 8 weeks. After that, a seasonal average is doing about the same.\n\nBy forecast date\n\nBy forecast_date, the models show similar forecast error for dates early and late in the season with the larger differences coming at the weeks with higher incidence.\nProjecting a flat line from the data (what the random walk does, essentially) does not do well at the important and dynamic times of the season. The combination of factoring in some auto-regressive and seasonal dynamics appears to do the best over this experiment.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#challenge",
    "href": "sessions/forecast-evaluation.html#challenge",
    "title": "Forecast evaluation",
    "section": "Challenge",
    "text": "Challenge\n\nIn which other ways could we summarise the performance of the forecasts?\nWhat other metrics could we use?\nWhich aspects of forecast accuracy does CRPS address and how?\nThere is no one-size-fits-all approach to forecast evaluation, often you will need to use a combination of metrics to understand the performance of your model and typically the metrics you use will depend on the context of the forecast. What attributes of the forecast are most important to you?\nOne useful way to think about evaluating forecasts is to consider exploring the scores as a data analysis in its own right. For example, you could look at how the scores change over time, how they change with different forecast horizons, or how they change with different models. This can be a useful way to understand the strengths and weaknesses of your model. Explore some of these aspects using the scores from this session.\nExpand the experiment run in the session above to include forecasts made in every week, instead of every 4 weeks. Does this change the evaluation results?",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/forecast-evaluation.html#methods-in-practice",
    "href": "sessions/forecast-evaluation.html#methods-in-practice",
    "title": "Forecast evaluation",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nThere are many other metrics that can be used to evaluate forecasts. The documentation for the {scoringutils} package has a good overview of these metrics and how to use them.",
    "crumbs": [
      "Forecast evaluation"
    ]
  },
  {
    "objectID": "sessions/end-of-course-summary-and-discussion.html",
    "href": "sessions/end-of-course-summary-and-discussion.html",
    "title": "End of course summary and discussion",
    "section": "",
    "text": "We hope that you’ve enjoyed taking this course on nowcasting and forecasting infectious disease dynamics. We will be very happy to have your feedback, comments or any questions on the course discussion page.\n\n\nEnd of course summary\n\n\n\nWe recommend exploring these topics in Further reading. You can also access these in a living library, which you are welcome to contribute to.\nAll the course material is openly available for you to come back to at any time. If you return to the course after some time, you may want to update your setup to ensure you have the latest content and package versions.\nYou are free to share and reuse this content in any way. Please consider including a citation (and/or let us know!) if you find it useful.",
    "crumbs": [
      "End of course summary and discussion"
    ]
  },
  {
    "objectID": "sessions/end-of-course-summary-and-discussion.html#slides",
    "href": "sessions/end-of-course-summary-and-discussion.html#slides",
    "title": "End of course summary and discussion",
    "section": "",
    "text": "End of course summary",
    "crumbs": [
      "End of course summary and discussion"
    ]
  },
  {
    "objectID": "sessions/end-of-course-summary-and-discussion.html#going-further",
    "href": "sessions/end-of-course-summary-and-discussion.html#going-further",
    "title": "End of course summary and discussion",
    "section": "",
    "text": "We recommend exploring these topics in Further reading. You can also access these in a living library, which you are welcome to contribute to.\nAll the course material is openly available for you to come back to at any time. If you return to the course after some time, you may want to update your setup to ensure you have the latest content and package versions.\nYou are free to share and reuse this content in any way. Please consider including a citation (and/or let us know!) if you find it useful.",
    "crumbs": [
      "End of course summary and discussion"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html",
    "href": "sessions/biases-in-delay-distributions.html",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "So far, we’ve looked at the uncertainty of the time delays between epidemiological events. The next challenge is that our information on these delays is usually biased, especially when we’re analysing data in real time. We’ll consider two types of biases that commonly occur in reported infectious disease data:\n\nCensoring: when we know an event occurred at some time, but not exactly when.\nTruncation: when not enough time has passed for all the relevant epidemiological events to occur or be observed.\n\nWe can again handle these by including them as uncertain parameters in the modelling process.\n\n\nIntroduction to biases in epidemiological delays\n\n\n\nIn this session, we’ll introduce censoring and right truncation as typical properties of the process that generates infectious disease data sets, using the delay from symptom onset to hospitalisation as an example.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/biases-in-delay-distributions.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the purrr package for functional programming, and the tidybayes package for extracting results from model fits.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"purrr\")\nlibrary(\"lubridate\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#slides",
    "href": "sessions/biases-in-delay-distributions.html#slides",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "Introduction to biases in epidemiological delays",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#objectives",
    "href": "sessions/biases-in-delay-distributions.html#objectives",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "In this session, we’ll introduce censoring and right truncation as typical properties of the process that generates infectious disease data sets, using the delay from symptom onset to hospitalisation as an example.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/biases-in-delay-distributions.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the purrr package for functional programming, and the tidybayes package for extracting results from model fits.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"purrr\")\nlibrary(\"lubridate\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#source-file",
    "href": "sessions/biases-in-delay-distributions.html#source-file",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "The source file of this session is located at sessions/biases-in-delay-distributions.qmd.",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#libraries-used",
    "href": "sessions/biases-in-delay-distributions.html#libraries-used",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the purrr package for functional programming, and the tidybayes package for extracting results from model fits.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"purrr\")\nlibrary(\"lubridate\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#initialisation",
    "href": "sessions/biases-in-delay-distributions.html#initialisation",
    "title": "Biases in delay distributions",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#understanding-double-interval-censoring",
    "href": "sessions/biases-in-delay-distributions.html#understanding-double-interval-censoring",
    "title": "Biases in delay distributions",
    "section": "Understanding double interval censoring",
    "text": "Understanding double interval censoring\n\nPrimary vs Secondary Event Censoring\nIt’s important to understand that double interval censoring can be decomposed into two distinct components:\nPrimary event censoring: Uncertainty about when the first event (symptom onset) occurred within its reported day. Even if we know someone developed symptoms “on 20 June”, we don’t know if this was at 00:01 or 23:59.\nSecondary event censoring: Uncertainty about when the second event (hospitalisation) occurred within its reported day. Similarly, “admitted on 22 June” could mean any time during those 24 hours.\n\n\nWhy this distinction matters\nA common mistake in epidemiological delay estimation is to only account for secondary event censoring (uncertainty in the outcome event) while ignoring primary event censoring (uncertainty in the initiating event).\nThis approach is often incorrect for epidemiological data because:\n\nPrimary events (infections, symptom onsets) are rarely observed at exact times\nIgnoring primary event censoring leads to systematic bias in delay estimates\nThe magnitude of bias depends on the epidemic growth rate and censoring intervals\n\nFor accurate delay estimation, we need to account for both components as we do in our “double interval censored” approach.\n\n\n\n\n\n\nWhy are we so worried?\n\n\n\n\n\nWe’ll be spending a lot of time thinking about a potentially “short” 24 hour time interval. In this session, we’ll start to get a sense for how variation, uncertainty, and bias within this interval might combine to impact daily outbreak estimates. The concepts and techniques that we use here also apply to longer time intervals that we might find in surveillance data, for example, reported case counts aggregated by week.",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#implementing-bias-correction-for-double-interval-censoring",
    "href": "sessions/biases-in-delay-distributions.html#implementing-bias-correction-for-double-interval-censoring",
    "title": "Biases in delay distributions",
    "section": "Implementing bias correction for double interval censoring",
    "text": "Implementing bias correction for double interval censoring\nLet’s estimate the time from symptom onset to hospitalisation with the censored data.\nA naïve approach to estimating the delay would be to ignore the fact that the data are censored. To estimate the delay from onset to hospitalisation, we could just use the difference between the censored times, which is an integer (the number of days).\n\ndf_dates &lt;- df_dates |&gt;\n  mutate(\n    incubation_period = onset_time - infection_time,\n    onset_to_hosp = hosp_time - onset_time\n  )\n\nTo help us think about this, let’s summarise the original time-based incubation period\n\nsummary(df$onset_time - df$infection_time)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3569  3.3499  4.6329  4.9559  6.2027 18.5544 \n\n\nand then the date based incubation period as we observe it.\n\nsummary(df_dates$incubation_period)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    3.00    5.00    4.96    6.00   19.00 \n\n\nYou should see that they have nearly the same means but different medians (by about half a day).\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nFit the lognormal model used in the session on delay distributions to the estimates from the rounded data, i.e. using the df_dates data set. Do you still recover the parameters that we put in?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmod &lt;- nfidd_cmdstan_model(\"lognormal\")\nres &lt;- nfidd_sample(mod,\n  data = list(\n    n = nrow(na.omit(df_dates)),\n    y = na.omit(df_dates)$onset_to_hosp + 0.01\n  )\n)\n\n\n\n\n\n\n\nTip\n\n\n\nNote the + 0.01 in the data argument. We have had to add this to avoid delays of 0 days. This should be a hint that we are not modelling the data correctly.\n\n\n\nres\n\n variable     mean   median   sd  mad       q5      q95 rhat ess_bulk ess_tail\n  lp__    -1633.72 -1633.41 1.00 0.70 -1635.78 -1632.76 1.00     1007     1347\n  meanlog     0.96     0.96 0.01 0.01     0.93     0.98 1.00     1848     1437\n  sdlog       0.60     0.60 0.01 0.01     0.58     0.62 1.00     1811     1220\n\n\nWe can calculate the mean and standard deviation to see the bias more clearly:\n\nres |&gt;\n  summarise_lognormal()\n\n      mean             sd       \n Min.   :2.969   Min.   :1.832  \n 1st Qu.:3.087   1st Qu.:2.017  \n Median :3.120   Median :2.058  \n Mean   :3.120   Mean   :2.059  \n 3rd Qu.:3.152   3rd Qu.:2.101  \n Max.   :3.270   Max.   :2.305  \n\n\nUsually the estimates will be further from the “true” parameters than before when we worked with the unrounded data.\n\n\n\nThere are many ad-hoc solutions to this problem that, for example, introduce a shift in the data by half a day to centre it on mid-day or discretise the distribution and use the difference between two cumulative density functions with a day, or two day interval. Of these all but the two-day interval approach introduce more bias than doing nothing as above. See the How epidemic dynamics affect bias severity callout for a visual comparison of these approaches and Park et al. for a detailed discussion of approximate approaches (Park et al. 2024).\nTo properly account for double interval censoring, we need to modify the model to include the fact that we don’t know when exactly on any given day the event happened. For example, if we know that symptom onset of an individual occurred on 20 June, 2024, and they were admitted to hospital on 22 June, 2024, this could mean an onset-to-hospitalisation delay from 1 day (onset at 23:59 on the 20th, admitted at 0:01 on the 22nd) to 3 days (onset at 0:01 on the 20th, admitted at 23:59 on the 22nd).\n\n\n\n\n\n\nVisualising two types of censoring\n\n\n\n\n\nLet’s visualise the difference between double and secondary censoring using a simple example (you can change the parameters to see how the bias changes).\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Set parameters for our example\nn &lt;- 1e4\nmeanlog &lt;- 1.0  # Mean ~3 days - try changing this\nsdlog &lt;- 0.5\nobs_time &lt;- 15\n\n# Generate true delay distribution\ntrue_delays &lt;- rlnorm(n, meanlog = meanlog, sdlog = sdlog)\n\n# Primary censoring only - add uniform uncertainty to primary event timing\nprimary_censored &lt;- true_delays + runif(n, 0, 1)\n\n# Secondary censoring only (common mistake) - just discretise\nsecondary_only &lt;- floor(true_delays)\n\n# Double censoring - discretise the primary censored delays\ndouble_censored &lt;- floor(primary_censored)\n\n# Filter to a reasonable range and create PMF for discrete data\nkeep_range &lt;- function(x) x[x &lt;= obs_time]\nprimary_filtered &lt;- keep_range(primary_censored)\nsecondary_filtered &lt;- keep_range(secondary_only)\ndouble_filtered &lt;- keep_range(double_censored)\n\n# Create PMF for discrete data\nsecondary_pmf &lt;- table(secondary_filtered) / length(secondary_filtered)\ndouble_pmf &lt;- table(double_filtered) / length(double_filtered)\n\n# Create the comparison plot\nggplot() +\n  # True distribution (black line)\n  geom_function(\n    fun = dlnorm,\n    args = list(meanlog = meanlog, sdlog = sdlog),\n    color = \"#252525\",\n    linewidth = 1.2\n  ) +\n  # Primary censoring (continuous, blue density)\n  geom_density(\n    data = data.frame(x = primary_filtered),\n    aes(x = x),\n    fill = \"#4292C6\",\n    col = \"#252525\",\n    alpha = 0.6\n  ) +\n  # Secondary censoring only (discrete, coral bars)\n  geom_col(\n    data = data.frame(\n      x = as.numeric(names(secondary_pmf)),\n      y = as.numeric(secondary_pmf)\n    ),\n    aes(x = x, y = y),\n    fill = \"#E31A1C\",\n    col = \"#252525\",\n    alpha = 0.6,\n    width = 0.9\n  ) +\n  # Double censoring (discrete, green bars)\n  geom_col(\n    data = data.frame(\n      x = as.numeric(names(double_pmf)),\n      y = as.numeric(double_pmf)\n    ),\n    aes(x = x, y = y),\n    fill = \"#20b986\",\n    col = \"#252525\",\n    alpha = 0.4,\n    width = 0.9\n  ) +\n  labs(\n    title = \"Comparison of Different Censoring Approaches\",\n    x = \"Delay (days)\",\n    y = \"Density / Probability Mass\",\n    caption = paste0(\n      \"Black line: True log-normal distribution\\n\",\n      \"Blue density: Primary censoring (continuous)\\n\",\n      \"Red bars: Secondary censoring only (common mistake)\\n\",\n      \"Green bars: Double censoring (both components)\"\n    )\n  ) +\n  scale_x_continuous(limits = c(0, 15)) +\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    plot.title = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0)\n  )\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_col()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\nThis figure demonstrates why accounting for only secondary censoring (red bars) is problematic - it alters the mean versus the true censoring process and so induces bias when double interval censored data is treated as single interval censored data.\n\n\n\nWe can use the interval information for both events in our delay estimation by making the exact time of the events based on the dates given part of the estimation procedure:\n\ncmod &lt;- nfidd_cmdstan_model(\"censored-delay-model\")\ncmod\n\n 1: data {\n 2:   int&lt;lower = 0&gt; n;\n 3:   array[n] int&lt;lower = 0&gt; onset_to_hosp;\n 4: }\n 5: \n 6: parameters {\n 7:   real meanlog;\n 8:   real&lt;lower = 0&gt; sdlog;\n 9:   array[n] real&lt;lower = 0, upper = 1&gt; onset_day_time;\n10:   array[n] real&lt;lower = 0, upper = 1&gt; hosp_day_time;\n11: }\n12: \n13: transformed parameters {\n14:   array[n] real&lt;lower = 0&gt; true_onset_to_hosp;\n15:   for (i in 1:n) {\n16:     true_onset_to_hosp[i] =\n17:       onset_to_hosp[i] + hosp_day_time[i] - onset_day_time[i];\n18:   }\n19: }\n20: \n21: model {\n22:   meanlog ~ normal(0, 10);\n23:   sdlog ~ normal(0, 10) T[0, ];\n24:   onset_day_time ~ uniform(0, 1);\n25:   hosp_day_time ~ uniform(0, 1);\n26: \n27:   true_onset_to_hosp ~ lognormal(meanlog, sdlog);\n28: }\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nFamiliarise yourself with the model above. Do you understand all the lines? Which line(s) define the parameter prior distribution(s), which one(s) the likelihood, and which one(s) reflect that we have now provided the delay as the difference in integer days?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLines 21-24 define the parametric prior distributions (for parameters meanlog and sdlog, and the estimates of exact times of events). The key idea here is that the true event can happen at any time within the observed interval for both events. Line 27 defines the likelihood. Lines 15-17 reflect the integer delays, adjusted by the estimated times of day.\n\n\n\nNow we can use this model to re-estimate the parameters of the delay distribution:\n\ncres &lt;- nfidd_sample(cmod,\n  data = list(\n    n = nrow(na.omit(df_dates)),\n    onset_to_hosp = na.omit(df_dates)$onset_to_hosp\n  )\n)\n\n\ncres\n\n          variable      mean    median    sd   mad        q5       q95 rhat\n lp__              -10218.47 -10217.65 51.14 51.97 -10307.51 -10132.67 1.02\n meanlog                0.98      0.98  0.01  0.01      0.96      1.00 1.00\n sdlog                  0.49      0.49  0.01  0.01      0.47      0.51 1.00\n onset_day_time[1]      0.34      0.28  0.24  0.25      0.03      0.80 1.00\n onset_day_time[2]      0.56      0.58  0.28  0.35      0.07      0.96 1.01\n onset_day_time[3]      0.57      0.61  0.29  0.34      0.07      0.96 1.01\n onset_day_time[4]      0.48      0.48  0.29  0.36      0.05      0.94 1.01\n onset_day_time[5]      0.34      0.28  0.25  0.27      0.02      0.82 1.01\n onset_day_time[6]      0.53      0.55  0.29  0.37      0.07      0.96 1.00\n onset_day_time[7]      0.33      0.28  0.24  0.26      0.02      0.80 1.00\n ess_bulk ess_tail\n      449      875\n     4645     1458\n     3442     1696\n     3332     1406\n     4040     1185\n     5005     1180\n     4174     1218\n     3996     1049\n     4118     1397\n     3122      917\n\n # showing 10 of 5394 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nWe can also examine the mean and standard deviation of the estimated delay distribution:\n\ncres |&gt;\n  summarise_lognormal()\n\n      mean             sd       \n Min.   :2.876   Min.   :1.426  \n 1st Qu.:2.981   1st Qu.:1.538  \n Median :3.005   Median :1.567  \n Mean   :3.005   Mean   :1.567  \n 3rd Qu.:3.029   3rd Qu.:1.596  \n Max.   :3.137   Max.   :1.767  \n\n\n\n\n\n\n\n\nTake 10 minutes\n\n\n\nTry resimulating the delays using different parameters of the delay distribution. Can you establish under which conditions the bias in recovered parameters gets worse? Is it the same as when we just simulated the data? If not, why not?",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#implementing-bias-correction-for-truncation",
    "href": "sessions/biases-in-delay-distributions.html#implementing-bias-correction-for-truncation",
    "title": "Biases in delay distributions",
    "section": "Implementing bias correction for truncation",
    "text": "Implementing bias correction for truncation\nIf we take the naïve mean of delays we get an underestimate as expected:\n\n# truncated mean delay\nmean(df_realtime$onset_to_hosp)\n\n[1] 5.952562\n\n# compare with the mean delay over the full outbreak\nmean(df$hosp_time - df$onset_time, na.rm=TRUE)\n\n[1] 6.382549\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nFit the lognormal model used above to the estimates from the truncated data, i.e. using the df_realtime data set. How far away from the “true” parameters do you end up?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nres &lt;- nfidd_sample(mod,\n  data = list(\n    n = nrow(na.omit(df_realtime)),\n    y = na.omit(df_realtime)$onset_to_hosp\n  )\n)\n\n\nres\n\n variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n  lp__    -141.69 -141.40 0.97 0.68 -143.63 -140.78 1.00      932     1230\n  meanlog    1.67    1.67 0.03 0.03    1.62    1.72 1.00     1872     1242\n  sdlog      0.47    0.47 0.02 0.02    0.43    0.51 1.00     1639     1203\n\n\nThe mean and standard deviation show the underestimation due to truncation:\n\nres |&gt;\n  summarise_lognormal()\n\n      mean             sd       \n Min.   :5.312   Min.   :2.334  \n 1st Qu.:5.824   1st Qu.:2.818  \n Median :5.950   Median :2.948  \n Mean   :5.952   Mean   :2.960  \n 3rd Qu.:6.083   3rd Qu.:3.098  \n Max.   :6.708   Max.   :3.852  \n\n\n\n\n\nOnce again, we can write a model that adjusts for truncation by re-creating the simulated truncation effect in the stan model:\n\ntmod &lt;- nfidd_cmdstan_model(\"truncated-delay-model\")\ntmod\n\n 1: data {\n 2:   int&lt;lower = 0&gt; n;\n 3:   array[n] real&lt;lower = 0&gt; onset_to_hosp;\n 4:   array[n] real&lt;lower = 0&gt; time_since_onset;\n 5: }\n 6: \n 7: parameters {\n 8:   real meanlog;\n 9:   real&lt;lower = 0&gt; sdlog;\n10: }\n11: \n12: model {\n13:   meanlog ~ normal(0, 10);\n14:   sdlog ~ normal(0, 10) T[0, ];\n15: \n16:   for (i in 1:n) {\n17:     onset_to_hosp[i] ~ lognormal(meanlog, sdlog) T[0, time_since_onset[i]];\n18:   }\n19: }\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nFamiliarise yourself with the model above. Which line introduces the truncation, i.e. the fact that we have not been able to observe hospitalisation times beyond the cutoff of (here) 70 days?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLine 17 defines the upper limit of onset_to_hosp as time_since_onset. This line introduces the truncation using the T[0, time_since_onset[i]] syntax, which is equivalent to target += lognormal_lpdf(onset_to_hosp[i] | meanlog, sdlog) - lognormal_lcdf(time_since_onset[i] | meanlog, sdlog). This is normalising the likelihood by the CDF of the relative observation time.\n\n\n\nNow we can use this model to re-estimate the parameters of the delay distribution:\n\ntres &lt;- nfidd_sample(tmod,\n  data = list(\n    n = nrow(df_realtime),\n    onset_to_hosp = df_realtime$onset_to_hosp, \n    time_since_onset = 70 - df_realtime$onset_time\n  )\n)\n\n\ntres\n\n variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n  lp__    -115.57 -115.27 0.98 0.70 -117.57 -114.65 1.01      745     1128\n  meanlog    1.77    1.77 0.04 0.04    1.71    1.84 1.01     1317     1197\n  sdlog      0.50    0.50 0.03 0.03    0.46    0.55 1.01     1050     1023\n\n\nLet’s also check the mean and standard deviation of the truncation-adjusted delay distribution:\n\ntres |&gt;\n  summarise_lognormal()\n\n      mean             sd       \n Min.   :5.719   Min.   :2.684  \n 1st Qu.:6.469   1st Qu.:3.351  \n Median :6.665   Median :3.561  \n Mean   :6.688   Mean   :3.604  \n 3rd Qu.:6.888   3rd Qu.:3.828  \n Max.   :8.161   Max.   :5.408  \n\n\n\n\n\n\n\n\nTake 10 minutes\n\n\n\nTry estimating the delays at different times (i.e. varying the observation cut-off) and also try re-simulating the delays using different parameters of the delay distribution. Can you establish under which conditions the bias in estimation gets worse?",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#decision-framework-for-bias-correction",
    "href": "sessions/biases-in-delay-distributions.html#decision-framework-for-bias-correction",
    "title": "Biases in delay distributions",
    "section": "Decision framework for bias correction",
    "text": "Decision framework for bias correction\nWhen faced with real epidemiological data, it can be challenging to decide which bias corrections to apply. The flowchart below provides practical guidance for making these decisions (Charniga et al. 2024):\n\n\n\nDecision framework for bias adjustment\n\n\n\n\n\n\n\n\nKey decision points\n\n\n\nThe flowchart highlights several critical decision points:\n\nTime variation interest: Whether you need to model changes in delays over time\nAnalysis type: Retrospective vs real-time analysis affects truncation concerns\nObservation cut-off: Whether truncation near primary events affects representativeness\nAscertainment method: Whether cases are identified via primary or secondary events",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#challenge",
    "href": "sessions/biases-in-delay-distributions.html#challenge",
    "title": "Biases in delay distributions",
    "section": "Challenge",
    "text": "Challenge\n\nWe have looked at censoring and truncation separately, but in reality often both are present. Can you combine the two in a model?\nThe solutions we introduced for addressing censoring and truncation are only some possible ones for the censoring problem. Other solutions reduce the biases from estimation even further. For an overview, see the review by (Park et al. 2024).",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#methods-in-practice",
    "href": "sessions/biases-in-delay-distributions.html#methods-in-practice",
    "title": "Biases in delay distributions",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nCharniga et al. (2024) summarises challenges in estimating delay distributions into a set of best practices, with a practical flowchart and reporting checklist.\nThe primarycensored R package provides a framework for dealing with censored and truncated delay distributions. It implements methods and techniques for handling primary event censoring, secondary event censoring, and right truncation in a unified and efficient manner with analytical solutions.\nThe epidist package extends primarycensored to work with brms for estimating epidemiological delay distributions. It enables flexible modelling including time-varying components, spatial effects, and partially pooled estimates of demographic characteristics.\nThe coarseDataTools package provides methods for double censored data but does not support truncation.",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "sessions/biases-in-delay-distributions.html#references",
    "href": "sessions/biases-in-delay-distributions.html#references",
    "title": "Biases in delay distributions",
    "section": "References",
    "text": "References\n\n\nCharniga, Kelly, Sang Woo Park, Andrei R. Akhmetzhanov, Anne Cori, Jonathan Dushoff, Sebastian Funk, Katelyn M. Gostic, et al. 2024. “Best Practices for Estimating and Reporting Epidemiological Delay Distributions of Infectious Diseases.” PLOS Computational Biology 20 (10): e1012520. https://doi.org/10.1371/journal.pcbi.1012520.\n\n\nPark, Sang Woo, Andrei R. Akhmetzhanov, Kelly Charniga, Anne Cori, Nicholas G. Davies, Jonathan Dushoff, Sebastian Funk, et al. 2024. “Estimating Epidemiological Delay Distributions for Infectious Diseases.” medRxiv. https://doi.org/10.1101/2024.01.12.24301247.",
    "crumbs": [
      "Biases in delay distributions"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html",
    "href": "reference/using-our-stan-models.html",
    "title": "Using our Stan models",
    "section": "",
    "text": "This guide covers how to use NFIDD’s Stan tools for working with both package models and custom Stan code.",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html#using-stan-models",
    "href": "reference/using-our-stan-models.html#using-stan-models",
    "title": "Using our Stan models",
    "section": "Using Stan Models",
    "text": "Using Stan Models\n\nBasic Package Models\nUse models included in the NFIDD package:\n\nnfidd_stan_models()\n\n [1] \"censored-delay-model\"           \"coin\"                          \n [3] \"estimate-inf-and-r-rw-forecast\" \"estimate-inf-and-r-rw\"         \n [5] \"estimate-inf-and-r\"             \"estimate-infections\"           \n [7] \"estimate-r\"                     \"gamma\"                         \n [9] \"joint-nowcast-with-r\"           \"joint-nowcast\"                 \n[11] \"lognormal\"                      \"mechanistic-r\"                 \n[13] \"simple-nowcast-rw\"              \"simple-nowcast\"                \n[15] \"statistical-r\"                  \"truncated-delay-model\"         \n\n\nCreate a model using a package model:\n\nmodel &lt;- nfidd_cmdstan_model(\"simple-nowcast\")\n\nSample with course defaults (faster):\n\nfit &lt;- nfidd_sample(model, data = your_data)\n\n\n\nCustom Stan Files\nUse a custom Stan file:\n\nmodel &lt;- nfidd_cmdstan_model(model_file = \"path/to/your/model.stan\")\n\nYou still get access to NFIDD Stan functions:\n\nfit &lt;- nfidd_sample(model, data = your_data)\n\n\n\nCustom Include Paths\nSet globally using R options:\n\noptions(nfidd.stan_path = \"/path/to/your/stan\")\nmodel &lt;- nfidd_cmdstan_model(\"simple-nowcast\")\n\nOr override per-model:\n\nmodel &lt;- nfidd_cmdstan_model(\n  model_file = \"custom.stan\",\n  include_paths = c(\"/custom/path1\", \"/custom/path2\")\n)",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html#working-with-stan-functions",
    "href": "reference/using-our-stan-models.html#working-with-stan-functions",
    "title": "Using our Stan models",
    "section": "Working with Stan Functions",
    "text": "Working with Stan Functions\n\nDiscover Available Functions\nGet all function names from NFIDD:\n\nfunctions &lt;- nfidd_stan_functions()\nfunctions\n\n[1] \"combine_obs_with_predicted_obs_rng\" \"condition_onsets_by_report\"        \n[3] \"convolve_with_delay\"                \"geometric_diff_ar\"                 \n[5] \"geometric_random_walk\"              \"observe_onsets_with_delay\"         \n[7] \"pop_bounded_renewal\"                \"renewal\"                           \n\n\nFind which files contain specific functions:\n\nfiles &lt;- nfidd_stan_function_files(functions = c(\"renewal\"))\nfiles\n\n[1] \"functions/renewal.stan\"\n\n\n\n\nExtract Functions for Local Use\nLoad specific functions as a string:\n\nrenewal_code &lt;- nfidd_load_stan_functions(\n  functions = c(\"renewal\")\n)\n\nShow first few lines:\n\ncat(substr(renewal_code, 1, 200), \"...\")\n\n// Stan functions from nfidd version 1.2.0.9000\narray[] real renewal(real I0, array[] real R, array[] real gen_time) {\n  // length of time series\n  int n = num_elements(R);\n  int max_gen_time = num_el ...\n\n\nWrite functions to a temporary file for demonstration:\n\ntemp_file &lt;- file.path(tempdir(), \"my_functions.stan\")\nnfidd_load_stan_functions(\n  functions = c(\"renewal\"),\n  write_to_file = TRUE,\n  output_file = temp_file,\n  wrap_in_block = TRUE\n)\n\nStan functions written to: /tmp/Rtmp2y3WDe/my_functions.stan\n\n\n[1] \"functions {\\n// Stan functions from nfidd version 1.2.0.9000\\narray[] real renewal(real I0, array[] real R, array[] real gen_time) {\\n  // length of time series\\n  int n = num_elements(R);\\n  int max_gen_time = num_elements(gen_time);\\n  array[n + 1] real I;\\n  I[1] = I0;\\n  for (i in 1:n) {\\n    int first_index = max(1, i - max_gen_time + 1);\\n    int len = i - first_index + 1;\\n    array[len] real I_segment = I[first_index:i];\\n    array[len] real gen_pmf = reverse(gen_time[1:len]);\\n    I[i + 1] = dot_product(I_segment, gen_pmf) * R[i];\\n  }\\n  return(I[2:(n + 1)]);\\n}\\n}\"\n\n\nVerify file was created:\n\ncat(\"File created at:\", temp_file)\n\nFile created at: /tmp/Rtmp2y3WDe/my_functions.stan\n\ncat(\"\\nFile exists:\", file.exists(temp_file))\n\n\nFile exists: TRUE\n\n\n\n\nLoad All Functions\nGet all NFIDD functions:\n\nall_functions &lt;- nfidd_load_stan_functions()\n\nWrite all functions to file:\n\nnfidd_load_stan_functions(\n  write_to_file = TRUE,\n  output_file = \"nfidd_functions.stan\",\n  wrap_in_block = TRUE\n)",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html#writing-package-models-locally",
    "href": "reference/using-our-stan-models.html#writing-package-models-locally",
    "title": "Using our Stan models",
    "section": "Writing Package Models Locally",
    "text": "Writing Package Models Locally\nSometimes you want to copy a package model to modify it locally rather than modifying the package source.\nLoad a package model:\n\nmodel &lt;- nfidd_cmdstan_model(\"simple-nowcast\", compile = FALSE)\n\nGet the Stan code from the model:\n\nstan_code &lt;- model$code()\n\nWrite it to a local file:\n\nwriteLines(stan_code, \"local-simple-nowcast.stan\")\n\nCreate functions directory:\n\ndir.create(\"functions\", showWarnings = FALSE)\n\nCopy all function files individually:\n\nstan_functions_path &lt;- file.path(nfidd_stan_path(), \"functions\")\nfunction_files &lt;- list.files(stan_functions_path, pattern = \"\\\\.stan$\", full.names = TRUE)\n\nfor (file in function_files) {\n  file.copy(file, \"functions/\", overwrite = TRUE)\n}\n\nSet options to use local functions:\n\noptions(nfidd.stan_path = \".\")\n\nNow you can modify the local model and it will use local functions:\n\nmodified_model &lt;- nfidd_cmdstan_model(model_file = \"local-simple-nowcast.stan\")",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html#practical-workflows",
    "href": "reference/using-our-stan-models.html#practical-workflows",
    "title": "Using our Stan models",
    "section": "Practical Workflows",
    "text": "Practical Workflows\n\nBuilding Custom Models with NFIDD Functions\nExtract the functions you need:\n\nnfidd_load_stan_functions(\n  functions = c(\"renewal\", \"convolve_with_delay\"),\n  write_to_file = TRUE,\n  output_file = \"my_functions.stan\"\n)\n\nCreate your custom model file:\n\n#include my_functions.stan\n\ndata {\n  // Your data block\n}\n\nparameters {\n  // Your parameters\n}\n\nmodel {\n  // Use NFIDD functions like renewal(), delay_pmf(), etc.\n}\n\nCompile and use:\n\nmodel &lt;- nfidd_cmdstan_model(\n  model_file = \"my_custom_model.stan\",\n  include_paths = \".\"\n)\n\n\n\nDevelopment Workflow\nExplore existing models:\nSee what models are available:\n\nnfidd_stan_models()\n\n [1] \"censored-delay-model\"           \"coin\"                          \n [3] \"estimate-inf-and-r-rw-forecast\" \"estimate-inf-and-r-rw\"         \n [5] \"estimate-inf-and-r\"             \"estimate-infections\"           \n [7] \"estimate-r\"                     \"gamma\"                         \n [9] \"joint-nowcast-with-r\"           \"joint-nowcast\"                 \n[11] \"lognormal\"                      \"mechanistic-r\"                 \n[13] \"simple-nowcast-rw\"              \"simple-nowcast\"                \n[15] \"statistical-r\"                  \"truncated-delay-model\"         \n\n\nLook at model locations:\n\nnfidd_stan_path()\n\n[1] \"/home/runner/work/_temp/Library/nfidd/stan\"\n\n\nUnderstand function dependencies:\nSee all available functions:\n\nnfidd_stan_functions()\n\nFind which files contain functions you need:\n\nnfidd_stan_function_files(functions = c(\"function_name\"))\n\nBuild incrementally:\nStart with package model:\n\nbase_model &lt;- nfidd_cmdstan_model(\"simple-nowcast\")\n\nExtend with custom functions:\n\ncustom_model &lt;- nfidd_cmdstan_model(\n  model_file = \"extended_model.stan\"\n)",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/using-our-stan-models.html#file-organisation",
    "href": "reference/using-our-stan-models.html#file-organisation",
    "title": "Using our Stan models",
    "section": "File Organisation",
    "text": "File Organisation\nFor projects using custom Stan code:\nyour_project/\n├── models/\n│   ├── my_model.stan\n│   └── functions/\n│       └── my_functions.stan\nIn your R code:\n\noptions(nfidd.stan_path = c(\"models/functions\", nfidd_stan_path()))\nmodel &lt;- nfidd_cmdstan_model(model_file = \"models/my_model.stan\")\n\nThis setup gives you access to both your custom functions and all NFIDD functions.",
    "crumbs": [
      "Using our Stan models"
    ]
  },
  {
    "objectID": "reference/sessions.html",
    "href": "reference/sessions.html",
    "title": "Session timetable",
    "section": "",
    "text": "Wednesday - Half day starting at 1:30pm\n\n\nWednesday: 13.30-14.00\n\nIntroduction: the course and the instructors (10 mins)\nGetting started: getting started with the course (5 mins)\nIntroduction to the course: How is the course taught? (5 mins)\nMotivating the course: from an epidemiological line list to informing decisions in real-time (10 mins)\n\n\n\n\nWednesday: 14.00-14.45\n\nIntroduction: epidemiological delays and how to represent them with probability distributions (10 mins)\nPractice: simulate and estimate epidemiological delays (30 mins)\nWrap up (5 mins)\n\n\n\n\nWednesday: 14.45-15.30 and 16.00-16.45\n\nIntroduction: biases in delay distributions (15 mins)\nPractice: interval censoring - estimating delays from discrete dates (30 mins)\nPractice: right truncation - estimating delays in real-time with incomplete data (30 mins)\nWrap up (5 mins)\n\n\n\n\nWednesday: 16.45-17.30\n\nIntroduction: Using delay distributions to model the data generating process of an epidemic (10 mins)\nPractice: implementing a convolution model and identifying potential problems (30 mins)\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-0-introduction-and-course-overview",
    "href": "reference/sessions.html#session-0-introduction-and-course-overview",
    "title": "Session timetable",
    "section": "",
    "text": "Wednesday: 13.30-14.00\n\nIntroduction: the course and the instructors (10 mins)\nGetting started: getting started with the course (5 mins)\nIntroduction to the course: How is the course taught? (5 mins)\nMotivating the course: from an epidemiological line list to informing decisions in real-time (10 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-1-delay-distributions",
    "href": "reference/sessions.html#session-1-delay-distributions",
    "title": "Session timetable",
    "section": "",
    "text": "Wednesday: 14.00-14.45\n\nIntroduction: epidemiological delays and how to represent them with probability distributions (10 mins)\nPractice: simulate and estimate epidemiological delays (30 mins)\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-2-biases-in-delay-distributions",
    "href": "reference/sessions.html#session-2-biases-in-delay-distributions",
    "title": "Session timetable",
    "section": "",
    "text": "Wednesday: 14.45-15.30 and 16.00-16.45\n\nIntroduction: biases in delay distributions (15 mins)\nPractice: interval censoring - estimating delays from discrete dates (30 mins)\nPractice: right truncation - estimating delays in real-time with incomplete data (30 mins)\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-3-using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic",
    "href": "reference/sessions.html#session-3-using-delay-distributions-to-model-the-data-generating-process-of-an-epidemic",
    "title": "Session timetable",
    "section": "",
    "text": "Wednesday: 16.45-17.30\n\nIntroduction: Using delay distributions to model the data generating process of an epidemic (10 mins)\nPractice: implementing a convolution model and identifying potential problems (30 mins)\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-4-r_t-estimation-and-the-renewal-equation",
    "href": "reference/sessions.html#session-4-r_t-estimation-and-the-renewal-equation",
    "title": "Session timetable",
    "section": "Session 4: \\(R_t\\) estimation and the renewal equation",
    "text": "Session 4: \\(R_t\\) estimation and the renewal equation\nThursday: 09.15-10.00\n\nIntroduction: the time-varying reproduction number (10 mins)\nPractice: using the renewal equation to estimate R (30 mins)\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-5-nowcasting-concepts",
    "href": "reference/sessions.html#session-5-nowcasting-concepts",
    "title": "Session timetable",
    "section": "Session 5: Nowcasting concepts",
    "text": "Session 5: Nowcasting concepts\nThursday: 10.00-10.30\n\nIntroduction: nowcasting as a right-truncation problem (10 mins)\nPractice: simulating and basic nowcasting (20 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-6-nowcasting-with-an-unknown-reporting-delay",
    "href": "reference/sessions.html#session-6-nowcasting-with-an-unknown-reporting-delay",
    "title": "Session timetable",
    "section": "Session 6: Nowcasting with an unknown reporting delay",
    "text": "Session 6: Nowcasting with an unknown reporting delay\nThursday: 11.00-12.00\n\nIntroduction: joint estimation of delays and nowcasts (10 mins)\nPractice:\n\njoint estimation of delays and nowcasts (20 mins)\njoint estimation of delays, nowcasts and reproduction numbers (20 mins)\n\nWrap up (10 mins)\n\nThursday: 12.00-13.00\n\nLunch",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-7-forecasting-concepts",
    "href": "reference/sessions.html#session-7-forecasting-concepts",
    "title": "Session timetable",
    "section": "Session 7: Forecasting concepts",
    "text": "Session 7: Forecasting concepts\nThursday: 13.00-13.30\n\nIntroduction: forecasting as an epidemiological problem (10 mins)\nPractice: basic forecasting with ARIMA models (20 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-8-improving-forecasting-models",
    "href": "reference/sessions.html#session-8-improving-forecasting-models",
    "title": "Session timetable",
    "section": "Session 8: Improving forecasting models",
    "text": "Session 8: Improving forecasting models\nThursday: 13.30-14.15\n\nIntroduction: ARIMA concepts and transformations (10 mins)\nPractice: ACF plots, transformations, and seasonality (30 mins)\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-9-forecast-evaluation",
    "href": "reference/sessions.html#session-9-forecast-evaluation",
    "title": "Session timetable",
    "section": "Session 9: Forecast evaluation",
    "text": "Session 9: Forecast evaluation\nThursday: 14.15-15.00\n\nIntroduction: scoring rules and metrics (10 mins)\nPractice: evaluating forecasts with various metrics (30 mins)\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-10-forecasting-with-ensembles",
    "href": "reference/sessions.html#session-10-forecasting-with-ensembles",
    "title": "Session timetable",
    "section": "Session 10: Forecasting with ensembles",
    "text": "Session 10: Forecasting with ensembles\nThursday: 15.30-17.00\n\nIntroduction: strategies for collating and combining models (10 mins)\nPractice: evaluating methods for ensemble forecasts (75 mins)\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-11-evaluating-real-world-outbreak-forecasts",
    "href": "reference/sessions.html#session-11-evaluating-real-world-outbreak-forecasts",
    "title": "Session timetable",
    "section": "Session 11: Evaluating real-world outbreak forecasts",
    "text": "Session 11: Evaluating real-world outbreak forecasts\nFriday: 09.15-10.30\n\nIntroduction: hub concepts and data structures (10 mins)\nPractice:\n\nevaluation example with existing hub (30 mins)\nensembles with existing hub (30 mins)\n\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-12-local-hub-playground",
    "href": "reference/sessions.html#session-12-local-hub-playground",
    "title": "Session timetable",
    "section": "Session 12: Local hub playground",
    "text": "Session 12: Local hub playground\nFriday: 11.00-12.00\n\nIntroduction: Why a local hub? (10 mins)\nPractice: getting stuck in with a local hub (45 mins)\nWrap up (5 mins)\n\nFriday: 12.00-13.00\n\nLunch",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-13-combining-nowcasting-and-forecasting",
    "href": "reference/sessions.html#session-13-combining-nowcasting-and-forecasting",
    "title": "Session timetable",
    "section": "Session 13: Combining nowcasting and forecasting",
    "text": "Session 13: Combining nowcasting and forecasting\nFriday: 13.00-14.00\n\nIntroduction: Why combine approaches? (10 mins)\nPractice: connecting nowcasting and forecasting (45 mins)\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-14-catch-up-and-deeper-exploration",
    "href": "reference/sessions.html#session-14-catch-up-and-deeper-exploration",
    "title": "Session timetable",
    "section": "Session 14: Catch-Up and Deeper Exploration",
    "text": "Session 14: Catch-Up and Deeper Exploration\nFriday: 14.00-15.00\n\nCatch-up and deeper exploration\nWrap up (5 mins)",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/sessions.html#session-15-research-talks-and-wrap-up",
    "href": "reference/sessions.html#session-15-research-talks-and-wrap-up",
    "title": "Session timetable",
    "section": "Session 15: Research talks and wrap-up",
    "text": "Session 15: Research talks and wrap-up\nFriday: 15.30-17.00\n\nResearch talk 1: Nick (15 mins)\nResearch talk: Why am I so late?\nResearch talk 3: Nick (15 mins)\nResearch talk 4: Nick (15 mins)\nSummary of the course (10 mins)\nFinal discussion and closing (20 mins)\n\nFurther reading",
    "crumbs": [
      "Session timetable"
    ]
  },
  {
    "objectID": "reference/help.html",
    "href": "reference/help.html",
    "title": "Getting help",
    "section": "",
    "text": "For any questions about the course or its content, feel free to use the Discussion board",
    "crumbs": [
      "Getting help"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nowcasting and forecasting infectious disease dynamics - SISMID",
    "section": "",
    "text": "A course and living resource for learning about nowcasting and forecasting infectious disease dynamics originally developed by the nfidd team and adapted for SISMID.\nWe invite anyone to use the materials here in their own teaching and learning. For questions and discussion of the course or its content, we welcome all users to the Discussion board. We welcome all forms of contributions, additions and suggestions for improving the materials.\nAll materials here are provided under the permissive MIT License."
  },
  {
    "objectID": "authors.html",
    "href": "authors.html",
    "title": "Authors",
    "section": "",
    "text": "The NFIDD course was developed by the NFIDD course contributors:\nThe SISMID course was adapted by the SISMID course contributors:"
  },
  {
    "objectID": "authors.html#how-to-cite",
    "href": "authors.html#how-to-cite",
    "title": "Authors",
    "section": "How to Cite",
    "text": "How to Cite\nIf you use materials from this course in your work, please cite:\n\nNFIDD course contributors (2025). NFIDD: Nowcasting and Forecasting Infectious Disease Dynamics. Version [VERSION]. DOI: [PLACEHOLDER - Zenodo DOI will be added upon release]\n\n\nSISMID course contributors (2025). NFIDD SISMID: Nowcasting and Forecasting Infectious Disease Dynamics - SISMID. Version [VERSION]. DOI: [PLACEHOLDER - Zenodo DOI will be added upon release]\n\n\nBibTeX Entry\n@misc{nfidd2025,\n  author = {{Sam Abbott, Katherine Sherratt, Sebastian Funk}},\n  title = {{NFIDD: Nowcasting and Forecasting Infectious Disease Dynamics}},\n  year = {2025},\n  version = {[VERSION]},\n  doi = {[PLACEHOLDER - Zenodo DOI]},\n  url = {https://nfidd.github.io/nfidd/}\n}\n@misc{sismid2025,\n  author = {{Sam Abbott, Thomas Robacker, Nick Reich}},\n  title = {{NFIDD SISMID: Nowcasting and Forecasting Infectious Disease Dynamics - SISMID}},\n  year = {2025},\n  version = {[VERSION]},\n  doi = {[PLACEHOLDER - Zenodo DOI]},\n  url = {https://nfidd.github.io/sismid/}\n}"
  },
  {
    "objectID": "authors.html#funding",
    "href": "authors.html#funding",
    "title": "Authors",
    "section": "Funding",
    "text": "Funding\nThese training materials have been developed with support by the National Institutes of General Medical Sciences (R35GM119582), the US Centers for Disease Control and Prevention (NU38FT000008), and the Wellcome Trust (210758/Z/18/Z). The content is solely the responsibility of the authors and does not necessarily represent the official views of NIGMS, the National Institutes of Health, the CDC, or the Wellcome Trust."
  },
  {
    "objectID": "authors.html#license",
    "href": "authors.html#license",
    "title": "Authors",
    "section": "License",
    "text": "License\nAll course materials are provided under the MIT License, making them freely available for teaching, learning, and adaptation."
  },
  {
    "objectID": "authors.html#contributing",
    "href": "authors.html#contributing",
    "title": "Authors",
    "section": "Contributing",
    "text": "Contributing\nWe welcome contributions to improve and expand the course materials. You can:\n\nReport issues or suggest improvements\nJoin discussions\nSubmit pull requests"
  },
  {
    "objectID": "authors.html#acknowledgements",
    "href": "authors.html#acknowledgements",
    "title": "Authors",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nWe thank all participants who have helped improve these materials through feedback, bug reports, and code contributions."
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "NFIDD (Nowcasting and Forecasting Infectious Disease Dynamics) is an MIT-licensed library of sessions for learning about nowcasting and forecasting of infectious disease surveillance data. We have adapted this course for the SISMID course on nowcasting and forecasting infectious disease dynamics. This course, in both its original and SISMID versions, is a living resource designed to help epidemiologists, public health professionals, and researchers understand and apply real-time analysis methods to infectious disease surveillance data."
  },
  {
    "objectID": "getting-started.html#what-is-nfidd",
    "href": "getting-started.html#what-is-nfidd",
    "title": "Getting started",
    "section": "",
    "text": "NFIDD (Nowcasting and Forecasting Infectious Disease Dynamics) is an MIT-licensed library of sessions for learning about nowcasting and forecasting of infectious disease surveillance data. We have adapted this course for the SISMID course on nowcasting and forecasting infectious disease dynamics. This course, in both its original and SISMID versions, is a living resource designed to help epidemiologists, public health professionals, and researchers understand and apply real-time analysis methods to infectious disease surveillance data."
  },
  {
    "objectID": "getting-started.html#set-up",
    "href": "getting-started.html#set-up",
    "title": "Getting started",
    "section": "Set up",
    "text": "Set up\nEach session in this course uses R code for demonstration. All the content is self-contained within a software package designed for the course.\nYou have three options for using this course:\n\nWeb-only: View sessions on the website\nLocal setup: Install R, packages, and download course materials for the full interactive experience\nHybrid: Install just the packages but use the website for viewing content\n\n\n\n\n\n\n\nImportant\n\n\n\nInstallation Issues? If you’re having trouble with any installation steps, ask for help early! Don’t skip ahead - each step builds on the previous ones. On the day of the course, we have a small number of web clients available as backup if installation issues persist.\n\n\n\nSummary of Installation Steps\nIf you choose the local setup option, here’s what you’ll need to do:\n\nInstall R and RStudio\nInstall the nfidd R package\nInstall cmdstan\nDownload course materials (if using the hybrid approach you may not need to do this)\n\nDon’t skip any steps - they all work together to provide the full course experience.\n\n\nInstalling R\n\nR is used as the main programming language. You can check which version you have by typing R.version in your R session. We recommend installing the latest R version 4.5.1 (2025-06-13).\nRStudio is a popular graphic user interface (GUI). Its Visual Editor provides the best experience of going through this course. Please make sure you update RStudio to the latest version.\n\n\n\nInstalling additional requirements\nBefore you get started with the course, you will first need to install the following software.\n\nInstallation of the nfidd package\nTo install the packages needed in the course, including the nfidd package that contains data files and helper functions used throughout:\n\nif (!require(\"pak\")) {\n    install.packages(\"pak\")\n}\npak::pak(\"nfidd/sismid\", dependencies = TRUE)\n\n\n\n\n\n\n\nNote\n\n\n\nIf pak fails to install, you can try using remotes as an alternative:\n\nif (!require(\"remotes\")) {\n    install.packages(\"remotes\")\n}\nremotes::install_github(\"nfidd/sismid\", dependencies = TRUE)\n\n\n\nThen you can check that the installation completed successfully by loading the package into your R session:\n\nlibrary(\"nfidd\")\n\n\n\n\nInstalling cmdstan\nThe course relies on running stan through the cmdstanr R package, which itself uses the cmdstan software. This requires a separate installation step:\n\ncmdstanr::install_cmdstan()\n\n\n\n\n\n\n\nNote\n\n\n\nThis may take a few minutes. Also you’re likely to see lots of warnings and other messages printed to your screen - don’t worry, this is normal and doesn’t mean there is a problem.\n\n\nIf there are any problems with this, you can try (on Windows) to fix them using\n\ncmdstanr::check_cmdstan_toolchain(fix = TRUE)\n\nYou can test that you have a working cmdstanr setup using\n\ncmdstanr::cmdstan_version()\n\n[1] \"2.36.0\"\n\n\nFor more details, and for links to resources in case something goes wrong, see the Getting Started with CmdStanr vignette of the package."
  },
  {
    "objectID": "getting-started.html#accessing-the-course",
    "href": "getting-started.html#accessing-the-course",
    "title": "Getting started",
    "section": "Accessing the course",
    "text": "Accessing the course\nIf you want to use the local workflow (recommended), you will need a local copy of the course material.\n\nDirectly download the course material:\n\n\n\n\n\n\nTip\n\n\n\nDownload\n\n\nAlternatively, if you are familiar with git you can clone the repo.\nIf you prefer to use a hybrid workflow, you can view each session on the website (where formatting is nicest). Using this approach you can either copy-paste the code from the webpages into your own R script or use the .qmd files as Notebooks where you can go from chunk to chunk running the code.\n\nTip: if you decide to copy-paste code, then you don’t need to download the material.\nTip: if you hover over each code chunk on the website you can use a “Copy” button at the top right corner.\n\n\n\nInteracting with a local copy of the course material\nA benefit of downloading or cloning all the material is that you can interact with the session files directly.\nIn this course, all content is written using Quarto notebooks (.qmd files). This means that we can combine text with code and see the output directly. The notebooks are then directly reproduced on the course website (for example, this page).\nRecommended approach: Work with the notebooks using RStudio’s visual editor mode. See guidance on this below.\n\n\n\n\n\n\nUsing RStudio’s Visual Editor (Recommended for Notebooks)\n\n\n\n\nOpen a session notebook: Each session is saved as a .qmd file in /sessions/.\nSwitch to Visual mode: Look for the “Visual” button in the top-left of the editor pane (next to “Source”).\nExecute code: Use the green “play” button at the top-right corner of each code chunk, or Ctrl/Cmd + Enter for line-by-line execution.\nVisual mode benefits: Easier to read formatted text and equations, better experience with code chunks and outputs\n\n\n\nAlternative approaches that also work:\n\nOther visual editors: Use VS Code or other editors that support Quarto notebooks. The .qmd files will work in any Quarto-compatible environment.\n\n\n\n\n\n\n\nTip\n\n\n\nThe Quarto extension for VS Code also supports a visual editor mode. You can find it in the command palette.\n\n\n\nOther source code editors: Use RStudio, VS Code, or other editors that support interactive notebooks (without necessarily using the WYSIWYG formatting of “Visual” mode) and use the notebooks, but only in the source-code mode. This can be helpful if you want a more bare-bones experience of interacting with the code and data."
  },
  {
    "objectID": "getting-started.html#day-of-course-updates",
    "href": "getting-started.html#day-of-course-updates",
    "title": "Getting started",
    "section": "Day-of-Course Updates",
    "text": "Day-of-Course Updates\nIf you’re returning to the course after some time or joining a live session, you may want to update your setup to ensure you have the latest content and package versions.\n\nQuick Update (Recommended)\n\nUpdate the nfidd package (this is quick and ensures you have the latest functions):\n\npak::pak(\"nfidd/sismid\", dependencies = TRUE)\n\nDownload fresh course materials if using local files:\n\nDownload the latest version: Download\nOr use git pull if you cloned the repository\n\nYou don’t need to reinstall cmdstan unless you’re having specific issues with it"
  },
  {
    "objectID": "reference/further_reading.html",
    "href": "reference/further_reading.html",
    "title": "Further reading",
    "section": "",
    "text": "The following is a highly subjective list of papers we would recommend to read for those interested in engaging further with the topics discussed here. You can also access this via Zotero in an open living library, which you are welcome to contribute to.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#delay-estimation",
    "href": "reference/further_reading.html#delay-estimation",
    "title": "Further reading",
    "section": "Delay estimation",
    "text": "Delay estimation\n\nPark et al. (2024) provide a comprehensive overview of challenges in estimating delay distribution and how to overcome them.\nCharniga et al. (2024) summarises challenges in estimating delay distributions into a set of best practices.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#r_t-estimation",
    "href": "reference/further_reading.html#r_t-estimation",
    "title": "Further reading",
    "section": "\\(R_t\\) estimation",
    "text": "\\(R_t\\) estimation\n\nGostic et al. (2020) provides an overview of some of the challenges in estimating reproduction numbers.\nBrockhaus et al. (2023) compares reproduction number estimates from different models and investigates their differences.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#nowcasting",
    "href": "reference/further_reading.html#nowcasting",
    "title": "Further reading",
    "section": "Nowcasting",
    "text": "Nowcasting\n\nWolffram et al. (2023) compares the performance of a range of methods that were used in a nowcasting hub and investigates what might explain performance differences.\nLison et al. (2024) develops a generative model for nowcasting and \\(R_t\\) estimation and compares its performance to an approach where the steps for estimating incidence and reproduction number are separated.\nStoner, Economou, and Halliday (2020) contains a nice review of different methods for nowcasting evaluates a range of methods, in addition to introducing a new approach.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#forecasting",
    "href": "reference/further_reading.html#forecasting",
    "title": "Further reading",
    "section": "Forecasting",
    "text": "Forecasting\n\nFunk et al. (2019) evaluates the performance of a forecasting method that combines a mechanistic SEIR model with a random walk prior for the reproduction number.\nHeld, Meyer, and Bracher (2017) makes a compelling argument for the use of probabilistic forecasts and evaluates spatial forecasts based on routine surveillance data.\nLopez et al. (2024) describes the difficulty encountered in making accurate forecasts in COVID-19 cases in the US COVID-19 Forecast Hub.\nAsher (2018) describes model that implements an extension to the random walk with a drift term.\n“CDCgov/Wastewater-Informed-Covid-Forecasting” (2025), code repo for Wastewater-informed COVID-19 forecasting.\nHyndman and Athanasopoulos (2021) is a free online text book on forecasting with a range of time series models and a great resource for finding out more about them.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#ensembles",
    "href": "reference/further_reading.html#ensembles",
    "title": "Further reading",
    "section": "Ensembles",
    "text": "Ensembles\n\nSherratt et al. (2023) investigates the performance of different ensembles in the European COVID-19 Forecast Hub.\nAmaral et al. (2025) discusses the challenges in improving on the predictive performance of simpler approaches using weighted ensembles.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/further_reading.html#references",
    "href": "reference/further_reading.html#references",
    "title": "Further reading",
    "section": "References",
    "text": "References\n\n\nAmaral, André Victor Ribeiro, Daniel Wolffram, Paula Moraga, and Johannes Bracher. 2025. “Post-Processing and Weighted Combination of Infectious Disease Nowcasts.” PLoS Computational Biology 21 (3): e1012836. https://doi.org/10.1371/journal.pcbi.1012836.\n\n\nAsher, Jason. 2018. “Forecasting Ebola with a Regression Transmission Model.” Epidemics 22 (March): 50–55. https://doi.org/10.1016/j.epidem.2017.02.009.\n\n\nBrockhaus, Elisabeth K., Daniel Wolffram, Tanja Stadler, Michael Osthege, Tanmay Mitra, Jonas M. Littek, Ekaterina Krymova, et al. 2023. “Why Are Different Estimates of the Effective Reproductive Number so Different? A Case Study on COVID-19 in Germany.” PLOS Computational Biology 19 (11): e1011653. https://doi.org/10.1371/journal.pcbi.1011653.\n\n\n“CDCgov/Wastewater-Informed-Covid-Forecasting.” 2025. Centers for Disease Control and Prevention.\n\n\nCharniga, Kelly, Sang Woo Park, Andrei R. Akhmetzhanov, Anne Cori, Jonathan Dushoff, Sebastian Funk, Katelyn M. Gostic, et al. 2024. “Best Practices for Estimating and Reporting Epidemiological Delay Distributions of Infectious Diseases.” PLOS Computational Biology 20 (10): e1012520. https://doi.org/10.1371/journal.pcbi.1012520.\n\n\nFunk, Sebastian, Anton Camacho, Adam J. Kucharski, Rachel Lowe, Rosalind M. Eggo, and W. John Edmunds. 2019. “Assessing the Performance of Real-Time Epidemic Forecasts: A Case Study of Ebola in the Western Area Region of Sierra Leone, 2014-15.” PLOS Computational Biology 15 (2): e1006785. https://doi.org/10.1371/journal.pcbi.1006785.\n\n\nGostic, Katelyn M., Lauren McGough, Edward B. Baskerville, Sam Abbott, Keya Joshi, Christine Tedijanto, Rebecca Kahn, et al. 2020. “Practical Considerations for Measuring the Effective Reproductive Number, Rt.” PLOS Computational Biology 16 (12): e1008409. https://doi.org/10.1371/journal.pcbi.1008409.\n\n\nHeld, Leonhard, Sebastian Meyer, and Johannes Bracher. 2017. “Probabilistic Forecasting in Infectious Disease Epidemiology: The 13th Armitage Lecture.” Statistics in Medicine 36 (22): 3443–60. https://doi.org/10.1002/sim.7363.\n\n\nHyndman, Rob J, and George Athanasopoulos. 2021. Forecasting: Principles and Practice. 3rd ed. Melbourne, Australia: OTexts.\n\n\nLison, Adrian, Sam Abbott, Jana Huisman, and Tanja Stadler. 2024. “Generative Bayesian Modeling to Nowcast the Effective Reproduction Number from Line List Data with Missing Symptom Onset Dates.” PLOS Computational Biology 20 (4): e1012021. https://doi.org/10.1371/journal.pcbi.1012021.\n\n\nLopez, Velma K., Estee Y. Cramer, Robert Pagano, John M. Drake, Eamon B. O’Dea, Madeline Adee, Turgay Ayer, et al. 2024. “Challenges of COVID-19 Case Forecasting in the US, 2020–2021.” PLOS Computational Biology 20 (5): e1011200. https://doi.org/10.1371/journal.pcbi.1011200.\n\n\nPark, Sang Woo, Andrei R. Akhmetzhanov, Kelly Charniga, Anne Cori, Nicholas G. Davies, Jonathan Dushoff, Sebastian Funk, et al. 2024. “Estimating Epidemiological Delay Distributions for Infectious Diseases.” medRxiv. https://doi.org/10.1101/2024.01.12.24301247.\n\n\nSherratt, Katharine, Hugo Gruson, Rok Grah, Helen Johnson, Rene Niehus, Bastian Prasse, Frank Sandmann, et al. 2023. “Predictive Performance of Multi-Model Ensemble Forecasts of COVID-19 Across European Nations.” Edited by Amy Wesolowski, Neil M Ferguson, Jeffrey L Shaman, and Sen Pei. eLife 12 (April): e81916. https://doi.org/10.7554/eLife.81916.\n\n\nStoner, Oliver, Theo Economou, and Alba Halliday. 2020. “A Powerful Modelling Framework for Nowcasting and Forecasting COVID-19 and Other Diseases.” arXiv. https://doi.org/10.48550/arXiv.1912.05965.\n\n\nWolffram, Daniel, Sam Abbott, Matthias an der Heiden, Sebastian Funk, Felix Günther, Davide Hailer, Stefan Heyder, et al. 2023. “Collaborative Nowcasting of COVID-19 Hospitalization Incidences in Germany.” PLOS Computational Biology 19 (8): e1011394. https://doi.org/10.1371/journal.pcbi.1011394.",
    "crumbs": [
      "Further reading"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html",
    "href": "reference/learning_objectives.html",
    "title": "Learning outcomes",
    "section": "",
    "text": "The skills and methods taught in this course apply broadly across infectious disease epidemiology, from outbreak response to routine surveillance of endemic diseases.",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#delay-distributions",
    "href": "reference/learning_objectives.html#delay-distributions",
    "title": "Learning outcomes",
    "section": "Delay distributions",
    "text": "Delay distributions\n\nunderstanding of common probability distributions used for epidemiological delays\nfamiliarity with using Stan to estimate parameters of a probability distribution\nunderstanding of the ubiquity of delays in epidemiological data\nfamiliarity with interpreting posterior distributions and quantifying parameter uncertainty",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#biases-in-delay-distributions",
    "href": "reference/learning_objectives.html#biases-in-delay-distributions",
    "title": "Learning outcomes",
    "section": "Biases in delay distributions",
    "text": "Biases in delay distributions\n\nunderstanding of how interval censoring affects the estimation and interpretation of epidemiological delay distributions\nunderstanding of right truncation in epidemiological data and its impact on real-time analysis\nfamiliarity with statistical methods for adjusting delay estimates for censoring and truncation\nunderstanding of how biases compound during exponential growth phases of epidemics",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#using-delay-distributions-to-model-the-data-generating-process",
    "href": "reference/learning_objectives.html#using-delay-distributions-to-model-the-data-generating-process",
    "title": "Learning outcomes",
    "section": "Using delay distributions to model the data generating process",
    "text": "Using delay distributions to model the data generating process\n\nunderstanding of using delay distributions to model population-level data generating processes\nfamiliarity with convolutions to combine count data with individual probability distributions\nunderstanding of double interval censoring and discretisation for population-level data\nunderstanding of the need to introduce additional uncertainty to account for the observation process at a population level",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#r_t-estimation-and-the-renewal-equation",
    "href": "reference/learning_objectives.html#r_t-estimation-and-the-renewal-equation",
    "title": "Learning outcomes",
    "section": "\\(R_t\\) estimation and the renewal equation",
    "text": "\\(R_t\\) estimation and the renewal equation\n\nunderstanding of the reproduction number and challenges in its estimation\nunderstanding of the renewal equation as an epidemiological model for infection generation\nfamiliarity with the generation time as a particular type of delay distribution\nunderstanding of the role of the generative model in the estimation of \\(R_t\\)\nfamiliarity with geometric random walk models for smoothing \\(R_t\\) estimates",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#nowcasting",
    "href": "reference/learning_objectives.html#nowcasting",
    "title": "Learning outcomes",
    "section": "Nowcasting",
    "text": "Nowcasting\n\nunderstanding of nowcasting as a particular right truncation problem\nunderstanding of the difference between report date and event dates\nfamiliarity with simple nowcasting using known delay distributions\nfamiliarity with improving the model of the data generating process with geometric random walk models to improve nowcast performance in some circumstances",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#joint-nowcasting-with-unknown-delays",
    "href": "reference/learning_objectives.html#joint-nowcasting-with-unknown-delays",
    "title": "Learning outcomes",
    "section": "Joint nowcasting with unknown delays",
    "text": "Joint nowcasting with unknown delays\n\nunderstanding of the reporting triangle structure for epidemiological surveillance data\nunderstanding of the benefits of joint estimation of delay distributions and nowcasts\nunderstanding of population-level modelling with observation error\nunderstanding of the link between Rt estimation and nowcasting",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#forecasting",
    "href": "reference/learning_objectives.html#forecasting",
    "title": "Learning outcomes",
    "section": "Forecasting",
    "text": "Forecasting\n\nunderstanding of forecasting as an epidemiological problem\nfamiliarity with ARIMA models for forecasting epidemiological time series\nunderstanding of autocorrelation and partial autocorrelation functions for time series analysis\nunderstanding of stationarity and data transformations for forecasting",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#combining-nowcasting-and-forecasting",
    "href": "reference/learning_objectives.html#combining-nowcasting-and-forecasting",
    "title": "Learning outcomes",
    "section": "Combining nowcasting and forecasting",
    "text": "Combining nowcasting and forecasting\n\nunderstanding of the challenges of forecasting with incomplete data due to reporting delays\nunderstanding of the link between nowcasting and forecasting\nunderstanding of pipeline approaches for combining nowcasting and forecasting\nunderstanding of joint approaches for simultaneous nowcasting and forecasting\nunderstanding of the trade-offs between timeliness and completeness in real-time analysis",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#evaluating-forecasts-and-nowcasts",
    "href": "reference/learning_objectives.html#evaluating-forecasts-and-nowcasts",
    "title": "Learning outcomes",
    "section": "Evaluating forecasts (and nowcasts)",
    "text": "Evaluating forecasts (and nowcasts)\n\nunderstanding of the four principles of good probabilistic forecasts: calibration, unbiasedness, accuracy, and sharpness\nfamiliarity with scoring metrics: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and Continuous Ranked Probability Score (CRPS)\nunderstanding of time-series cross-validation for forecast evaluation\nfamiliarity with visual assessment of forecasts and nowcasts\nunderstanding the challenges with, and possible solutions for, evaluating datasets with incomplete or missing forecast data",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#ensemble-models",
    "href": "reference/learning_objectives.html#ensemble-models",
    "title": "Learning outcomes",
    "section": "Ensemble models",
    "text": "Ensemble models\n\nunderstanding of predictive ensembles and their properties\nfamiliarity with different forecast representation formats (samples, quantiles, bins)\nunderstanding of linear opinion pools and Vincent averaging for ensemble methods\nfamiliarity with hubverse data standards for collaborative forecasting",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/learning_objectives.html#collaborative-modeling",
    "href": "reference/learning_objectives.html#collaborative-modeling",
    "title": "Learning outcomes",
    "section": "Collaborative modeling",
    "text": "Collaborative modeling\n\nunderstanding of modeling hubs and hubverse-style tools\ndeveloping and evaluating forecasting models using real epidemiological data\nimplementing time-series cross-validation for model assessment\ngenerating and formatting forecasts for submission to a hub\nsubmitting forecasts to a local and/or online hub, generating and interpreting evaluation metrics",
    "crumbs": [
      "Learning outcomes"
    ]
  },
  {
    "objectID": "reference/stan.html",
    "href": "reference/stan.html",
    "title": "Stan Reference",
    "section": "",
    "text": "Uncertainty is an unavoidable part of real-world data\nNeed to estimate unobserved quantities (true cases, the effective reproduction number, future trends)\nBayesian approach naturally handles missing data and incorporates prior knowledge\nStan is a powerful tool for Bayesian inference\n\n\n\nStan is a probabilistic programming language for Bayesian inference. It allows us to:\n\nWrite down models in a text file (ending .stan)\nGenerate samples from the posterior distribution using various methods (like Hamiltonian Monte Carlo)\nGet proper uncertainty quantification for our estimates\n\nWe use Stan because: - We’ll need to estimate things (delays, reproduction numbers, case numbers now and in the future) - We’ll want to correctly specify uncertainty - We’ll want to incorporate our domain expertise - We’ll do this using Bayesian inference",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "reference/stan.html#why-stan-in-epidemiology",
    "href": "reference/stan.html#why-stan-in-epidemiology",
    "title": "Stan Reference",
    "section": "",
    "text": "Uncertainty is an unavoidable part of real-world data\nNeed to estimate unobserved quantities (true cases, the effective reproduction number, future trends)\nBayesian approach naturally handles missing data and incorporates prior knowledge\nStan is a powerful tool for Bayesian inference\n\n\n\nStan is a probabilistic programming language for Bayesian inference. It allows us to:\n\nWrite down models in a text file (ending .stan)\nGenerate samples from the posterior distribution using various methods (like Hamiltonian Monte Carlo)\nGet proper uncertainty quantification for our estimates\n\nWe use Stan because: - We’ll need to estimate things (delays, reproduction numbers, case numbers now and in the future) - We’ll want to correctly specify uncertainty - We’ll want to incorporate our domain expertise - We’ll do this using Bayesian inference",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "reference/stan.html#just-enough-probability",
    "href": "reference/stan.html#just-enough-probability",
    "title": "Stan Reference",
    "section": "2. Just Enough Probability",
    "text": "2. Just Enough Probability\n\nWhat is a Probability Distribution?\nA probability distribution describes how likely different outcomes are for a random variable.\nDiscrete distributions (for countable outcomes): - Example: Number of horse kick deaths per year (0, 1, 2, 3, …) - Poisson distribution with \\(\\lambda\\) = 0.61 kicks per year - Probability of exactly 2 deaths: dpois(2, lambda = 0.61) = 0.11\nContinuous distributions (for measurable quantities): - Example: Temperature in Stockholm tomorrow - Normal distribution with mean 23°C, standard deviation 2°C - Probability density at 30°C: dnorm(30, mean = 23, sd = 2) = 0.0001\n\n\nTwo Key Operations\n\nCalculate probability/density: Given parameters, what’s the probability of observing a value?\nGenerate samples: Given parameters, simulate random observations\n\n\n\nDistributions You’ll See\n\n\n\n\n\n\n\n\n\nDistribution\nUsed for\nParameters\nExample\n\n\n\n\nLog-normal\nDelays (e.g., incubation period)\nmeanlog, sdlog\nSymptom onset time\n\n\nGamma\nDelays (alternative)\nshape, rate\nHospital length of stay\n\n\nNegative Binomial\nCount data with overdispersion\nmu, phi\nDaily case counts\n\n\nNormal\nContinuous measurements\nmean, sd\nLog(Rt)\n\n\nBeta\nProportions\nalpha, beta\nReporting probability\n\n\n\n\n\nKey Concept: Parameters vs Data\n\nData: What we observe (onset dates, test results)\nParameters: What we want to learn (mean delay, Rt)\nModel: How parameters generate data\n\n\n\nBayesian Inference in a Nutshell\nThe generative model can produce output which looks like data given a set of parameters \\(\\theta\\)\nIdea of Bayesian inference: treat \\(\\theta\\) as random variables (with a probability distribution) and condition on data: posterior probability p(\\(\\theta\\) | data) as target of inference.\nUsing Bayes’ rule: \\[p(\\theta | \\text{data}) = \\frac{p(\\text{data} | \\theta) p(\\theta)}{p(\\text{data})}\\]\n\np(data | θ) is the likelihood\np(θ) is the prior\np(data) is a normalisation constant\n\nIn words: (posterior) ∝ (normalised likelihood) × (prior)\n\n\nMCMC: Getting Samples from the Posterior\nMarkov-chain Monte Carlo (MCMC) is a method to generate samples of \\(\\theta\\) that come from the posterior distribution given data. This is our target of inference.\nMany flavours of MCMC exist:\n\nMetropolis-Hastings\nHamiltonian Monte Carlo (what Stan uses)\nGibbs sampling\n\nStan uses the No-U-TURN sampler a form of Hamiltonian Monte Carlo sampler to efficiently explore the posterior distribution and generate samples. This has been shown to be efficient across a wide range of models It’s main limitation is that it doesn’t support discrete latent parameters.",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "reference/stan.html#stan-basics-for-this-course",
    "href": "reference/stan.html#stan-basics-for-this-course",
    "title": "Stan Reference",
    "section": "3. Stan Basics for This Course",
    "text": "3. Stan Basics for This Course\n\nModel Structure\nEvery Stan model has three essential blocks:\n\ndata {\n  // What we observe\n  int&lt;lower=0&gt; N;  // number of observations\n  array[N] real y; // the observations\n}\n\nparameters {\n  // What we want to estimate\n  real&lt;lower=0&gt; mean_delay;\n  real&lt;lower=0&gt; sd_delay;\n}\n\nmodel {\n  // How parameters relate to data\n  // Prior\n  mean_delay ~ normal(5, 2);\n  sd_delay ~ normal(2, 1);\n  \n  // Likelihood\n  y ~ lognormal(log(mean_delay), sd_delay);\n}\n\n\n\nRunning Stan from R\n\nlibrary(cmdstanr)\nlibrary(nfidd)\n\n# Load a model\nmodel &lt;- nfidd_cmdstan_model(\"delays\")\n\n# Prepare data\nstan_data &lt;- list(\n  N = length(observations),\n  y = observations\n)\n\n# Fit model\nfit &lt;- nfidd_sample(model, data = stan_data)\n\n# Extract results\nsummarise_draws(fit)",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "reference/stan.html#debugging-tips",
    "href": "reference/stan.html#debugging-tips",
    "title": "Stan Reference",
    "section": "4. Debugging Tips",
    "text": "4. Debugging Tips\n\nCheck Your Priors\n\nDo your priors make sense?\nWhat range of predictions do they lead to?\nDoes this make sense for the data you’re modelling?\n\nBefore fitting your model, simulate from your priors to check they make sense:\n\n# Example: checking delay priors\nprior_mean_delay &lt;- abs(rnorm(1000, mean = 1.5, sd = 0.5))\nprior_sd_delay &lt;- abs(rnorm(1000, mean = 0.5, sd = 0.1))\n\n# Simulate delays from these priors\nprior_delays &lt;- rlnorm(\n  1000, \n  meanlog = log(prior_mean_delay), \n  sdlog = prior_sd_delay\n)\n\n# Plot to check if reasonable\nhist(prior_delays, breaks = 50)\n\n\n\n\n\n\n\nquantile(prior_delays, c(0.025, 0.5, 0.975))\n\n     2.5%       50%     97.5% \n0.3635362 1.5352948 4.8070182 \n\n\n\n\nCheck your model\n\nDoes your model make sense?\nDoes it have the parameters you expect?\nDoes it have the data you expect?\n\n\n\nCheck your data\n\nDoes your data make sense?\nDoes it have the variables you expect?\nDoes it have the right number of observations?\n\n\n\nCheck your results\n\nDoes your model fit the data?\nDo the parameter estimates make sense?\n\nUse posterior predictive checks to see if your model can reproduce data similar to what you observed: - Does it capture the central tendency of the data? - Does it get the right amount of variability? - Can it reproduce extreme values appropriately?\nCompare your posterior predictions to the observed data using density overlays, empirical CDFs, and summary statistics.\n\n\nCommon Issues & Solutions\n\n\n\n\n\n\n\n\n\nIssue\nSymptom\nWhat\nSolution\n\n\n\n\nDivergent transitions\nWarning message\nSampler can’t explore posterior due to geometry of the posterior\nIncrease adapt_delta\n\n\nTreedepth\nWarning message\nSampler hit maximum tree depth before completing trajectory (complex posterior geometry requires longer integration paths)\nIncrease max_treedepth (e.g., to 12 or 15)\n\n\nLow ESS\nESS &lt; 400\nPoor mixing between chains\nRun more iterations\n\n\nHigh Rhat\nRhat &gt; 1.01\nChains sampling different distributions (not converged)\nCheck model specification or run more warmup iterations\n\n\nInitialization failed\nError on startup\nNumerical instability at start\nSet init values\n\n\n\n\n\nUnderstanding Divergences\nDivergent transitions occur when the sampler encounters regions of the posterior that are difficult to explore. This often happens when:\n\nThe posterior has areas of high curvature\nParameters are on very different scales\nThere are strong correlations between parameters\n\nSolutions:\n\n# Increase adapt_delta (default is 0.8)\nfit &lt;- nfidd_sample(\n  model, \n  data = stan_data,\n  adapt_delta = 0.95  # or even 0.99 for difficult models\n)\n\nor reparameterise your model.",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "reference/stan.html#going-deeper",
    "href": "reference/stan.html#going-deeper",
    "title": "Stan Reference",
    "section": "5. Going Deeper",
    "text": "5. Going Deeper\n\nCourse Models\n\nSee /inst/stan/ for all model code\nEach model has comments explaining the approach\n\n\n\nExternal Resources\n\nStan User’s Guide\nPrior Choice Wiki\nBayesian Workflow\nStan Forums - helpful community",
    "crumbs": [
      "Stan Reference"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html",
    "href": "sessions/R-estimation-and-the-renewal-equation.html",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "In the last session we used the idea of convolutions as a way to interpret individual time delays at a population level. In that session, we linked symptom onsets back to infections. Now we want to link infections themselves together over time, knowing that current infections were infected by past infections. Correctly capturing this transmission process is crucial to modelling infections in the present and future.\n\n\n\nIntroduction to the reproduction number\n\n\n\n\nThe aim of this session is to introduce the renewal equation as an infection generating process, and to show how it can be used to estimate a time-varying reproduction number.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/R-estimation-and-the-renewal-equation.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference, and the purrr package for functional programming.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"purrr\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#slides",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#slides",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "Introduction to the reproduction number",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#objectives",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#objectives",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "The aim of this session is to introduce the renewal equation as an infection generating process, and to show how it can be used to estimate a time-varying reproduction number.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/R-estimation-and-the-renewal-equation.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference, and the purrr package for functional programming.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"purrr\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#source-file",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#source-file",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "The source file of this session is located at sessions/R-estimation-and-the-renewal-equation.qmd.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#libraries-used",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#libraries-used",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference, and the purrr package for functional programming.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"purrr\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#initialisation",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#initialisation",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#simulating-a-geometric-random-walk",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#simulating-a-geometric-random-walk",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "Simulating a geometric random walk",
    "text": "Simulating a geometric random walk\nYou can have a look at an R function for performing the geometric random walk:\n\ngeometric_random_walk\n\nfunction (init, noise, std) \n{\n    n &lt;- length(noise) + 1\n    x &lt;- numeric(n)\n    x[1] &lt;- log(init)\n    for (i in 2:n) {\n        x[i] &lt;- x[i - 1] + noise[i - 1] * std\n    }\n    exp(x)\n}\n&lt;bytecode: 0x55dade9add90&gt;\n&lt;environment: namespace:nfidd&gt;\n\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nLook at this function and try to understand what it does. Note that we use the fact that we can generate a random normally distributed variable \\(X\\) with mean 0 and standard deviation \\(\\sigma\\) by mutiplying a standard normally distributed variable (i.e., mean 0 and standard deviation 1) \\(Y\\) with \\(\\sigma\\). Using this non-centred parameterisation for efficiency) will improve our computational efficency later when using an equivalent function in stan\n\n\nWe can use this function to simulate a random walk (shown in black below) and compare it to a normal(1,1) prior (shown in red below) that we used in the previous model:\n\nR &lt;- geometric_random_walk(init = 1, noise = rnorm(100), std = 0.1)\ndata &lt;- tibble(t = seq_along(R), R = exp(R))\n\n# Generate normal(1,1) prior samples for comparison\nnormal_prior &lt;- rnorm(100, mean = 1, sd = 1)\nnormal_data &lt;- tibble(t = seq_along(normal_prior), R = normal_prior)\n\nggplot(data, aes(x = t, y = R)) +\n  geom_line() +\n  geom_line(data = normal_data, aes(x = t, y = R), colour = \"red\") +\n  labs(title = \"Simulated data from a random walk model\",\n       subtitle = \"Random walk (black) vs Normal(1,1) prior (red)\",\n       x = \"Time\",\n       y = \"R\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nRepeat this multiple times, either with the same parameters or changing some to get a feeling for what this does compared to the normal(1,1) prior.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#estimating-r_t-with-a-geometric-random-walk-prior",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#estimating-r_t-with-a-geometric-random-walk-prior",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "Estimating \\(R_t\\) with a geometric random walk prior",
    "text": "Estimating \\(R_t\\) with a geometric random walk prior\nWe can now include this in a stan model,\n\nrw_mod &lt;- nfidd_cmdstan_model(\"estimate-inf-and-r-rw\")\nrw_mod\n\n 1: functions {\n 2:   #include \"functions/convolve_with_delay.stan\"\n 3:   #include \"functions/renewal.stan\"\n 4:   #include \"functions/geometric_random_walk.stan\"\n 5: }\n 6: \n 7: data {\n 8:   int n;                // number of days\n 9:   int I0;              // number initially infected\n10:   array[n] int obs;     // observed symptom onsets\n11:   int gen_time_max;     // maximum generation time\n12:   array[gen_time_max] real gen_time_pmf;  // pmf of generation time distribution\n13:   int&lt;lower = 1&gt; ip_max; // max incubation period\n14:   array[ip_max + 1] real ip_pmf;\n15: }\n16: \n17: parameters {\n18:   real&lt;lower = 0&gt; init_R;         // initial reproduction number\n19:   array[n-1] real rw_noise; // random walk noise\n20:   real&lt;lower = 0&gt; rw_sd; // random walk standard deviation\n21: }\n22: \n23: transformed parameters {\n24:   array[n] real R = geometric_random_walk(init_R, rw_noise, rw_sd);\n25:   array[n] real infections = renewal(I0, R, gen_time_pmf);\n26:   array[n] real onsets = convolve_with_delay(infections, ip_pmf);\n27: }\n28: \n29: model {\n30:   // priors\n31:   init_R ~ normal(1, 0.5) T[0, ];\n32:   rw_noise ~ std_normal();\n33:   rw_sd ~ normal(0, 0.05) T[0, ];\n34:   obs ~ poisson(onsets);\n35: }\n\n\nNote that the model is very similar to the one we used earlier, but with the addition of the random walk model for the reproduction number using a function in stan that does the same as our R function of the same name we defined.\nWe can now generate estimates from this model:\n\ndata &lt;- list(\n  n = length(obs) - 1,\n  obs = obs[-1],\n  I0 = inf_ts$infections[1],\n  gen_time_max = length(gen_time_pmf),\n  gen_time_pmf = gen_time_pmf,\n  ip_max = length(ip_pmf) - 1,\n  ip_pmf = ip_pmf\n)\nr_rw_inf_fit &lt;- nfidd_sample(\n  rw_mod, data = data, max_treedepth = 12, \n  init = \\() list(init_R = 1, rw_sd = 0.01)\n)\n\n\nr_rw_inf_fit\n\n    variable     mean   median   sd  mad       q5      q95 rhat ess_bulk\n lp__        21467.47 21467.50 8.65 8.60 21452.70 21481.10 1.01      801\n init_R          1.62     1.62 0.20 0.19     1.31     1.97 1.00     1372\n rw_noise[1]     0.14     0.14 0.99 1.03    -1.46     1.73 1.00     2310\n rw_noise[2]     0.11     0.08 0.96 0.95    -1.46     1.68 1.00     2283\n rw_noise[3]     0.07     0.11 0.96 0.95    -1.56     1.66 1.00     2107\n rw_noise[4]     0.01     0.01 1.00 1.01    -1.58     1.67 1.00     2649\n rw_noise[5]    -0.02    -0.04 0.99 1.00    -1.64     1.58 1.00     2175\n rw_noise[6]    -0.11    -0.11 1.01 1.00    -1.74     1.53 1.01     2567\n rw_noise[7]    -0.18    -0.16 1.01 1.08    -1.82     1.48 1.00     2337\n rw_noise[8]    -0.16    -0.17 1.00 1.01    -1.79     1.53 1.00     2304\n ess_tail\n     1155\n     1370\n     1508\n     1404\n     1760\n     1678\n     1618\n     1184\n     1583\n     1214\n\n # showing 10 of 566 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\n\nAs this is a more complex model we have increased the max_treedepth parameter to 12 to allow for more complex posterior distributions and we have also provided an initialisation for the init_R and rw_sd parameters to help the sampler find the right region of parameter space. This is a common technique when fitting more complex models and is needed as it is hard a priori to know where the sampler should start.\n\nWe can again extract and visualise the posteriors in the same way as earlier.\n\nrw_posteriors &lt;- r_rw_inf_fit |&gt;\n  gather_draws(infections[infection_day], R[infection_day]) |&gt;\n  ungroup() |&gt;\n  mutate(infection_day = infection_day - 1) |&gt;\n  filter(.draw %in% sample(.draw, 100))\n\n\nrw_inf_posterior &lt;- rw_posteriors |&gt;\n  filter(.variable == \"infections\")\nggplot(mapping = aes(x = infection_day)) +\n  geom_line(\n    data = rw_inf_posterior, mapping = aes(y = .value, group = .draw), alpha = 0.1\n  ) +\n  geom_line(data = inf_ts, mapping = aes(y = infections), colour = \"red\") +\n  labs(title = \"Infections, estimated (grey) and observed (red)\", \n       subtitle = \"Model 3: renewal equation with random walk\")\n\n\n\n\n\n\n\n\nand reproduction numbers\n\nrw_r_inf_posterior &lt;- rw_posteriors |&gt;\n  filter(.variable == \"R\") |&gt;\n  filter(.draw %in% sample(.draw, 100))\nggplot(\n  data = rw_r_inf_posterior,\n  mapping = aes(x = infection_day, y = .value, group = .draw)\n) +\n  geom_line(alpha = 0.1) +\n  labs(title = \"Estimated R\", \n       subtitle = \"Model 3: renewal equation with random walk\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nWhat do you think of these estimates? In particular, what do you think of the estimates at the beginning and end of the outbreak? Are they consistent with the true Rt trajectory and with each other? Are they consistent with the estimates from the previous model?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe estimates are smoothest so far, and the model is able to capture the true Rt trajectory more accurately than the previous model. Unlike the previous model, the model is able to capture the true Rt trajectory at the end of the outbreak with variance increasing towards the date of estimation. The infection estimates are the least uncertain from any model and potentially overly certain as they don’t fully cover the observed infections.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#comparing-the-models",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#comparing-the-models",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "Comparing the models",
    "text": "Comparing the models\nWe can now plot all the Rt trajectories from the models together to compare them.\n\n## earlier posteriors\nr_posterior &lt;- r_posterior |&gt;\n  mutate(data = \"infections\")\nr_inf_posterior &lt;- r_inf_posterior |&gt;\n  mutate(data = \"onsets (normal)\")\nrw_r_inf_posterior &lt;- rw_r_inf_posterior |&gt;\n  mutate(data = \"onsets (random walk)\")\n\nall_posteriors &lt;- rbind(\n  r_inf_posterior,\n  rw_r_inf_posterior,\n  r_posterior\n)\n\nggplot(\n  all_posteriors,\n  mapping = aes(x = infection_day, y = .value, group = .draw,\n                colour = data)\n) +\n  geom_line(alpha = 0.1) +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(\n    title = \"Rt estimates from renewal equation models\",\n    subtitle = paste(\n      \"Estimates from infections, from symptom onsets, and from onsets with a\",\n      \"random walk\"\n    )\n  ) +\n  guides(colour = guide_legend(override.aes = list(alpha = 1))) + \n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nRevisit your answers to the previous questions in this session. What are the key differences between the Rt estimates from the models? Which model do you think is the best fit for the data? Which model is the most realistic?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can see that the estimates are smoother when using the random walk model for the reproduction number, compared to the normal model. The model that fits directly to infections has the lowest uncertainty, which we would expect as it doesn’t have to infer the number of infections from symptom onsets but even here the reproduction number estimates are unrealistically noisy due to the assumption of independence between infections each day when infection counts are low. The random walk model is the most realistic model, as it is able to capture the true Rt trajectory more accurately than the normal model. The model that fits directly to infections is the best fit for the data, but depends on the availability of infections data which in practice is never available.\n\n\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nCompare the results across the models used in this session, and the one used in the session on convolutions. How do the models vary in the number of parameters that need to be estimated? How do the assumptions about the infections time series differ between the models? What do you notice about the level of uncertainty in the estimates of infections and \\(R_t\\) over the course of the time series?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can see that using the renewal model as generative model we recover the time series of infections more accurately compared to previously when we assumed independent numbers of infections each day and that using a more believable model (i.e the geometric random walk) for the reproduction number improves things even more. Of course, this is helped by the fact that the data was generated by a model similar to the renewal model used for inference.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#challenge",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#challenge",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "Challenge",
    "text": "Challenge\n\nWe have used symptom onsets under the assumption that every infected person develops symptoms. Earlier we also created a time series of hospitalisation under the assumption that only a proportion (e.g., 30%) of symptomatic individuals get hospitalised. How would you change the model in this case? What are the implications for inference?\nIf you have time you could try re-running the experiment with different \\(R_t\\) trajectories (using the renewal() function to simulate data) and delay distributions to see whether results change.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#methods-in-practice",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#methods-in-practice",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nEpiEstim provides a range of accessible tools for estimating \\(R_t\\), using a simpler approach than the model we have used here.\nEpiNow2 (Abbott et al. 2025) package again implements a range of models similar to the model we have used here.\nEpidemia Bhatt et al. (2020) implements a regression model approach to \\(R_t\\) estimation.\nIn this course we focused on the instantaneous reproduction number. An alternative is the case reproduction number implemented in Wallinga and Teunis (2004).\nWe face many choices when estimating the reproduction number. Brockhaus et al. (2023) explores the impact of these choices on resulting estimates.\nGostic et al. (2020) has further guidance on best practice.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/R-estimation-and-the-renewal-equation.html#references",
    "href": "sessions/R-estimation-and-the-renewal-equation.html#references",
    "title": "\\(R_t\\) estimation and the renewal equation",
    "section": "References",
    "text": "References\n\n\nAbbott, Sam, Joel Hellewell, Katharine Sherratt, Katelyn Gostic, Joe Hickson, Hamada S. Badr, Michael DeWitt, James M. Azam, EpiForecasts, and Sebastian Funk. 2025. “EpiNow2: Estimate Real-Time Case Counts and Time-Varying Epidemiological Parameters.”\n\n\nBhatt, Samir, Neil Ferguson, Seth Flaxman, Axel Gandy, Swapnil Mishra, and James A. Scott. 2020. “Semi-Mechanistic Bayesian Modeling of COVID-19 with Renewal Processes.” arXiv. https://doi.org/10.48550/arXiv.2012.00394.\n\n\nBrockhaus, Elisabeth K., Daniel Wolffram, Tanja Stadler, Michael Osthege, Tanmay Mitra, Jonas M. Littek, Ekaterina Krymova, et al. 2023. “Why Are Different Estimates of the Effective Reproductive Number so Different? A Case Study on COVID-19 in Germany.” PLOS Computational Biology 19 (11): e1011653. https://doi.org/10.1371/journal.pcbi.1011653.\n\n\nGostic, Katelyn M., Lauren McGough, Edward B. Baskerville, Sam Abbott, Keya Joshi, Christine Tedijanto, Rebecca Kahn, et al. 2020. “Practical Considerations for Measuring the Effective Reproductive Number, Rt.” PLOS Computational Biology 16 (12): e1008409. https://doi.org/10.1371/journal.pcbi.1008409.\n\n\nWallinga, Jacco, and Peter Teunis. 2004. “Different Epidemic Curves for Severe Acute Respiratory Syndrome Reveal Similar Impacts of Control Measures.” American Journal of Epidemiology 160 (6): 509–16. https://doi.org/10.1093/aje/kwh255.",
    "crumbs": [
      "$R_t$ estimation and the renewal equation"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html",
    "href": "sessions/delay-distributions.html",
    "title": "Delay distributions",
    "section": "",
    "text": "Every chain of infectious disease transmission starts with one person infecting another. But we rarely observe these infection events directly. Instead, we typically collect epidemiological data from events that occur after the time of infection: symptom onsets, hospitalisations and discharge, etc. We often then need to understand how much time has passed between these events. For example, the incubation period might be used to determine the appropriate length of quarantine. We also need to have some estimate for these delays when interpreting new data, in order to understand what is happening now 1 and predict what might happen in the future. However, the length of time between any of these events might be highly variable (between individuals) and fundamentally uncertain (across the population as a whole). We capture this variability using probability distributions, which is the focus of this session.\n\n\n\nIntroduction to epidemiological delays\n\n\n\n\nThe aim of this session is for you to familiarise yourself with the concept of delay distributions used to describe reporting in infectious disease epidemiology. First we’ll look at this working forwards from infections in an outbreak. We’ll use R to probabilistically simulate delays from infection to reporting symptoms and hospitalisations. Then we’ll work only from this set of outcome data and use a stan model to estimate the parameters of one specific delay (in this example, symptom onset to hospitalisation).\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/delay-distributions.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the posterior and tidybayes packages for investigating the results of the inference conducted with stan.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"lubridate\")\nlibrary(\"posterior\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(1234)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#slides",
    "href": "sessions/delay-distributions.html#slides",
    "title": "Delay distributions",
    "section": "",
    "text": "Introduction to epidemiological delays",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#objectives",
    "href": "sessions/delay-distributions.html#objectives",
    "title": "Delay distributions",
    "section": "",
    "text": "The aim of this session is for you to familiarise yourself with the concept of delay distributions used to describe reporting in infectious disease epidemiology. First we’ll look at this working forwards from infections in an outbreak. We’ll use R to probabilistically simulate delays from infection to reporting symptoms and hospitalisations. Then we’ll work only from this set of outcome data and use a stan model to estimate the parameters of one specific delay (in this example, symptom onset to hospitalisation).\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/delay-distributions.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the posterior and tidybayes packages for investigating the results of the inference conducted with stan.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"lubridate\")\nlibrary(\"posterior\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(1234)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#source-file",
    "href": "sessions/delay-distributions.html#source-file",
    "title": "Delay distributions",
    "section": "",
    "text": "The source file of this session is located at sessions/delay-distributions.qmd.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#libraries-used",
    "href": "sessions/delay-distributions.html#libraries-used",
    "title": "Delay distributions",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the ggplot2 package for plotting, the dplyr and tidyr packages to wrangle data, the lubridate package to deal with dates, the posterior and tidybayes packages for investigating the results of the inference conducted with stan.\n\nlibrary(\"nfidd\")\nlibrary(\"ggplot2\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"lubridate\")\nlibrary(\"posterior\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#initialisation",
    "href": "sessions/delay-distributions.html#initialisation",
    "title": "Delay distributions",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(1234)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#load-the-outbreak-data",
    "href": "sessions/delay-distributions.html#load-the-outbreak-data",
    "title": "Delay distributions",
    "section": "Load the outbreak data",
    "text": "Load the outbreak data\nWe will work with a data set that is included in the nfidd R package that you installed initially. The column infection_time is a linelist of infections from our example outbreak, given as a decimal number of days that have passed since the initial infection of the outbreak. It can be loaded with the data command.\n\ndata(infection_times)\nhead(infection_times)\n\n  infection_time\n1       0.000000\n2       2.236708\n3       4.091861\n4       7.347199\n5       8.990060\n6       4.635069\n\n### visualise the infection curve\nggplot(infection_times, aes(x = infection_time)) +\n  geom_histogram(binwidth = 1) +\n  scale_x_continuous(n.breaks = 10) +\n  labs(x = \"Infection time (in days)\", y = \"Number of infections\",\n       title = \"Infections during an outbreak\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn reality, data from infectious disease surveillance will usually be given as dates, not decimals; and those will usually not represent infection, but an observed outcome such as symptom onset or hospital admission. For now we don’t want to spend too much time manipulating dates in R, but we will get back to working with more realistic surveillance data later.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#working-forwards-simulating-a-sequence-of-delays-after-infection",
    "href": "sessions/delay-distributions.html#working-forwards-simulating-a-sequence-of-delays-after-infection",
    "title": "Delay distributions",
    "section": "Working forwards: simulating a sequence of delays after infection",
    "text": "Working forwards: simulating a sequence of delays after infection\nWe would now like to simulate hospitalisations arising from this outbreak. We will start with our data on infection times, and work forwards to symptom onsets and then hospitalisations. We’ll make the following assumptions about the process from infection to hospital admission:\n\nInfection to symptoms:\n\nWe’ll assume all infections cause symptoms.\nTime from infection to symptom onset (incubation period): We assume that the incubation period is gamma-distributed with shape 5 and rate 1, i.e. a mean of 5 days.\n\nSymptoms to hospital admission:\n\nWe’ll assume that 30% of symptomatic cases become hospitalised.\nTime from symptom onset to hospital admission: We assume that this delay is lognormally distributed, with meanlog 1.75 and sdlog 0.5, corresponding to a mean delay of about a week.\n\n\nLet’s put these assumptions into practice by simulating some data, adding onset and hospitalisation times (in decimal number of days after the first infection) to the infection times. We’ll use random values for each infection from the probability distributions we’ve assumed above.\n\n\n\n\n\n\nTake a few minutes\n\n\n\nThroughout the course, we repeatedly use some small chunks of code. Instead of copying these between sessions, we’ve put them into functions in the nfidd R package that we can use when needed. To see exactly what the function does, just type the function name, e.g. add_delays for the function below.\n\n\n\ndf &lt;- add_delays(infection_times)\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nHave a look at the add_delays() function and try to understand its inner workings. You can also access a help file for this function, just like any other R function, using ?add_delays.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe add_delays() function takes a dataframe with infection_time and creates a complete simulated dataset:\n\nAdds onset_time by sampling gamma-distributed incubation delays (rgamma(n(), shape = 5, rate = 1))\nAdds hosp_time by sampling lognormal-distributed delays from onset (rlnorm(n(), meanlog = 1.75, sdlog = 0.5))\nSets hosp_time to NA for 70% of cases (only 30% are hospitalised) using rbinom()\nReturns a dataframe with all three time columns\n\n\n\n\nNow we can plot infections, hospitalisations and onsets.\n\n# convert our data frame to long format\ndfl &lt;- df |&gt;\n  pivot_longer(\n    cols = c(infection_time, onset_time, hosp_time),\n    names_to = \"type\", values_to = \"time\"\n  ) |&gt; \n  mutate(type = ordered(type, \n    levels = c(\"infection_time\", \"onset_time\", \"hosp_time\"), \n    labels = c(\"Infections\", \"Symptom onsets\", \"Hospitalisations\")\n  ))\n# plot\nggplot(dfl, aes(x = time)) +\n  geom_histogram(position = \"dodge\", binwidth = 1) +\n  facet_wrap(~ type, ncol = 1) +\n  xlab(\"Time (in days)\") +\n  ylab(\"Count\")\n\nWarning: Removed 4524 rows containing non-finite outside the scale range\n(`stat_bin()`).",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#estimating-delay-distributions-from-outcome-data",
    "href": "sessions/delay-distributions.html#estimating-delay-distributions-from-outcome-data",
    "title": "Delay distributions",
    "section": "Estimating delay distributions from outcome data",
    "text": "Estimating delay distributions from outcome data\nAs mentioned above, our data set of infection, symptom onset and hospitalisation times is not the typical data set we encounter in infectious disease surveillance. In reality, we don’t have infection dates, and we also have to deal with missing data, incomplete observations, data entry errors etc. For now, let us just assume we have a data set only including symptom onset times and some hospitalisation times.\nLet’s look at a specific problem: we would like to estimate how long it takes for people to become hospitalised after becoming symptomatic. This might be an important delay to know about, for example when modelling and forecasting hospitalisations, or more generally for estimating required hospital capacity.\nTo do this we can use our data with stan to model the delay from symptom onset to hospitalisation, with the same assumption that it follows a lognormal distribution.\n\n\n\n\n\n\nTake a few minutes\n\n\n\nThis is where we apply the Bayesian inference concepts from the slides. We’re using Stan to estimate the parameters of a lognormal distribution (meanlog and sdlog) from our observed delay data. The nfidd_cmdstan_model() function loads a pre-written Stan model for this purpose.\n\n\n\nmod &lt;- nfidd_cmdstan_model(\"lognormal\")\nmod\n\n 1: // lognormal_model.stan\n 2: data {\n 3:   int&lt;lower=0&gt; n; // number of data points\n 4:   array[n] real y; // data\n 5: }\n 6: \n 7: parameters {\n 8:   real meanlog;\n 9:   real&lt;lower=0&gt; sdlog;\n10: }\n11: \n12: model {\n13:   meanlog ~ normal(0, 10);  // prior distribution\n14:   sdlog ~ normal(0, 10) T[0, ]; // prior distribution\n15: \n16:   y ~ lognormal(meanlog, sdlog);\n17: }\n\n\n\n\n\n\n\n\nTake a few minutes\n\n\n\nLook at the Stan model code above and try to understand its structure. What are the data, parameters, and model blocks doing? What priors are being used?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe lognormal Stan model has three main blocks:\n\nData block: Specifies we need n (number of observations) and y (the delay data)\nParameters block: We want to estimate meanlog and sdlog (the two parameters of the lognormal distribution)\nModel block:\n\nSets weakly informative normal priors for both parameters\nsdlog is constrained to be positive (standard deviations must be positive)\nThe likelihood states that our data y follows a lognormal distribution with these parameters\n\n\n\n\n\n\n\n\n\n\n\nStan reference materials\n\n\n\n\n\nFor more information on the Stan based tools used in this session:\n\nStan reference guide - covers probability distributions (log-normal, gamma), Bayesian inference basics, and Stan model structure\nUsing our Stan models guide - practical guide to working with NFIDD’s Stan models, including nfidd_cmdstan_model() and nfidd_sample() functions used below\n\n\n\n\nLet’s estimate the onset-to-hospitalisation delay using the simulated data set we created above. Do we recover the parameters used for simulation?\nWe can do this by sampling from the model’s posterior distribution by feeding it our simulated data set.\n\n## Specify the time from onset to hospitalisation\ndf_onset_to_hosp &lt;- df |&gt;\n  mutate(onset_to_hosp = hosp_time - onset_time) |&gt; \n  # exclude infections that didn't result in hospitalisation\n  drop_na(onset_to_hosp)\n## Use the data to sample from the model posterior\nres &lt;- nfidd_sample(\n  mod,\n  data = list(\n    n = nrow(df_onset_to_hosp),\n    y = df_onset_to_hosp$onset_to_hosp\n  )\n)\n\n\n\n\n\n\n\nWhat is sampling?\n\n\n\nStan has used Markov Chain Monte Carlo (MCMC) to generate samples from the posterior distribution of our parameters. The posterior distribution is the conditional probability distribution of the parameters given the observed data - p(parameters | data). This represents our updated beliefs about the parameter values after combining our prior knowledge with the evidence from the observed delays. Using Bayes’ theorem: posterior ∝ likelihood × prior. Each sample represents a plausible set of parameter values given our data and priors.\n\n\nTo find out more about the arguments that can be passed to cmdstanr::sample via the nfidd_sample() function we used here, you can use ?cmdstanr::sample. To see the estimates, we can use:\n\nres$summary() ## could also simply type `res`\n\n# A tibble: 3 × 10\n  variable      mean    median      sd     mad        q5      q95  rhat ess_bulk\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__     -1322.    -1322.    0.983   0.697   -1324.    -1.32e+3  1.00    1092.\n2 meanlog      1.73      1.73  0.0116  0.0115      1.71   1.75e+0  1.00    1835.\n3 sdlog        0.493     0.493 0.00781 0.00800     0.481  5.06e-1  1.00    1591.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\n\n\n\n\n\n\nTake a few minutes\n\n\n\nLook at the summary output above. What do the columns mean? What are the values of meanlog and sdlog? How do they compare to the true values we used in simulation (meanlog = 1.75, sdlog = 0.5)?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe summary shows key statistics for each parameter:\n\nMean: Average value across all posterior samples\nMedian: Middle value when samples are sorted\nSD: Standard deviation of the posterior samples (uncertainty)\nMAD: Median absolute deviation (robust measure of uncertainty)\nQ5, Q95: 5th and 95th percentiles (90% credible interval)\nRhat: Convergence diagnostic (should be close to 1.0)\nESS: Effective sample size (should be &gt; 400 for reliable estimates)\n\nThe meanlog and sdlog estimates should be close to the true simulation values (1.75 and 0.5), showing our model successfully recovered the parameters.\nFor more details on interpreting Stan output, see the Stan reference guide.\n\n\n\nThese estimates should look similar to what we used in the simulations.\nWe can also calculate the mean and standard deviation of the lognormal distribution from the estimated parameters:\n\nres |&gt;\n  summarise_lognormal()\n\n      mean             sd       \n Min.   :6.145   Min.   :3.038  \n 1st Qu.:6.316   1st Qu.:3.282  \n Median :6.369   Median :3.339  \n Mean   :6.372   Mean   :3.343  \n 3rd Qu.:6.424   3rd Qu.:3.400  \n Max.   :6.654   Max.   :3.621  \n\n\nThis shows us the mean and standard deviation of the onset-to-hospitalisation delay in days, which is often more interpretable than the lognormal parameters. We can plot the resulting probability density functions.\n\n\n\n\n\n\nNote\n\n\n\nEvery time we sample from the model posterior, we create 4000 iterations. That is a lot of data to plot, so instead, when we produce plots we’ll use a random sample of 100 iterations. You’ll see this throughout the course.\n\n\n\n## get shape and rate samples from the posterior\nres_df &lt;- res |&gt;\n  as_draws_df() |&gt;\n  filter(.draw %in% sample(.draw, 100)) # sample 100 draws\n\n## find the value (x) that includes 99% of the cumulative density\nmax_x &lt;- max(qlnorm(0.99, meanlog = res_df$meanlog, sdlog = res_df$sdlog))\n\n## calculate density on grid of x values\nx &lt;- seq(0, max_x, length.out = 100)\nres_df &lt;- res_df |&gt;\n  crossing(x = x) |&gt; ## add grid to data frame\n  mutate(density = dlnorm(x, meanlog, sdlog))\n\n## plot\nggplot(res_df, aes(x = x, y = density, group = .draw)) +\n  geom_line(alpha = 0.3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake a few minutes\n\n\n\nLook at the plot above. What do you see? Why are there multiple lines? What does this tell us about our uncertainty in the delay distribution?\n\n\n\n\n\n\n\n\nSolutions\n\n\n\n\n\nThe plot shows multiple semi-transparent lines, each representing a different plausible delay distribution:\n\nMultiple lines: Each line represents one possible lognormal distribution based on a different sample from the posterior\nUncertainty visualisation: The spread of lines shows our uncertainty about the true delay distribution - we’re not certain about the exact shape\nLimited data: This uncertainty comes from having limited data - with more data, the lines would cluster more tightly together\nParameter trade-offs: Different combinations of meanlog and sdlog can produce very similar-looking distributions, making it hard to pin down exact parameter values\nRight-skewed shape: All curves show the characteristic right-skewed shape of lognormal distributions, appropriate for delays\nMost likely region: Where lines cluster together shows where we’re most confident about the distribution shape\n\nThis visualisation captures both our estimate of the delay distribution and our uncertainty about it.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#challenge",
    "href": "sessions/delay-distributions.html#challenge",
    "title": "Delay distributions",
    "section": "Challenge",
    "text": "Challenge\n\nIn this session we were in the enviable situation of knowing which distribution was used to generate the data. With real data, of course, we don’t have this information available.\nImpact of sample size: Try subsampling your data to see how fewer observations affect your parameter estimates and uncertainty.\nSubsample to 100 observations and refit the model to compare results:\n\ndf_small &lt;- df_onset_to_hosp |&gt; slice_sample(n = 100)\n\nDifferent delay lengths: The updated add_delays() function now allows you to change the delay parameters.\nTry longer delays and see how this affects your parameter recovery:\n\ndf_long &lt;- add_delays(infection_times, \n                     hosp_params = list(meanlog = 2.5, sdlog = 0.5))\n\nDifferent variability of the delay: Try changing the variability of the delay to see how it affects uncertainty in estimates.\nMore variable delays (higher sdlog):\n\ndf_variable &lt;- add_delays(infection_times,\n                         hosp_params = list(meanlog = 1.75, sdlog = 1.0))\n\nLess variable delays (lower sdlog):\n\ndf_precise &lt;- add_delays(infection_times,\n                        hosp_params = list(meanlog = 1.75, sdlog = 0.2))\n\nDifferent type of distribution: Try using a completely different distribution for the delays.\nUse gamma distribution instead of lognormal:\n\ndf_gamma &lt;- add_delays(infection_times,\n                      hosp_fun = rgamma,\n                      hosp_params = list(shape = 2, rate = 0.3))\n\nDifferent fitted distribution: Try fitting a different distribution to the data. For example, use the gamma Stan model instead of lognormal:\n\nmod_gamma &lt;- nfidd_cmdstan_model(\"gamma\")\n\n\nSee the Using our Stan models guide for more available models\n\nTry using one of the methods in practice below to fit a distribution to the data.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#methods-in-practice",
    "href": "sessions/delay-distributions.html#methods-in-practice",
    "title": "Delay distributions",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nThere are many tools for fitting a distribution to a set of observed delays. For example, we could implement a regression approach in e.g. brms, or use a package such as fitdistrplus or the ‘naive’ model in epidist.",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#references",
    "href": "sessions/delay-distributions.html#references",
    "title": "Delay distributions",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/delay-distributions.html#footnotes",
    "href": "sessions/delay-distributions.html#footnotes",
    "title": "Delay distributions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMuch like Alice through the Looking Glass, it often seems like we are running to stay in the same place!↩︎",
    "crumbs": [
      "Delay distributions"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html",
    "href": "sessions/forecast-ensembles.html",
    "title": "Forecast ensembles",
    "section": "",
    "text": "As we saw in the sessions on improving forecasting models and forecast evaluation, different modelling approaches have different strength and weaknesses, and we do not usually know in advance which one will produce the best forecast in any given situation. One way to attempt to draw strength from a diversity of approaches is the creation of so-called forecast ensembles from the forecasts produced by different models.\nIn this session, we’ll start with forecasts from the models we explored in the previous sessions and build ensembles of these models. We will then compare the performance of these ensembles to the individual models and to each other.\n\n\n\n\n\n\nRepresentations of probabilistic forecasts\n\n\n\n\n\nProbabilistic predictions can be described as coming from a probabilistic probability distributions. In general, it is not safe to assume that these distributions can be expressed in a simple mathematical form as we can do if, e.g., talking about common probability distributions such as the normal or gamma distributions. (Although some of the fable models, like ARIMA can express forecasts as normal distributions.)\nIt can be helpful to have a model-agnostic way to represent probabilistic distributions.\n\nOne common way is to use a limited number (say, between 100 and 1000) of samples generated from Monte Carlo methods to represent the predictive distribution.\nAnother approach uses values that corresponds to a specific set of quantile levels of the predictive distribution distribution. For example, the median is the 50th quantile of a distribution, meaning that 50% of the values in the distribution are less than the median and 50% are greater. Similarly, the 90th quantile is the value that corresponds to 90% of the distribution being less than this value. If we characterise a predictive distribution by its quantiles, we specify these values at a range of specific quantile levels, e.g. from 5% to 95% in 5% steps.\nAnother approach might be to define bins (intervals) that an outcome might fall into and assign probabilities to those bins\n\nDeciding how to represent forecasts depends on many things, for example the method used (and whether it produces samples by default) but also logistical considerations. Many collaborative forecasting projects and so-called forecasting hubs use quantile-based representations of forecasts in the hope to be able to characterise both the centre and tails of the distributions more reliably and with less demand on storage space than a sample-based representation. Quantiles are also better than a bin-based representation when the range of possible outcomes is large.\n\n\n\n\n\n\nIntroduction to ensembles\n\n\n\n\nThe aim of this session is to introduce the concept of ensembles of forecasts and to evaluate the performance of ensembles of the models we explored in the earlier sessions.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecast-ensembles.qmd.\n\n\n\nIn this session we will use the fable package for fitting simple forecasting models, the dplyr package for data wrangling, the ggplot2 library for plotting and the epidatr package for accessing and downloading versions of epidemiological surveillance data from the Delphi EpiData API.\nAdditionally, we will use some hubverse packages such as hubEvals, hubUtils, hubEnsembles packages for building ensembles.\n\nlibrary('nfidd')\nlibrary(\"fable\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\nlibrary(\"hubUtils\")\nlibrary(\"hubEvals\")\nlibrary(\"hubEnsembles\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do.\n\nset.seed(42) ## for Douglas Adams!",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#slides",
    "href": "sessions/forecast-ensembles.html#slides",
    "title": "Forecast ensembles",
    "section": "",
    "text": "Introduction to ensembles",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#objectives",
    "href": "sessions/forecast-ensembles.html#objectives",
    "title": "Forecast ensembles",
    "section": "",
    "text": "The aim of this session is to introduce the concept of ensembles of forecasts and to evaluate the performance of ensembles of the models we explored in the earlier sessions.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecast-ensembles.qmd.\n\n\n\nIn this session we will use the fable package for fitting simple forecasting models, the dplyr package for data wrangling, the ggplot2 library for plotting and the epidatr package for accessing and downloading versions of epidemiological surveillance data from the Delphi EpiData API.\nAdditionally, we will use some hubverse packages such as hubEvals, hubUtils, hubEnsembles packages for building ensembles.\n\nlibrary('nfidd')\nlibrary(\"fable\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\nlibrary(\"hubUtils\")\nlibrary(\"hubEvals\")\nlibrary(\"hubEnsembles\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do.\n\nset.seed(42) ## for Douglas Adams!",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#source-file",
    "href": "sessions/forecast-ensembles.html#source-file",
    "title": "Forecast ensembles",
    "section": "",
    "text": "The source file of this session is located at sessions/forecast-ensembles.qmd.",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#libraries-used",
    "href": "sessions/forecast-ensembles.html#libraries-used",
    "title": "Forecast ensembles",
    "section": "",
    "text": "In this session we will use the fable package for fitting simple forecasting models, the dplyr package for data wrangling, the ggplot2 library for plotting and the epidatr package for accessing and downloading versions of epidemiological surveillance data from the Delphi EpiData API.\nAdditionally, we will use some hubverse packages such as hubEvals, hubUtils, hubEnsembles packages for building ensembles.\n\nlibrary('nfidd')\nlibrary(\"fable\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\nlibrary(\"hubUtils\")\nlibrary(\"hubEvals\")\nlibrary(\"hubEnsembles\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#initialisation",
    "href": "sessions/forecast-ensembles.html#initialisation",
    "title": "Forecast ensembles",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do.\n\nset.seed(42) ## for Douglas Adams!",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#reformatting-model-output",
    "href": "sessions/forecast-ensembles.html#reformatting-model-output",
    "title": "Forecast ensembles",
    "section": "Reformatting model output",
    "text": "Reformatting model output\nForecasts in fable follow their own standard format, and interacting with them requires an advanced understanding of tibble objects. Most of the fable models that we have used return distributional objects (a standard representation of, say, a Normal distribution) as the output, although some models return samples by default. We will migrate our existing forecasts from fable models into samples, as this is a flexible, interchangeable format.\n\n\n\n\n\n\nGory details on data reformatting\n\n\n\n\n\nWe will note that by turning a closed-form distribution into a set of samples, we lose some information, or add some noise, to the resulting output. If we were worried about this (and not so worried about eating up the RAM on our computer), we could extract many thousands of samples to reduce variability in the output. To learn more about the pros and cons of other model output type formats, check out the hubverse website.\n\n\n\nTo start with, we will use the generate() function to create 100 sampled trajectories (at 1 to 8 weeks ahead) from each of the fitted models above.\n\nsampled_forecasts &lt;- generate(\n  cv_models, \n  h=8, \n  times=100) |&gt; \n  group_by(.split, region, .model, .rep) |&gt; \n  mutate(h = row_number())\nsampled_forecasts\n\n# A tsibble: 38,400 x 7 [7D]\n# Key:       .split, region, .model, .rep [4,800]\n# Groups:    .split, region, .model, .rep [4,800]\n   .split region .model epiweek    .rep   .sim     h\n    &lt;int&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;date&gt;     &lt;chr&gt; &lt;dbl&gt; &lt;int&gt;\n 1      1 nat    rw     2017-09-03 1     1.12      1\n 2      1 nat    rw     2017-09-10 1     1.14      2\n 3      1 nat    rw     2017-09-17 1     1.26      3\n 4      1 nat    rw     2017-09-24 1     1.24      4\n 5      1 nat    rw     2017-10-01 1     1.02      5\n 6      1 nat    rw     2017-10-08 1     1.08      6\n 7      1 nat    rw     2017-10-15 1     0.750     7\n 8      1 nat    rw     2017-10-22 1     0.634     8\n 9      1 nat    rw     2017-09-03 10    1.01      1\n10      1 nat    rw     2017-09-10 10    0.987     2\n# ℹ 38,390 more rows\n\n\nThese forecasts above are still in fable-style formatting. The code below renames and reformats columns and makes this valid hubverse-style model output.\n\nsampled_forecasts_hub &lt;- sampled_forecasts |&gt; \n  rename(value = .sim,\n         model_id = .model,\n         target_epiweek = epiweek) |&gt; \n  mutate(output_type = \"sample\",\n         output_type_id = stringr::str_c(region, stringr::str_pad(.rep, width = 3, pad = \"0\")),\n         forecast_date = target_epiweek - h*7L) |&gt; \n  ungroup() |&gt; \n  as_tibble() |&gt; \n  select(model_id,\n         forecast_date, \n         target_epiweek,\n         h,\n         output_type,\n         output_type_id,\n         value)\nsampled_forecasts_hub\n\n# A tibble: 38,400 × 7\n   model_id forecast_date target_epiweek     h output_type output_type_id value\n   &lt;chr&gt;    &lt;date&gt;        &lt;date&gt;         &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n 1 rw       2017-08-27    2017-09-03         1 sample      nat001         1.12 \n 2 rw       2017-08-27    2017-09-10         2 sample      nat001         1.14 \n 3 rw       2017-08-27    2017-09-17         3 sample      nat001         1.26 \n 4 rw       2017-08-27    2017-09-24         4 sample      nat001         1.24 \n 5 rw       2017-08-27    2017-10-01         5 sample      nat001         1.02 \n 6 rw       2017-08-27    2017-10-08         6 sample      nat001         1.08 \n 7 rw       2017-08-27    2017-10-15         7 sample      nat001         0.750\n 8 rw       2017-08-27    2017-10-22         8 sample      nat001         0.634\n 9 rw       2017-08-27    2017-09-03         1 sample      nat010         1.01 \n10 rw       2017-08-27    2017-09-10         2 sample      nat010         0.987\n# ℹ 38,390 more rows\n\n\nOne of the foundational data structures for hubverse-style output is the three columns:\n\noutput_type: here we have sample trajectories\noutput_type_id: for sample output type, this column contains unique IDs that identify, for each forecast_date, which rows are from the same trajectory.\nvalue: this is the actual predicted value.\n\nWe can quickly visualize these sample trajectory outputs to see the structure of the forecasts for one week and one model:\n\nsampled_forecasts_hub |&gt; \n  filter(forecast_date == as.Date(\"2018-01-14\"),\n         model_id == \"fourier_ar2\") |&gt; \n  ggplot() +\n  geom_line(aes(x = target_epiweek, y = value, group = output_type_id), \n            color = \"red\",\n            alpha = 0.2) +\n  geom_line(data = flu_data |&gt; filter(epiweek &gt;= as.Date(\"2017-09-01\")), \n            aes(x=epiweek, y=wili))",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#quantile-based-ensembles",
    "href": "sessions/forecast-ensembles.html#quantile-based-ensembles",
    "title": "Forecast ensembles",
    "section": "Quantile-based ensembles",
    "text": "Quantile-based ensembles\nWe will first consider forecasts based on the individual quantiles of each model. This corresponds to a situation where each forecast aims to correctly determine a single target predictive distribution. By taking an average of all models, we aim to get a better estimate of this distribution than from the individual models. If we have reason to believe that some models are consistently better than others at estimating this distribution, then this might be reason to try to create a weighted version of this average.\n\nConverting sample-based forecasts to quantile-based forecasts\nIn this session we will be thinking about forecasts in terms quantiles of the predictive distributions, we will need to convert our sample-based forecasts to quantile-based forecasts. We will do this by focusing on the marginal distribution at each predicted time point, that is we treat each time point as independent of all others and calculate quantiles based on the sample predictive trajectories at that time point. An easy way to do this is to use the hubUtils package, which has utility functions for interacting with hubverse-style data. The steps to do this are to first do some reformatting so the forecasts can be seen as hubverse-style sample forecasts.\nNow we convert these sample forecasts to quantile forecasts.\n\nquantiles_to_save &lt;- c(0.025, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.975)\nquantile_forecasts &lt;- sampled_forecasts_hub |&gt;\n  hubUtils::convert_output_type(\n    to = list(quantile = quantiles_to_save)\n    )\nquantile_forecasts\n\n# A tibble: 3,456 × 7\n   model_id forecast_date target_epiweek     h output_type output_type_id value\n * &lt;chr&gt;    &lt;date&gt;        &lt;date&gt;         &lt;int&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;\n 1 ar2      2017-08-27    2017-09-03         1 quantile             0.025 0.806\n 2 ar2      2017-08-27    2017-09-03         1 quantile             0.05  0.858\n 3 ar2      2017-08-27    2017-09-03         1 quantile             0.1   0.909\n 4 ar2      2017-08-27    2017-09-03         1 quantile             0.25  0.966\n 5 ar2      2017-08-27    2017-09-03         1 quantile             0.5   1.06 \n 6 ar2      2017-08-27    2017-09-03         1 quantile             0.75  1.18 \n 7 ar2      2017-08-27    2017-09-03         1 quantile             0.9   1.30 \n 8 ar2      2017-08-27    2017-09-03         1 quantile             0.95  1.36 \n 9 ar2      2017-08-27    2017-09-03         1 quantile             0.975 1.41 \n10 ar2      2017-08-27    2017-09-10         2 quantile             0.025 0.657\n# ℹ 3,446 more rows\n\n\n\n\n\n\n\n\nWhat is happening here?\n\n\n\n\n\n\nInternally hubUtils is calculating the quantiles of the sample-based forecasts.\nIt does this by using a set of default quantiles but different ones can be specified by the user to override the default.\nIt then calls the quantile() function from base R to calculate the quantiles.\nThis is estimating the value that corresponds to each given quantile level by ordering the samples and then taking the value at the appropriate position.\n\n\n\n\n\n\nSimple unweighted ensembles\nA good place to start when building ensembles is to take the mean or median of the unweighted forecast at each quantile level, and treat these as quantiles of the ensemble predictive distribution. Typically, the median is preferred when outlier forecasts are likely to be present as it is less sensitive to these. However, the mean is preferred when forecasters have more faith in models that diverge from the median performance and want to represent this in the ensemble.\n\n\n\n\n\n\nVincent average\n\n\n\n\n\nThe procedure of calculating quantiles of a new distribution as a weighted average of quantiles of constituent distributions (e.g., different measurements) is called a Vincent average, after the biologist Stella Vincent who described this as early as 1912 when studying the function of whiskers in the behaviour of white rats.\n\n\n\n\nConstruction\nWe can calculate the mean quantile ensemble by taking the mean of the forecasts at each quantile level. This function is also already built for us by the hubverse, so we will use the hubEnsembles::simple_ensemble() function for this:\n\nmean_ensemble &lt;- quantile_forecasts |&gt;\n  hubEnsembles::simple_ensemble(\n    model_id = \"mean_ensemble\",\n    agg_fun = mean ## this is the default, but writing it to be explicit\n  )\n\nSimilarly, we can calculate the median ensemble by taking the median of the forecasts at each quantile level.\n\nmedian_ensemble &lt;- quantile_forecasts |&gt;\n  hubEnsembles::simple_ensemble(\n    model_id = \"median_ensemble\",\n    agg_fun = median ## this is the default, but writing it to be explicit\n  )\n\nWe combine the ensembles into a single data frame along with the individual forecasts in order to make visualisation easier.\n\nall_quantile_forecasts &lt;- bind_rows(\n  mean_ensemble,\n  median_ensemble,\n  quantile_forecasts\n)\n\n\n\nVisualisation\nHow do these ensembles visually differ from the individual models? Lets start by plotting a single forecast from each model and comparing them. Here we are using the hubVis package which creates (by default) an interactive figure using plotly.\n\nhubVis::plot_step_ahead_model_output(\n  model_out_tbl = all_quantile_forecasts,\n  target_data = flu_data |&gt; \n    rename(observation = wili) |&gt; \n    filter(epiweek &gt;= as.Date(\"2017-09-01\")),\n  use_median_as_point = TRUE,\n  x_col_name = \"target_epiweek\",\n  x_target_col_name = \"epiweek\",\n  facet = \"forecast_date\",\n  facet_nrow = 6,\n  intervals = 0.8,\n  fill_transparency = 0.5,\n  interactive = TRUE\n)\n\n\n\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nHow do these ensembles differ from each other? (Hint: the plot above is interactive, so you can zoom in/out and click on one model in the legend to select/de-select a model for targeted comparison.)\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe ensembles are mostly very similar to each other, except for during the peak of the season, when the fourier model is predicting something very different from the other forecasts. In those cases, the mean ensemble is impacted more, since that single outlying forecast pulls the mean down at various quantile levels. However, the median ensemble remains close to the three other models that have close agreement, at least for the first few horizons.\n\n\n\n\n\nEvaluation\nAs in the forecast evaluation session, we can evaluate the accuracy of the ensembles. Here, instead of using the fable utilities, we will start to use the hubverse hubEvals package which relies on the scoringutils package under the hood.\nIn the hubverse, there is a representation of observations called oracle output data. This is a way of representing an observed data point as if it were a prediction from an oracle model, that is, one that knows the future before it happens. The package hubEvals expects observations to be stored in this format, so we will first begin by restructuring the flu_data into this format.\n\ntarget_epiweeks &lt;- unique(all_quantile_forecasts$target_epiweek)\n\noracle_output &lt;- flu_data |&gt; \n  filter(epiweek %in% target_epiweeks) |&gt; \n  rename(target_epiweek = epiweek,\n         oracle_value = wili) |&gt; \n  select(-region)\n\n\n\n\n\n\n\nNote\n\n\n\nThe weighted interval score (WIS) is a proper scoring rule for quantile forecasts that approximates the Continuous Ranked Probability Score (CRPS) by considering a weighted sum of multiple prediction intervals. As the number of intervals increases, the WIS converges to the CRPS, combining sharpness and penalties for over- and under-prediction.(Bracher et al. 2021)\nWe see it here as we are scoring quantiles and known distributions hence we cannot use CRPS as we did before.\n\n\nAgain we start with a high level overview of the scores by model.\n\nforecast_scores &lt;- hubEvals::score_model_out(\n  all_quantile_forecasts,\n  oracle_output, \n  by = \"model_id\"\n)\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\nforecast_scores|&gt; \n  arrange(wis) |&gt; \n  knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_id\nwis\noverprediction\nunderprediction\ndispersion\nbias\ninterval_coverage_50\ninterval_coverage_90\nae_median\n\n\n\n\nfourier_ar2\n0.30\n0.02\n0.18\n0.10\n-0.18\n0.70\n0.91\n0.54\n\n\nmedian_ensemble\n0.37\n0.04\n0.21\n0.12\n-0.04\n0.62\n0.87\n0.71\n\n\nmean_ensemble\n0.40\n0.05\n0.24\n0.12\n-0.11\n0.60\n0.79\n0.77\n\n\nar2\n0.44\n0.08\n0.23\n0.13\n0.05\n0.42\n0.84\n0.88\n\n\nfourier\n0.56\n0.00\n0.44\n0.12\n-0.37\n0.68\n0.78\n0.88\n\n\nrw\n0.64\n0.27\n0.25\n0.12\n0.04\n0.26\n0.72\n1.12\n\n\n\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nWhat do you think the scores are telling you? Which model do you think is best? What other scoring breakdowns might you want to look at?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWhat do you think the scores are telling you? Which model do you think is best?\n\nThe fourier_ar2 model appears to be the best performing ensemble model overall according to WIS, although both ensemble models have less bias.\nOften ensemble forecasts will be better than all individual models, but as this example shows, this is not always the case. It is often not the case when one model is substantially better than the others, so the ensemble gives too much weight to models that aren’t that good.\n\nWhat other scoring breakdowns might you want to look at?\n\nThere might be variation over forecast dates or horizons between the different ensemble methods\n\n\n\n\n\n\n\nUnweighted ensembles of filtered models\nA simple method that is often used to improve ensemble performance is to prune out models that perform very poorly. Balancing this can be tricky however as it can be hard to know how much to prune. The key tradeoff to consider is how much to optimise for which models have performed well in the past (and what your definition of the past is, for example all time or only the last few weeks) versus how much you want to allow for the possibility that these models may not perform well in the future. There is strong evidence from multiple forecasting challenges that forecast accuracy from a given model can vary widely over time.(Ray et al. 2023)\n\nConstruction\nAs we just saw, the random walk model is performing poorly in comparison to the other models. We can remove this model from the ensemble and see if this improves the performance of the ensemble.\n\n\n\n\n\n\nWarning\n\n\n\nHere we are technically cheating a little as we are using the test data to help select the models to include in the ensemble. In the real world you would not do this as you would not have access to the test data and so this is an idealised scenario.\n\n\n\nfiltered_forecasts &lt;- quantile_forecasts |&gt;\n  filter(model_id != \"rw\")\n\nWe then need to recalculate the ensembles. First the mean ensemble,\n\nfiltered_mean_ensembles &lt;- filtered_forecasts |&gt;\n  hubEnsembles::simple_ensemble(\n    model_id = \"mean_ensemble_filtered\",\n    agg_fun = mean ## this is the default, but writing it to be explicit\n  )\n\nand then the median ensemble.\n\nfiltered_median_ensembles &lt;- filtered_forecasts |&gt;\n  hubEnsembles::simple_ensemble(\n    model_id = \"median_ensemble_filtered\",\n    agg_fun = median ## this is the default, but writing it to be explicit\n  )\n\nWe combine these new ensembles with our previous ensembles in order to make visualisation easier.\n\nfiltered_ensembles &lt;- bind_rows(\n  filtered_mean_ensembles,\n  filtered_median_ensembles,\n  all_quantile_forecasts\n)\n\n\n\nVisualisation\nAs for the simple ensembles, we can plot a single forecast from each model and ensemble.\n\nhubVis::plot_step_ahead_model_output(\n  model_out_tbl = filtered_ensembles,\n  target_data = flu_data |&gt; \n    rename(observation = wili) |&gt; \n    filter(epiweek &gt;= as.Date(\"2017-09-01\")),\n  use_median_as_point = TRUE,\n  x_col_name = \"target_epiweek\",\n  x_target_col_name = \"epiweek\",\n  facet = \"forecast_date\",\n  facet_nrow = 6,\n  intervals = 0.8,\n  fill_transparency = 0.5,\n  interactive = TRUE\n)\n\n\n\n\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nUse the interactive plot above to answer these questions. How do the mean and median filtered ensembles compare to their simple ensemble counterparts? Which of the two filtered ensembles do you think is better?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHow do the filtered ensembles compare to the simple ensembles?\n\nThe filtered ensembles appear to be less variable and closer to the eventual observations than the simple ensembles, especially during times of rapid change.\n\nWhich do you think is best?\n\nVisually, the filtered ensembles appear very similar. This makes sense given we know there are only three models left in the ensemble. There are a few places where it appears the median is a bit closer to the observed data.\n\n\n\n\n\n\nEvaluation\nLet us score the filtered ensembles to obtain a high level overview of the scores by model.\n\nfiltered_forecast_scores &lt;- hubEvals::score_model_out(\n  filtered_ensembles,\n  oracle_output, \n  by = \"model_id\"\n)\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\nfiltered_forecast_scores |&gt; \n  arrange(wis) |&gt; \n  knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_id\nwis\noverprediction\nunderprediction\ndispersion\nbias\ninterval_coverage_50\ninterval_coverage_90\nae_median\n\n\n\n\nfourier_ar2\n0.30\n0.02\n0.18\n0.10\n-0.18\n0.70\n0.91\n0.54\n\n\nmedian_ensemble_filtered\n0.32\n0.02\n0.19\n0.11\n-0.13\n0.68\n0.90\n0.60\n\n\nmedian_ensemble\n0.37\n0.04\n0.21\n0.12\n-0.04\n0.62\n0.87\n0.71\n\n\nmean_ensemble_filtered\n0.38\n0.02\n0.25\n0.12\n-0.09\n0.68\n0.84\n0.68\n\n\nmean_ensemble\n0.40\n0.05\n0.24\n0.12\n-0.11\n0.60\n0.79\n0.77\n\n\nar2\n0.44\n0.08\n0.23\n0.13\n0.05\n0.42\n0.84\n0.88\n\n\nfourier\n0.56\n0.00\n0.44\n0.12\n-0.37\n0.68\n0.78\n0.88\n\n\nrw\n0.64\n0.27\n0.25\n0.12\n0.04\n0.26\n0.72\n1.12\n\n\n\n\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nHow do the filtered ensembles compare to the simple ensembles?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHow do the filtered ensembles compare to the simple ensembles?\n\nThe filtered ensembles appear to be more accurate than the simple ensembles.\nThe ensembles still do not quite beat out the fourier_ar2 model, although the gap has narrowed.\nIn general, the median ensemble improved more than the mean ensemble after the filtering.",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#sample-based-ensembles",
    "href": "sessions/forecast-ensembles.html#sample-based-ensembles",
    "title": "Forecast ensembles",
    "section": "Sample-based ensembles",
    "text": "Sample-based ensembles\nQuantile averaging can be interpreted as a combination of different uncertain estimates of a true distribution of a given shape. Instead, we might want to interpret multiple models as multiple possible versions of this truth, with weights assigned to each of them representing the probability of each one being the true one. In that case, we want to create a (possibly weighted) mixture distribution of the constituent models. This can be done very straight-forwardly using samples. The procedure is sometimes called a linear opinion pool.\n\n\n\n\n\n\nComparing ensembling methods\n\n\n\nThis nice figure from Howerton et al. shows visually the difference between different ensembling approaches.(Howerton et al. 2023)\n\n\n\nComparing LOP vs vincent averaging for ensemble.\n\n\nAbove we used Vincent averaging through the hubEnsembles::simple_ensemble() function. Below we will use the linear opinion pool, illustrated by the LOP line in the figure.\n\n\nWe will start by adding two new unweighted ensemble approaches (one filtered, one not, similar to the section above) to our growing collection of models. Recall that we had created hubverse-style sample-based forecasts.\n\nsampled_forecasts_hub\n\n# A tibble: 38,400 × 7\n   model_id forecast_date target_epiweek     h output_type output_type_id value\n   &lt;chr&gt;    &lt;date&gt;        &lt;date&gt;         &lt;int&gt; &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n 1 rw       2017-08-27    2017-09-03         1 sample      nat001         1.12 \n 2 rw       2017-08-27    2017-09-10         2 sample      nat001         1.14 \n 3 rw       2017-08-27    2017-09-17         3 sample      nat001         1.26 \n 4 rw       2017-08-27    2017-09-24         4 sample      nat001         1.24 \n 5 rw       2017-08-27    2017-10-01         5 sample      nat001         1.02 \n 6 rw       2017-08-27    2017-10-08         6 sample      nat001         1.08 \n 7 rw       2017-08-27    2017-10-15         7 sample      nat001         0.750\n 8 rw       2017-08-27    2017-10-22         8 sample      nat001         0.634\n 9 rw       2017-08-27    2017-09-03         1 sample      nat010         1.01 \n10 rw       2017-08-27    2017-09-10         2 sample      nat010         0.987\n# ℹ 38,390 more rows\n\n\n\nGenerate sample-based ensembles\nWe can generate a linear opinion pool of the samples from these models, where we specify that we want to sample unique trajectories predicted by each model. In this example, a trajectory is defined by a \"model_id\" and \"forecast_date\" combination. In the code below, we specify that we want to generate a linear opinion pool ensemble that has a total of 100 sample trajectories (in this case, it will sample 25 trajectories from each model) for each \"forecast_date\". As we did earlier, we compute one “filtered” ensemble that removes the random walk model.\n\nlop_ensemble &lt;- sampled_forecasts_hub |&gt; \n  hubEnsembles::linear_pool(\n  n_output_samples = 100,\n  model_id = \"lop_ensemble\",\n  task_id_cols = c(\"forecast_date\", \"h\", \"target_epiweek\"), ## columns that define a forecast task\n  compound_taskid_set = c(\"forecast_date\"), ## column that defines a single trajectory\n  derived_task_ids = c(\"target_epiweek\")    ## column that can be derived from the others\n  )   \n\nlop_ensemble_filtered &lt;- sampled_forecasts_hub |&gt; \n  filter(model_id != \"rw\") |&gt; \n  hubEnsembles::linear_pool(\n  n_output_samples = 100,\n  model_id = \"lop_ensemble_filtered\",\n  task_id_cols = c(\"forecast_date\", \"h\", \"target_epiweek\"), ## columns that define a forecast task\n  compound_taskid_set = c(\"forecast_date\"), ## column that defines a single trajectory\n  derived_task_ids = c(\"target_epiweek\")    ## column that can be derived from the others\n  )   \n\n\n\nVisualize sample-based forecasts\nTo facilitate visual and quantitative comparisons with the earlier ensemble forecasts, we convert the new ensemble forecasts to quantiles and then plot them.\n\nforecasts_with_lop &lt;- bind_rows(\n  lop_ensemble,\n  lop_ensemble_filtered\n) |&gt; \n  hubUtils::convert_output_type(to = list(quantile = quantiles_to_save)) |&gt; \n  bind_rows(filtered_ensembles)\n\nhubVis::plot_step_ahead_model_output(\n  model_out_tbl = forecasts_with_lop,\n  target_data = flu_data |&gt; \n    rename(observation = wili) |&gt; \n    filter(epiweek &gt;= as.Date(\"2017-09-01\")),\n  use_median_as_point = TRUE,\n  x_col_name = \"target_epiweek\",\n  x_target_col_name = \"epiweek\",\n  facet = \"forecast_date\",\n  facet_nrow = 6,\n  intervals = 0.8,\n  fill_transparency = 0.8,\n  interactive = TRUE,\n  pal_color = \"Set3\"\n)\n\n\n\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nUsing the interactive plot, isolate just the lop_ensemble_filtered and median_ensemble_filtered. How and when are they different from each other?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe LOP ensemble tends to be a bit wider than the median_ensemble_filtered forecast, especially at times when the individual component forecasts disagree more.\n\n\n\n\n\nEvaluate sample-based forecasts\nLooking at the same overall by-model evaluations as we ran earlier, the LOP ensembles are “in the pack” with the other ensembles, but do not provide substantial improvements over the other ensembles.\n\nforecast_scores &lt;- hubEvals::score_model_out(\n  forecasts_with_lop,\n  oracle_output, \n  by = \"model_id\"\n)\n\nℹ Some rows containing NA values may be removed. This is fine if not\n  unexpected.\n\nforecast_scores|&gt; \n  arrange(wis) |&gt; \n  knitr::kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel_id\nwis\noverprediction\nunderprediction\ndispersion\nbias\ninterval_coverage_50\ninterval_coverage_90\nae_median\n\n\n\n\nfourier_ar2\n0.30\n0.02\n0.18\n0.10\n-0.18\n0.70\n0.91\n0.54\n\n\nmedian_ensemble_filtered\n0.32\n0.02\n0.19\n0.11\n-0.13\n0.68\n0.90\n0.60\n\n\nmedian_ensemble\n0.37\n0.04\n0.21\n0.12\n-0.04\n0.62\n0.87\n0.71\n\n\nlop_ensemble_filtered\n0.37\n0.01\n0.22\n0.14\n-0.14\n0.76\n0.88\n0.64\n\n\nmean_ensemble_filtered\n0.38\n0.02\n0.25\n0.12\n-0.09\n0.68\n0.84\n0.68\n\n\nlop_ensemble\n0.39\n0.02\n0.22\n0.15\n-0.09\n0.72\n0.88\n0.69\n\n\nmean_ensemble\n0.40\n0.05\n0.24\n0.12\n-0.11\n0.60\n0.79\n0.77\n\n\nar2\n0.44\n0.08\n0.23\n0.13\n0.05\n0.42\n0.84\n0.88\n\n\nfourier\n0.56\n0.00\n0.44\n0.12\n-0.37\n0.68\n0.78\n0.88\n\n\nrw\n0.64\n0.27\n0.25\n0.12\n0.04\n0.26\n0.72\n1.12",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#weighted-ensembles",
    "href": "sessions/forecast-ensembles.html#weighted-ensembles",
    "title": "Forecast ensembles",
    "section": "Weighted ensembles",
    "text": "Weighted ensembles\nThe simple mean and median we used to average quantiles earlier treats every model as the same. We could try to improve performance by replacing this with a weighted mean (or weighted median), for example giving greater weight to models that have made better forecasts in the past.\n\n\n\n\n\n\nDoes weighting improve ensemble forecasts?\n\n\n\n\n\nThere is a rich literature on weighting individual forecasts to improve an ensemble. In epidemiological forecasting, the track record is mixed. In settings where a long track record (multiple seasons) of performance can be measured and where the accuracy of individual models doesn’t vary too much, then weighting has been shown to help with influenza(Reich et al. 2019), dengue fever(Colón-González et al. 2021), and some COVID-19(Ray et al. 2023) forecasts. However, in forecasting COVID-19 cases and hospitalizations, where model performance fluctuated over time, it was harder to see improvements in weighted ensembles.(Ray et al. 2023)\nHowever, estimating weights for a model adds parameters and uncertainty to the ensemble, and there are theoretical reasons to prefer a simple ensemble approach.(Clemen 1989; Claeskens et al. 2016)",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#challenge",
    "href": "sessions/forecast-ensembles.html#challenge",
    "title": "Forecast ensembles",
    "section": "Challenge",
    "text": "Challenge\n\nGiven the performance by forecast date shown by the different models, would you expect a weighted ensemble to perform better than the unweighted ensemble?\nRe-run the time-series cross-validation using a .step = 1 instead of 4. This increases the number of forecasts you can evaluate. Do the results change?",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#methods-in-the-real-world",
    "href": "sessions/forecast-ensembles.html#methods-in-the-real-world",
    "title": "Forecast ensembles",
    "section": "Methods in the real world",
    "text": "Methods in the real world\n\nHowerton et al. (2023) suggests that the choice of an ensemble method should be informed by an assumption about how to represent uncertainty between models: whether differences between component models is “noisy” variation around a single underlying distribution, or represents structural uncertainty about the system.\nSherratt et al. (2023) investigates the performance of different ensembles in the European COVID-19 Forecast Hub.\nAmaral et al. (2025) discusses the challenges in improving on the predictive performance of simpler approaches using weighted ensembles.\nRay et al. (2023) evaluates the performance of ensemble forecasts in predicting COVID-19 cases, hospitalization and deaths, both with weighted and unweighted ensembles.\nReich et al. (2019) evaluates the performance of ensemble forecasts in the context of seasonal influenza.",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecast-ensembles.html#references",
    "href": "sessions/forecast-ensembles.html#references",
    "title": "Forecast ensembles",
    "section": "References",
    "text": "References\n\n\nAmaral, André Victor Ribeiro, Daniel Wolffram, Paula Moraga, and Johannes Bracher. 2025. “Post-Processing and Weighted Combination of Infectious Disease Nowcasts.” PLoS Computational Biology 21 (3): e1012836. https://doi.org/10.1371/journal.pcbi.1012836.\n\n\nBracher, Johannes, Evan L. Ray, Tilmann Gneiting, and Nicholas G. Reich. 2021. “Evaluating Epidemic Forecasts in an Interval Format.” PLOS Computational Biology 17 (2): e1008618. https://doi.org/10.1371/journal.pcbi.1008618.\n\n\nClaeskens, Gerda, Jan R. Magnus, Andrey L. Vasnev, and Wendun Wang. 2016. “The Forecast Combination Puzzle: A Simple Theoretical Explanation.” International Journal of Forecasting 32 (3): 754–62. https://doi.org/10.1016/j.ijforecast.2015.12.005.\n\n\nClemen, Robert T. 1989. “Combining Forecasts: A Review and Annotated Bibliography.” International Journal of Forecasting 5 (4): 559–83. https://doi.org/10.1016/0169-2070(89)90012-5.\n\n\nColón-González, Felipe J., Leonardo Soares Bastos, Barbara Hofmann, Alison Hopkin, Quillon Harpham, Tom Crocker, Rosanna Amato, et al. 2021. “Probabilistic Seasonal Dengue Forecasting in Vietnam: A Modelling Study Using Superensembles.” PLOS Medicine 18 (3): e1003542. https://doi.org/10.1371/journal.pmed.1003542.\n\n\nHowerton, Emily, Michael C. Runge, Tiffany L. Bogich, Rebecca K. Borchering, Hidetoshi Inamine, Justin Lessler, Luke C. Mullany, et al. 2023. “Context-Dependent Representation of Within- and Between-Model Uncertainty: Aggregating Probabilistic Predictions in Infectious Disease Epidemiology.” Journal of The Royal Society Interface 20 (198): 20220659. https://doi.org/10.1098/rsif.2022.0659.\n\n\nRay, Evan L., Logan C. Brooks, Jacob Bien, Matthew Biggerstaff, Nikos I. Bosse, Johannes Bracher, Estee Y. Cramer, et al. 2023. “Comparing Trained and Untrained Probabilistic Ensemble Forecasts of COVID-19 Cases and Deaths in the United States.” International Journal of Forecasting 39 (3): 1366–83. https://doi.org/10.1016/j.ijforecast.2022.06.005.\n\n\nReich, Nicholas G., Craig J. McGowan, Teresa K. Yamana, Abhinav Tushar, Evan L. Ray, Dave Osthus, Sasikiran Kandula, et al. 2019. “Accuracy of Real-Time Multi-Model Ensemble Forecasts for Seasonal Influenza in the U.S.” PLOS Computational Biology 15 (11): e1007486. https://doi.org/10.1371/journal.pcbi.1007486.\n\n\nSherratt, Katharine, Hugo Gruson, Rok Grah, Helen Johnson, Rene Niehus, Bastian Prasse, Frank Sandmann, et al. 2023. “Predictive Performance of Multi-Model Ensemble Forecasts of COVID-19 Across European Nations.” Edited by Amy Wesolowski, Neil M Ferguson, Jeffrey L Shaman, and Sen Pei. eLife 12 (April): e81916. https://doi.org/10.7554/eLife.81916.",
    "crumbs": [
      "Forecast ensembles"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html",
    "href": "sessions/forecasting-concepts.html",
    "title": "Forecasting concepts",
    "section": "",
    "text": "Epidemiological forecasts are statements about what might happen to population disease burden in the future. These statements sometimes will be a simple point estimate. Other times, they might be probabilistic, meaning they are making an explicit statistical statement about the probabilities of different future outcomes. In this session we will make some simple forecasts using a decades-old statistical modeling approach, Auto-Regressive Integrated Moving Average (ARIMA), which has also shown itself to be a very capable framework for making predictions of infectious diseases. We will see how we can visualise and interpret such forecasts.\n\n\n\nIntroduction to forecasting\n\n\n\n\nThe aim of this session is to introduce the concept of forecasting and forecast visualisation using a simple model.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecast-concepts.qmd.\n\n\n\nIn this session we will use the fable package for fitting simple forecasting models, the dplyr package for data wrangling, the ggplot2 library for plotting and the epidatr package for accessing and downloading versions of epidemiological surveillance data from the Delphi EpiData API.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\nlibrary(\"feasts\")\nlibrary(\"fable\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(17)",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#slides",
    "href": "sessions/forecasting-concepts.html#slides",
    "title": "Forecasting concepts",
    "section": "",
    "text": "Introduction to forecasting",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#objectives",
    "href": "sessions/forecasting-concepts.html#objectives",
    "title": "Forecasting concepts",
    "section": "",
    "text": "The aim of this session is to introduce the concept of forecasting and forecast visualisation using a simple model.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecast-concepts.qmd.\n\n\n\nIn this session we will use the fable package for fitting simple forecasting models, the dplyr package for data wrangling, the ggplot2 library for plotting and the epidatr package for accessing and downloading versions of epidemiological surveillance data from the Delphi EpiData API.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\nlibrary(\"feasts\")\nlibrary(\"fable\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(17)",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#source-file",
    "href": "sessions/forecasting-concepts.html#source-file",
    "title": "Forecasting concepts",
    "section": "",
    "text": "The source file of this session is located at sessions/forecast-concepts.qmd.",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#libraries-used",
    "href": "sessions/forecasting-concepts.html#libraries-used",
    "title": "Forecasting concepts",
    "section": "",
    "text": "In this session we will use the fable package for fitting simple forecasting models, the dplyr package for data wrangling, the ggplot2 library for plotting and the epidatr package for accessing and downloading versions of epidemiological surveillance data from the Delphi EpiData API.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"epidatr\")\nlibrary(\"feasts\")\nlibrary(\"fable\")\ntheme_set(theme_bw())\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#initialisation",
    "href": "sessions/forecasting-concepts.html#initialisation",
    "title": "Forecasting concepts",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. This is not strictly necessary but will help us talk about the models.\n\nset.seed(17)",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#look-at-the-data",
    "href": "sessions/forecasting-concepts.html#look-at-the-data",
    "title": "Forecasting concepts",
    "section": "Look at the data",
    "text": "Look at the data\nHere are the first 10 rows of the dataset.\n\nflu_data\n\n# A tsibble: 732 x 3 [7D]\n# Key:       region [1]\n   region epiweek     wili\n   &lt;chr&gt;  &lt;date&gt;     &lt;dbl&gt;\n 1 nat    2003-08-24 0.490\n 2 nat    2003-08-31 0.736\n 3 nat    2003-09-07 0.582\n 4 nat    2003-09-14 0.654\n 5 nat    2003-09-21 0.750\n 6 nat    2003-09-28 0.884\n 7 nat    2003-10-05 1.03 \n 8 nat    2003-10-12 1.28 \n 9 nat    2003-10-19 1.33 \n10 nat    2003-10-26 1.77 \n# ℹ 722 more rows\n\n\nAnd here is a simple ggplot-style figure of the data.\n\nggplot(flu_data) +\n  geom_path(aes(x=epiweek, y=wili))\n\n\n\n\n\n\n\n\nBut note that one of the advantages of using tsibbles is that there are some default time-series plots that can be made:\n\nautoplot(flu_data, .vars = wili) +\n  labs(title = \"National-level ILI data from the US CDC\",\n       y = \"weighted ILI (% of visits)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nLook at the plot of data carefully. Write down at least three observations about the data. Share notes with your neighbor. What features of this time-series make it easier or harder to predict?",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-concepts.html#anatomy-of-a-fable-forecast",
    "href": "sessions/forecasting-concepts.html#anatomy-of-a-fable-forecast",
    "title": "Forecasting concepts",
    "section": "Anatomy of a fable forecast",
    "text": "Anatomy of a fable forecast\nWe will dive into this in more detail later in the course, but it might be interesting now to look under the hood of what format fable uses to store the forecast itself. Check it out:\n\nclass(first_forecast)\n\n[1] \"fbl_ts\"     \"tbl_ts\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nfirst_forecast\n\n# A fable: 25 x 5 [7D]\n# Key:     region, .model [1]\n   region .model  epiweek             wili .mean\n   &lt;chr&gt;  &lt;chr&gt;   &lt;date&gt;            &lt;dist&gt; &lt;dbl&gt;\n 1 nat    arima2… 2017-09-03 N(1.1, 0.083)  1.12\n 2 nat    arima2… 2017-09-10  N(1.2, 0.26)  1.22\n 3 nat    arima2… 2017-09-17  N(1.3, 0.48)  1.30\n 4 nat    arima2… 2017-09-24   N(1.4, 0.7)  1.37\n 5 nat    arima2… 2017-10-01  N(1.4, 0.89)  1.43\n 6 nat    arima2… 2017-10-08     N(1.5, 1)  1.48\n 7 nat    arima2… 2017-10-15   N(1.5, 1.2)  1.53\n 8 nat    arima2… 2017-10-22   N(1.6, 1.3)  1.57\n 9 nat    arima2… 2017-10-29   N(1.6, 1.3)  1.60\n10 nat    arima2… 2017-11-05   N(1.6, 1.4)  1.62\n# ℹ 15 more rows\n\n\nThe first_forecast object is fbl_ts which is a special kind of tsibble, which is in turn a special kind of data.frame. In this case, each horizon for which we’ve asked for a prediction has a row in the dataset. There is a .mean column where you can quickly see the predicted mean, and a wili column which contains values of class dist. Each of these represent the Normal distribution representing the predictive distribution for the WILI value at that horizon.\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nWhat mean value do you think the forecast would settle at if we predicted out for a hundred weeks or more?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere is the prediction for h=25.\n\nfirst_forecast[25,]\n\n# A fable: 1 x 5 [7D]\n# Key:     region, .model [1]\n  region .model   epiweek           wili .mean\n  &lt;chr&gt;  &lt;chr&gt;    &lt;date&gt;          &lt;dist&gt; &lt;dbl&gt;\n1 nat    arima200 2018-02-18 N(1.7, 1.5)  1.74\n\n\nHere is the overall mean of the dataset the model was fit to:\n\nmean(flu_data$wili)\n\n[1] 1.77135\n\n\nARIMA models that meet certain regularity conditions will have forecasts that converge to the mean of the dataset.",
    "crumbs": [
      "Forecasting concepts"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html",
    "href": "sessions/forecasting-nowcasting.html",
    "title": "Combining nowcasting and forecasting",
    "section": "",
    "text": "In the previous sessions, we’ve explored nowcasting (estimating what current data will look like once all reports are in) and forecasting (predicting future trends) as separate problems. However, in real-time epidemic analysis, these challenges are connected. When we want to forecast from the most recent data, we face the problem that this data is incomplete due to reporting delays.\nIn this session, we’ll explore different approaches to handling this challenge, from simple methods that ignore the problem to joint models.\n\n\n\nCombining nowcasting and forecasting\n\n\n\n\nThis session aims to show how to combine nowcasting and forecasting to make better predictions when data are subject to reporting delays.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecasting-nowcasting.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access Stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference, and scoringutils for forecast evaluation.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"scoringutils\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models. Finally, we set an option to not warn about the partial definition of initial conditions.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)\noptions(cmdstanr_warn_inits = FALSE)",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#slides",
    "href": "sessions/forecasting-nowcasting.html#slides",
    "title": "Combining nowcasting and forecasting",
    "section": "",
    "text": "Combining nowcasting and forecasting",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#objectives",
    "href": "sessions/forecasting-nowcasting.html#objectives",
    "title": "Combining nowcasting and forecasting",
    "section": "",
    "text": "This session aims to show how to combine nowcasting and forecasting to make better predictions when data are subject to reporting delays.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/forecasting-nowcasting.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access Stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference, and scoringutils for forecast evaluation.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"scoringutils\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models. Finally, we set an option to not warn about the partial definition of initial conditions.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)\noptions(cmdstanr_warn_inits = FALSE)",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#source-file",
    "href": "sessions/forecasting-nowcasting.html#source-file",
    "title": "Combining nowcasting and forecasting",
    "section": "",
    "text": "The source file of this session is located at sessions/forecasting-nowcasting.qmd.",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#libraries-used",
    "href": "sessions/forecasting-nowcasting.html#libraries-used",
    "title": "Combining nowcasting and forecasting",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access Stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, the tidybayes package for extracting results of the inference, and scoringutils for forecast evaluation.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\nlibrary(\"scoringutils\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#initialisation",
    "href": "sessions/forecasting-nowcasting.html#initialisation",
    "title": "Combining nowcasting and forecasting",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models. Finally, we set an option to not warn about the partial definition of initial conditions.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)\noptions(cmdstanr_warn_inits = FALSE)",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#generate-the-underlying-epidemic",
    "href": "sessions/forecasting-nowcasting.html#generate-the-underlying-epidemic",
    "title": "Combining nowcasting and forecasting",
    "section": "Generate the underlying epidemic",
    "text": "Generate the underlying epidemic\nFirst, let’s generate our simulated onset dataset as we did in previous sessions:\n\ngen_time_pmf &lt;- make_gen_time_pmf()\nip_pmf &lt;- make_ip_pmf()\nonset_df &lt;- simulate_onsets(\n  make_daily_infections(infection_times), gen_time_pmf, ip_pmf\n)\nhead(onset_df)\n\n# A tibble: 6 × 3\n    day onsets infections\n  &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;\n1     1      0          0\n2     2      0          1\n3     3      0          0\n4     4      1          2\n5     5      0          1\n6     6      0          1\n\n# Set our analysis cutoff point\ncutoff &lt;- 61\nforecast_horizon &lt;- 14  # We'll forecast 14 days ahead",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#apply-reporting-delays",
    "href": "sessions/forecasting-nowcasting.html#apply-reporting-delays",
    "title": "Combining nowcasting and forecasting",
    "section": "Apply reporting delays",
    "text": "Apply reporting delays\nNow we’ll simulate reporting delays and create our reporting triangle:\n\nreporting_delay_pmf &lt;- censored_delay_pmf(\n  rlnorm, max = 15, meanlog = 1, sdlog = 0.5\n)\nplot(reporting_delay_pmf)\n\n\n\n\n\n\n\n\n\nreporting_triangle &lt;- onset_df |&gt;\n  filter(day &lt;= cutoff + forecast_horizon) |&gt;\n  mutate(\n    reporting_delay = list(\n      tibble(d = 0:15, reporting_delay = reporting_delay_pmf)\n    )\n  ) |&gt;\n  unnest(reporting_delay) |&gt;\n  mutate(\n    reported_onsets = rpois(n(), onsets * reporting_delay)\n  ) |&gt;\n  mutate(reported_day = day + d)\n\n# Filter to what would be observed by the cutoff\nfiltered_reporting_triangle &lt;- reporting_triangle |&gt;\n  filter(reported_day &lt;= cutoff)\n\ntail(filtered_reporting_triangle)\n\n# A tibble: 6 × 7\n    day onsets infections     d reporting_delay reported_onsets reported_day\n  &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;table[1d]&gt;               &lt;int&gt;        &lt;dbl&gt;\n1    59     37         59     0 0.00363684                    0           59\n2    59     37         59     1 0.12356754                    4           60\n3    59     37         59     2 0.30290897                    8           61\n4    60     43         79     0 0.00363684                    0           60\n5    60     43         79     1 0.12356754                    8           61\n6    61     45         71     0 0.00363684                    0           61\n\n\nNow we create the available onsets by summing reported onsets by day - this represents what we observe at the cutoff:\n\navailable_onsets &lt;- filtered_reporting_triangle |&gt;\n  summarise(available_onsets = sum(reported_onsets), .by = day)\n\ntail(available_onsets)\n\n# A tibble: 6 × 2\n    day available_onsets\n  &lt;dbl&gt;            &lt;int&gt;\n1    56               33\n2    57               32\n3    58               21\n4    59               12\n5    60                8\n6    61                0",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#create-forecast-evaluation-datasets",
    "href": "sessions/forecasting-nowcasting.html#create-forecast-evaluation-datasets",
    "title": "Combining nowcasting and forecasting",
    "section": "Create forecast evaluation datasets",
    "text": "Create forecast evaluation datasets\nFor our analysis, we need two additional datasets:\n\nComplete data at forecast horizon: What we would observe up to the forecast horizon\nFiltered data for complete data approach: Data that is “complete” (i.e. expected to be fully reported) at the time of forecast\n\n\n# Dataset 1: What we observe over the forecast horizon\ncomplete_at_horizon &lt;- reporting_triangle |&gt;\n  filter(day &lt;= cutoff + forecast_horizon) |&gt;\n  summarise(complete_onsets = sum(reported_onsets), .by = day)\n\n# Dataset 2: \"Complete\" data for complete data approach (e.g., &gt;14 days old)\ncomplete_threshold &lt;- 14\ncomplete_data &lt;- available_onsets |&gt;\n  filter(day &lt;= cutoff - complete_threshold)\n\n\n\n\n\n\n\nChoosing the “complete data” threshold\n\n\n\n\n\nWe’ve chosen 14 days as our threshold for “complete” data, but this is quite conservative. Let’s examine what proportion of reports we expect to have received by different delays:\n\n# Calculate cumulative reporting proportions\nreporting_cdf &lt;- cumsum(reporting_delay_pmf)\ntibble(\n  delay = 0:15,\n  pmf = reporting_delay_pmf,\n  cdf = reporting_cdf\n) |&gt;\n  filter(delay %in% c(1, 3, 5, 7, 10, 14)) |&gt;\n  mutate(\n    `Proportion reported` = round(cdf, 3),\n    `Proportion missing` = round(1 - cdf, 3)\n  ) |&gt;\n  select(\n    `Days since onset` = delay,\n    `Proportion reported`,\n    `Proportion missing`\n  ) |&gt;\n  knitr::kable(caption = \"Expected reporting completeness by delay\")\n\n\nExpected reporting completeness by delay\n\n\nDays since onset\nProportion reported\nProportion missing\n\n\n\n\n1\n0.127\n0.873\n\n\n3\n0.689\n0.311\n\n\n5\n0.920\n0.080\n\n\n7\n0.979\n0.021\n\n\n10\n0.997\n0.003\n\n\n14\n1.000\n0.000\n\n\n\n\n\nAs we can see, after 5 days we expect to have received ~92% of reports, and after 7 days ~98%. Using a 14-day threshold means we’re losing 7-9 days of potentially useful data.\nTrade-offs to consider:\n\nLess conservative thresholds (e.g., 5-7 days): Use more recent data but need to account for 5-15% missing reports\nMore conservative thresholds (e.g., 14 days): Very complete data but lose recent epidemic trends\nNowcasting approaches: Use all available data and explicitly model the reporting process\n\nTry experimenting with different thresholds (e.g., complete_threshold &lt;- 7) to see how it affects the forecasts!\n\n\n\nNow lets visualise the different datasets\n\nggplot() +\n  geom_line(data = complete_at_horizon, \n            aes(x = day, y = complete_onsets, colour = \"Complete data\"), \n            linewidth = 1) +\n  geom_line(data = available_onsets, \n            aes(x = day, y = available_onsets, colour = \"Available now\"), \n            linewidth = 1) +\n  geom_line(data = complete_data, \n            aes(x = day, y = available_onsets, colour = \"Complete as of now\"), \n            linewidth = 1.2) +\n  geom_vline(xintercept = cutoff, linetype = \"dotted\") +\n  geom_vline(xintercept = cutoff - complete_threshold, \n             linetype = \"dotted\", alpha = 0.5) +\n  scale_colour_manual(\n    values = c(\n      \"Complete data\" = \"black\",\n      \"Available now\" = \"red\", \n      \"Complete as of now\" = \"green\"\n    )\n  ) +\n  labs(\n    x = \"Day\",\n    y = \"Onsets\",\n    title = \"Different views of the data\",\n    colour = \"Data type\"\n  ) +\n  guides(\n    colour = guide_legend(nrow = 2)) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#prepare-evaluation-data",
    "href": "sessions/forecasting-nowcasting.html#prepare-evaluation-data",
    "title": "Combining nowcasting and forecasting",
    "section": "Prepare evaluation data",
    "text": "Prepare evaluation data\n\n# Create evaluation dataset - what we're forecasting against\nevaluation_data &lt;- complete_at_horizon |&gt;\n  filter(day &gt; cutoff, day &lt;= cutoff + forecast_horizon) |&gt;\n  mutate(\n    forecast_horizon = day - cutoff,\n    observed = complete_onsets\n  ) |&gt;\n  select(day, forecast_horizon, observed)\n\nhead(evaluation_data)\n\n# A tibble: 6 × 3\n    day forecast_horizon observed\n  &lt;dbl&gt;            &lt;dbl&gt;    &lt;int&gt;\n1    62                1       50\n2    63                2       39\n3    64                3       48\n4    65                4       66\n5    66                5       77\n6    67                6       71",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#visualise-all-the-forecasts-together",
    "href": "sessions/forecasting-nowcasting.html#visualise-all-the-forecasts-together",
    "title": "Combining nowcasting and forecasting",
    "section": "Visualise all the forecasts together",
    "text": "Visualise all the forecasts together\nAs in all other sessions, we first visualise the forecasts together.\n\nforecast_results &lt;- bind_rows(\n  complete_data_forecasts |&gt;\n    filter(actual_day &gt; cutoff),\n  pipeline_forecasts,\n  joint_forecasts\n)\n\nforecast_results |&gt;\n  ggplot(aes(x = actual_day, y = .value, group = .draw, color = approach)) +\n  geom_line(alpha = 0.1) +\n  geom_point(data = evaluation_data, aes(x = day, y = observed, group = NULL, color = NULL), color = \"black\", size = 1) +\n  scale_y_log10() +\n  labs(x = \"Day\", y = \"Onsets\", title = \"All forecast approaches\") +\n  theme_minimal() +\n  guides(color = guide_legend(\n    title = \"Approach\", override.aes = list(alpha = 1, linewidth = 1))\n  ) +\n  theme(legend.position = \"bottom\") +\n  facet_wrap(vars(approach), ncol = 1)\n\nWarning in scale_y_log10(): log-10 transformation introduced infinite values.",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#quantitative-evaluation-using-scoringutils",
    "href": "sessions/forecasting-nowcasting.html#quantitative-evaluation-using-scoringutils",
    "title": "Combining nowcasting and forecasting",
    "section": "Quantitative evaluation using scoringutils",
    "text": "Quantitative evaluation using scoringutils\nUsing scoringutils, We first create a forecast sample object\n\nsc_forecasts &lt;- forecast_results |&gt;\n    left_join(evaluation_data, by = c(\"actual_day\" = \"day\")) |&gt;\n    group_by(approach, actual_day) |&gt;\n    mutate(\n      .draw = row_number()\n    ) |&gt;\n    as_forecast_sample(\n      forecast_unit = c(\"approach\", \"actual_day\"),\n      observed = \"observed\",\n      predicted = \".value\",\n      sample_id = \".draw\"\n    )\nsc_forecasts\n\nForecast type: sample\n\n\nForecast unit:\n\n\napproach and actual_day\n\n\n\n      sample_id predicted observed      approach actual_day\n          &lt;int&gt;     &lt;num&gt;    &lt;int&gt;        &lt;char&gt;      &lt;num&gt;\n   1:         1        83       50 Complete data         62\n   2:         2       145       50 Complete data         62\n   3:         3       108       50 Complete data         62\n   4:         4        31       50 Complete data         62\n   5:         5       150       50 Complete data         62\n  ---                                                      \n4196:        96       273       96         Joint         75\n4197:        97       802       96         Joint         75\n4198:        98       153       96         Joint         75\n4199:        99       283       96         Joint         75\n4200:       100        94       96         Joint         75\n\n\nand then calculate the scores:\n\nscores &lt;- sc_forecasts |&gt;\n  score()\n\nhead(scores)\n\n        approach actual_day  bias      dss    crps overprediction\n          &lt;char&gt;      &lt;num&gt; &lt;num&gt;    &lt;num&gt;   &lt;num&gt;          &lt;num&gt;\n1: Complete data         62  0.25 13.53014 20.4381           1.78\n2: Complete data         63  0.49 14.69952 30.4814           7.50\n3: Complete data         64  0.40 15.81995 34.4264           6.48\n4: Complete data         65  0.28 16.93241 38.0538           2.36\n5: Complete data         66  0.12 18.00671 47.8366           0.36\n6: Complete data         67  0.33 19.06576 66.0303           6.40\n   underprediction dispersion log_score      mad ae_median    se_mean\n             &lt;num&gt;      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:               0    18.6581  4.983849  55.5975      15.0   23565.32\n2:               0    22.9814  5.110518  64.4931      29.0   66594.96\n3:               0    27.9464  5.188909  71.9061      27.5  158913.85\n4:               0    35.6938  5.315048  83.0256      17.0  395238.54\n5:               0    47.4766  5.524244  94.1451      10.0 1024224.96\n6:               0    59.6303  5.665452 114.1602      37.0 2764006.00",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#overall-performance-table",
    "href": "sessions/forecasting-nowcasting.html#overall-performance-table",
    "title": "Combining nowcasting and forecasting",
    "section": "Overall performance table",
    "text": "Overall performance table\nWe summarise the performance across all forecast horizons:\n\n# Create summary table showing overall performance\noverall_performance &lt;- scores |&gt;\n  summarise_scores(by = \"approach\") |&gt;\n  arrange(crps)\n\nknitr::kable(overall_performance, \n             caption = \"Overall forecast performance by approach\",\n             digits = 3)\n\n\nOverall forecast performance by approach\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\napproach\nbias\ndss\ncrps\noverprediction\nunderprediction\ndispersion\nlog_score\nmad\nae_median\nse_mean\n\n\n\n\nPipeline\n-0.671\n7.814\n21.318\n0.003\n16.054\n5.260\n5.299\n20.333\n33.643\n8.005450e+02\n\n\nJoint\n0.654\n8.372\n23.215\n14.151\n0.000\n9.064\n4.972\n36.641\n34.893\n3.014737e+03\n\n\nComplete data\n0.325\n20.203\n193.802\n6.913\n0.000\n186.889\n5.898\n137.564\n39.536\n2.580990e+08",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#performance-by-forecast-horizon",
    "href": "sessions/forecasting-nowcasting.html#performance-by-forecast-horizon",
    "title": "Combining nowcasting and forecasting",
    "section": "Performance by forecast horizon",
    "text": "Performance by forecast horizon\nWe examine how performance varies with forecast horizon:\n\n# Calculate performance by horizon\nhorizon_performance &lt;- scores |&gt;\n  summarise_scores(by = c(\"approach\", \"actual_day\"))\n\n# Plot showing how performance varies by forecast horizon\nggplot(horizon_performance) +\n  aes(x = actual_day, y = crps, color = approach) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 1) +\n  scale_y_log10() +\n  labs(\n    x = \"Forecast horizon (days)\",\n    y = \"CRPS (lower is better)\",\n    title = \"Forecast performance by horizon\",\n    color = \"Approach\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 5 minutes\n\n\n\nCompare the four approaches across the evaluation metrics\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNote these results are stochastic. Why is this?\nFrom this evaluation, we can see:\n\nComplete data approach: Performs much worse overall. Performance degrades rapidly with longer forecast horizons.\nJoint approach: Has comparable performance to the pipeline approach with some overprediction bias and higher dispersion, indicating greater uncertainty.\nPipeline approach: Has some systematic underprediction bias, performs best at early and late forecast horizons, and performs worse at medium forecast horizons\n\nTo really understand the performance difference we would need to evaluate across multiple forecast dates.",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#challenge",
    "href": "sessions/forecasting-nowcasting.html#challenge",
    "title": "Combining nowcasting and forecasting",
    "section": "Challenge",
    "text": "Challenge\n\nTry implementing the pipeline approach with uncertainty propagation by sampling multiple nowcast trajectories and fitting the renewal model to each. How does this compare computationally and in terms of forecast accuracy to the other approaches?\nExperiment with different forecast horizons (7 days, 21 days, 28 days). How does the relative performance of each approach change with longer forecast horizons?\nCompare the approaches when there are changes in transmission (e.g., at different cut off dates) near the forecast date. Which methods are more robust to such changes?",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#methods-in-practice",
    "href": "sessions/forecasting-nowcasting.html#methods-in-practice",
    "title": "Combining nowcasting and forecasting",
    "section": "Methods in practice",
    "text": "Methods in practice\nSeveral packages implement these concepts:\n\nEpiNow2: Pipeline nowcasting and forecasting\nepinowcast: Joint nowcasting and forecasting in an efficient framework\n\nResearch has demonstrated that joint modelling of data sources provides significant advantages over combining estimates from separate models (Lison et al. 2024) when evaluating multiple nowcast dates. In practice, implementations often include hierarchical models for multiple locations, time-varying delays, and ensemble approaches for robustness.",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/forecasting-nowcasting.html#references",
    "href": "sessions/forecasting-nowcasting.html#references",
    "title": "Combining nowcasting and forecasting",
    "section": "References",
    "text": "References\n\n\nLison, Adrian, Sam Abbott, Jana Huisman, and Tanja Stadler. 2024. “Generative Bayesian Modeling to Nowcast the Effective Reproduction Number from Line List Data with Missing Symptom Onset Dates.” PLOS Computational Biology 20 (4): e1012021. https://doi.org/10.1371/journal.pcbi.1012021.",
    "crumbs": [
      "Combining nowcasting and forecasting"
    ]
  },
  {
    "objectID": "sessions/introduction-and-course-overview.html",
    "href": "sessions/introduction-and-course-overview.html",
    "title": "Introduction and course overview",
    "section": "",
    "text": "From an epidemiological line list to informing decisions in real-time",
    "crumbs": [
      "Introduction and course overview"
    ]
  },
  {
    "objectID": "sessions/introduction-and-course-overview.html#footnotes",
    "href": "sessions/introduction-and-course-overview.html#footnotes",
    "title": "Introduction and course overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTime travel is messy stuff↩︎",
    "crumbs": [
      "Introduction and course overview"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html",
    "href": "sessions/nowcasting.html",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "So far we’ve explored the delays and biases of real-time infectious disease surveillance data, started to correct for these, and considered the underlying process that drives the evolution of epidemics (looking at the reproduction number and renewal equation). Next, we’ll focus on predicting new information about how infectious disease transmission is evolving in the present and future.\nWe know that we have incomplete information in the present because of delays in the observation process (reporting delays). The aim of nowcasting is to predict what an epidemiological time series will look like after all delayed reports are in, for which we need to account for the delays and biases we’ve already considered.\n\n\n\nIntroduction to nowcasting\n\n\n\n\nThis session aims to introduce the concept of nowcasting, and see how we can perform a nowcast if we know the underlying delay distribution.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/nowcasting.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#slides",
    "href": "sessions/nowcasting.html#slides",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "Introduction to nowcasting",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#objectives",
    "href": "sessions/nowcasting.html#objectives",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "This session aims to introduce the concept of nowcasting, and see how we can perform a nowcast if we know the underlying delay distribution.\n\n\n\n\n\n\nSetup\n\n\n\n\n\n\n\nThe source file of this session is located at sessions/nowcasting.qmd.\n\n\n\nIn this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.\n\n\n\n\n\nWe set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#source-file",
    "href": "sessions/nowcasting.html#source-file",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "The source file of this session is located at sessions/nowcasting.qmd.",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#libraries-used",
    "href": "sessions/nowcasting.html#libraries-used",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "In this session we will use the nfidd package to load a data set of infection times and access stan models and helper functions, the dplyr and tidyr packages for data wrangling, ggplot2 library for plotting, and the tidybayes package for extracting results of the inference.\n\nlibrary(\"nfidd\")\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"ggplot2\")\nlibrary(\"tidybayes\")\n\n\n\n\n\n\n\nTip\n\n\n\nThe best way to interact with the material is via the Visual Editor of RStudio.",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#initialisation",
    "href": "sessions/nowcasting.html#initialisation",
    "title": "Nowcasting concepts",
    "section": "",
    "text": "We set a random seed for reproducibility. Setting this ensures that you should get exactly the same results on your computer as we do. We also set an option that makes cmdstanr show line numbers when printing model code. This is not strictly necessary but will help us talk about the models.\n\nset.seed(123)\noptions(cmdstanr_print_line_numbers = TRUE)",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#the-simplest-possible-nowcasting-model",
    "href": "sessions/nowcasting.html#the-simplest-possible-nowcasting-model",
    "title": "Nowcasting concepts",
    "section": "The simplest possible nowcasting model",
    "text": "The simplest possible nowcasting model\nHere we assume that the delay distribution is known and that we can use it to nowcast the most recent data. In practice, the delay distribution is often not known and needs to be estimated from the data. We could do this using methods from the session on biases in delay distributions.\nIn the session on convolutions we used delay distributions convolved with the infection times to estimate the time series of symptom onsets. A simple way to nowcast is to use the same approach but using the cumulative distribution function of the delay distribution rather than the probability density function and only apply it to the most recent data as this is the only data that can be subject to change (due to delays in reporting).\nWe will build intuition for this as usual using simulation. First we define the proportion reported using a delay distribution up to 15 days, again using a lognormal distribution with meanlog 1 and sdlog 0.5:\n\nproportion_reported &lt;- plnorm(1:15, 1, 0.5)\nplot(proportion_reported)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plnorm function\n\n\n\nThe plnorm() function is related to the rlnorm() function we used earlier to simulate the individual level reporting delay, but instead it gives the cumulative distribution function rather than random samples. That is, it gives us the probability that a report is made on day 1 or earlier, day 2 or earlier, etc.\n\n\nWe can now use this delay distribution to nowcast the most recent data. Here we use the same simulation approach as in the renewal session (here we have created helper functions make_gen_time_pmf() and make_ip_pmf() to make it easier to re-use; we recommend to have a look at what these functions do), and apply the reporting_delay to the last 15 days of data.\n\ngen_time_pmf &lt;- make_gen_time_pmf()\nip_pmf &lt;- make_ip_pmf()\nonset_df &lt;- simulate_onsets(\n  make_daily_infections(infection_times), gen_time_pmf, ip_pmf\n)\nreported_onset_df &lt;- onset_df |&gt;\n  filter(day &lt; cutoff) |&gt;\n  mutate(proportion_reported = c(rep(1, n() - 15), rev(proportion_reported)),\n         reported_onsets = rpois(n(), onsets * proportion_reported)\n  )\ntail(reported_onset_df)\n\n# A tibble: 6 × 5\n    day onsets infections proportion_reported reported_onsets\n  &lt;dbl&gt;  &lt;int&gt;      &lt;int&gt;               &lt;dbl&gt;           &lt;int&gt;\n1    65     63         83              0.943               51\n2    66     64         75              0.889               58\n3    67     75         92              0.780               57\n4    68     78        113              0.578               34\n5    69     69        120              0.270               20\n6    70     84        115              0.0228               4\n\n\n\n\n\n\n\n\nTake 3 minutes\n\n\n\nSpend a few minutes trying to understand the code above. What is the proportion_reported? What is the reported_onsets?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe proportion_reported is the cumulative distribution function of the delay distribution. It gives the probability that a report is made on day 1 or earlier, day 2 or earlier, etc. Note that for days more that 15 days into the past, this is 1, meaning all onsets that occurred on that day have been reported.\nThe reported_onsets are the number of onsets that are reported on each day. This is calculated by multiplying the number of onsets by the proportion of onsets that are reported on each day. It has Poisson noise added to it to simulate the stochasticity in the reporting process.\n\n\nreported_onset_df |&gt;\n  ggplot(aes(x = day, y = reported_onsets)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\nWe can now fit our first nowcasting model. Here we assume exactly the same generative process as we used for simulation and model the number of onsets as independent draws from a normal distribution.\n\nmod &lt;- nfidd_cmdstan_model(\"simple-nowcast\")\nmod\n\n 1: functions {\n 2:   #include \"functions/condition_onsets_by_report.stan\"\n 3: }\n 4: \n 5: data {\n 6:   int n;                // number of days\n 7:   array[n] int obs;     // observed symptom onsets\n 8:   int report_max;       // max reporting delay\n 9:   array[report_max + 1] real report_cdf;\n10: }\n11: \n12: parameters {\n13:   array[n] real&lt;lower = 0&gt; onsets;\n14: }\n15: \n16: transformed parameters {\n17:   array[n] real reported_onsets = condition_onsets_by_report(onsets, report_cdf);\n18: }\n19: \n20: model {\n21:   onsets ~ normal(5, 20) T[0,];\n22:   // Likelihood\n23:   obs ~ poisson(reported_onsets);\n24: }\n\n\n\n\n\n\n\n\nTake 2 minutes\n\n\n\nFamiliarise yourself with the model above. What does it do?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nOn line 2 we define a new function condition_onsets_by_report.stan which takes the number of onsets and reports and the delay distribution as input and returns the nowcasted number of onsets. To find out more about what this function does, you can inspect the condition_onsets_by_report R function or navigate to condition_onsets_by_report.stan.\nOn line 17, this function is used to calculate the nowcasted number of onsets and this is then used in the likelihood.\nOn line 21, we define the generative process for the number of onsets. Here we assume that onsets are independent with each drawn from a normal distribution.\n\n\n\n\nOnce again we can generate estimates from this model:\n\ndata &lt;- list(\n  n = nrow(reported_onset_df) - 1,\n  obs = reported_onset_df$reported_onsets[-1],\n  report_max = length(proportion_reported) - 1,\n  report_cdf = proportion_reported \n)\nsimple_nowcast_fit &lt;- nfidd_sample(mod, data = data)\n\n\nsimple_nowcast_fit\n\n  variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n lp__      2342.51 2342.82 5.96 5.96 2332.23 2351.72 1.00      755     1093\n onsets[1]    0.97    0.72 0.93 0.71    0.05    2.90 1.00     2788     1196\n onsets[2]    1.00    0.69 1.00 0.71    0.05    3.00 1.00     2000      778\n onsets[3]    1.03    0.69 1.07 0.74    0.05    3.13 1.00     2592      955\n onsets[4]    1.02    0.74 1.00 0.75    0.05    2.98 1.00     2872     1173\n onsets[5]    1.07    0.72 1.08 0.75    0.06    3.23 1.00     3070     1060\n onsets[6]    1.02    0.68 1.06 0.71    0.05    3.06 1.00     2501     1072\n onsets[7]    2.04    1.72 1.48 1.22    0.34    4.89 1.00     3156     1356\n onsets[8]    1.99    1.66 1.35 1.18    0.39    4.59 1.00     3441     1312\n onsets[9]    1.02    0.70 1.02 0.75    0.04    3.04 1.00     2757      834\n\n # showing 10 of 139 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nWe can now plot onsets alongside those nowcasted by the model:\n\nnowcast_onsets &lt;- simple_nowcast_fit |&gt;\n  gather_draws(onsets[day]) |&gt;\n  ungroup() |&gt;\n  filter(.draw %in% sample(.draw, 100)) |&gt;\n  mutate(day = day + 1)\n\n\nggplot(nowcast_onsets, aes(x = day)) +\n  geom_line(mapping = aes(y = .value, group = .draw), alpha = 0.1) +\n  geom_col(data = reported_onset_df, mapping = aes(y = onsets), alpha = 0.6) +\n  geom_point(data = reported_onset_df, mapping = aes(y = reported_onsets))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe points in this plot represent the data available when the nowcast was made (and so are truncated) whilst the bars represent the finally reported data (a perfect nowcast would exactly reproduce these).\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs we found in the using delay distributions to model the data generating process of an epidemic session, this simple model struggles to recreate the true number of onsets. This is because it does not capture the generative process of the data (i.e. the transmission process and delays from infection to onset). In the next section we will see how we can use a model that does capture this generative process to improve our nowcasts.\n\n\n\n\n\n\n\n\nSimple multiplicative nowcasting methods\n\n\n\nBefore moving to more complex models, it’s worth noting that there is an even simpler nowcasting method that simply divides currently observed counts by the cumulative distribution function (CDF) of the delay distribution. This approach is often called “multiplicative” or “scaling” nowcasting.\nWhile this method appears simple, it still embodies a model with specific assumptions that can be difficult to define precisely. Key challenges include:\n\nThe method struggles with noisy data, particularly when counts are low\nSparse data (e.g., zero counts) can lead to unstable estimates\nGenerating uncertainty\nThe implicit assumptions about the data generating process are not always clear\n\nThe baselinenowcast R package provides a set of tools for these types of methods, including approaches to handle both noise and sparsity in the data. These baseline methods can serve as useful benchmarks when developing more complex nowcasting models.",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#adding-in-a-geometric-random-walk-to-the-nowcasting-model",
    "href": "sessions/nowcasting.html#adding-in-a-geometric-random-walk-to-the-nowcasting-model",
    "title": "Nowcasting concepts",
    "section": "Adding in a geometric random walk to the nowcasting model",
    "text": "Adding in a geometric random walk to the nowcasting model\nAs we saw in the session on the renewal equation, a geometric random walk is a simple way to model multiplicative growth. Adding this into our simple nowcasting model may help us to better capture the generative process of the data and so produce a better nowcast.\nWe first load the model\n\nrw_mod &lt;- nfidd_cmdstan_model(\"simple-nowcast-rw\")\nrw_mod\n\n 1: functions {\n 2:   #include \"functions/geometric_random_walk.stan\"\n 3:   #include \"functions/condition_onsets_by_report.stan\"\n 4: }\n 5: \n 6: data {\n 7:   int n;                // number of days\n 8:   array[n] int obs;     // observed symptom onsets\n 9:   int report_max;       // max reporting delay\n10:   array[report_max + 1] real report_cdf;\n11: }\n12: \n13: parameters {\n14:   real&lt;lower=0&gt; init_onsets;\n15:   array[n-1] real rw_noise;\n16:   real&lt;lower=0&gt; rw_sd;\n17: }\n18: \n19: transformed parameters {\n20:   array[n] real onsets = geometric_random_walk(init_onsets, rw_noise, rw_sd);\n21:   array[n] real reported_onsets = condition_onsets_by_report(onsets, report_cdf);\n22: }\n23: \n24: model {\n25:   init_onsets ~ lognormal(0, 1) T[0,];\n26:   rw_noise ~ std_normal();\n27:   rw_sd ~ normal(0, 5) T[0,];\n28:   //Likelihood\n29:   obs ~ poisson(reported_onsets);\n30: }\n\n\nand then fit it\n\nrw_nowcast_fit &lt;- nfidd_sample(rw_mod, data = data)\n\n\nrw_nowcast_fit\n\n    variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n lp__        2186.23 2186.49 8.09 8.20 2172.81 2199.02 1.01      459      959\n init_onsets    0.43    0.37 0.23 0.20    0.14    0.89 1.00     2009     1493\n rw_noise[1]   -0.20   -0.20 0.94 0.96   -1.75    1.34 1.00     3064     1645\n rw_noise[2]   -0.06   -0.08 0.95 0.92   -1.60    1.58 1.00     2400     1379\n rw_noise[3]    0.04    0.03 0.95 0.94   -1.53    1.61 1.01     2532     1353\n rw_noise[4]    0.20    0.23 0.98 0.99   -1.43    1.77 1.00     2635     1282\n rw_noise[5]    0.32    0.32 0.96 0.96   -1.20    1.95 1.00     3536     1570\n rw_noise[6]    0.44    0.44 1.02 1.06   -1.19    2.10 1.01     2876     1485\n rw_noise[7]    0.32    0.33 0.92 0.91   -1.17    1.82 1.00     3044     1246\n rw_noise[8]    0.12    0.09 0.96 0.94   -1.45    1.69 1.00     3174     1156\n\n # showing 10 of 209 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nAgain we can extract the nowcasted onsets and plot them alongside the observed data:\n\nrw_nowcast_onsets &lt;- rw_nowcast_fit |&gt;\n  gather_draws(onsets[day]) |&gt;\n  ungroup() |&gt;\n  filter(.draw %in% sample(.draw, 100)) |&gt; ## sample 100 iterations randomly\n  mutate(day = day + 1)\n\n\nggplot(rw_nowcast_onsets, aes(x = day)) +\n  geom_col(data = reported_onset_df, mapping = aes(y = onsets), alpha = 0.6) +\n  geom_line(mapping = aes(y = .value, group = .draw), alpha = 0.1) +\n  geom_point(data = reported_onset_df, mapping = aes(y = reported_onsets))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 2 minute\n\n\n\nWhat do you think of the nowcast now? Does it look better than the previous one?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe nowcast better matches the ultimately observed data. The geometric random walk allows the model to capture the multiplicative growth in the data and so better capture that current indidence is related to past incidence.\nThis should be particularly true when the data is more truncated (i.e nearer to the date of the nowcast) as the geometric random walk allows the model to extrapolate incidence based on previous incidence rather than relying on the prior distribution as the simpler model did.\nHowever, the model is still quite simple and so may struggle to capture more complex patterns in the data. In particular, the prior model for the geometric random walk assumes that onsets are the same as the previous day with statistical noise. This may not be a good assumption in a rapidly changing epidemic (where the reproduction number is not near 1).",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#what-happens-if-we-get-the-delay-distribution-wrong",
    "href": "sessions/nowcasting.html#what-happens-if-we-get-the-delay-distribution-wrong",
    "title": "Nowcasting concepts",
    "section": "What happens if we get the delay distribution wrong?",
    "text": "What happens if we get the delay distribution wrong?\n\n\n\n\n\n\nOptional section\n\n\n\nThis section can be skipped if time is limited. It demonstrates the importance of accurate delay distribution estimation.\n\n\nIn practice, we often do not know the delay distribution and so need to estimate it using the data at hand. In the session on biases in delay distributions we saw how we could do this using individual-level records. We will now look at what happens if we get the delay distribution wrong.\nWe use the same data as before but now assume that the delay distribution is a gamma distribution with shape 2 and rate 3. This is a very different distribution to the lognormal distribution we used to simulate the data.\n\nwrong_proportion_reported &lt;- pgamma(1:15, 2, 3)\nplot(wrong_proportion_reported)\n\n\n\n\n\n\n\n\nWe first need to update the data to use this new delay distribution:\n\nwrong_delay_data &lt;- data\nwrong_delay_data$report_cdf &lt;- wrong_proportion_reported\n\nWe now fit the nowcasting model with the wrong delay distribution:\n\ngamma_nowcast_fit &lt;- nfidd_sample(rw_mod, data = wrong_delay_data)\n\n\ngamma_nowcast_fit\n\n    variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n lp__        2188.54 2188.85 8.24 7.96 2173.87 2201.57 1.01      492      970\n init_onsets    0.40    0.34 0.24 0.19    0.12    0.87 1.00     1942     1327\n rw_noise[1]   -0.26   -0.27 0.92 0.94   -1.79    1.31 1.00     2257     1407\n rw_noise[2]   -0.14   -0.15 0.96 0.94   -1.67    1.43 1.00     2124     1336\n rw_noise[3]    0.03    0.02 0.94 0.95   -1.48    1.60 1.00     2291     1567\n rw_noise[4]    0.14    0.14 0.92 0.93   -1.37    1.68 1.00     1693     1611\n rw_noise[5]    0.33    0.31 0.93 0.92   -1.19    1.88 1.00     2906     1407\n rw_noise[6]    0.47    0.47 0.87 0.90   -0.95    1.89 1.00     2238     1287\n rw_noise[7]    0.22    0.22 0.93 0.93   -1.30    1.81 1.00     2767     1415\n rw_noise[8]    0.07    0.05 0.95 0.95   -1.47    1.62 1.00     2023     1313\n\n # showing 10 of 209 rows (change via 'max_rows' argument or 'cmdstanr_max_rows' option)\n\n\nAgain we can extract the nowcast of symptom onsets and plot it alongside the observed data:\n\ngamma_nowcast_onsets &lt;- gamma_nowcast_fit |&gt;\n  gather_draws(onsets[day]) |&gt;\n  ungroup() |&gt;\n  filter(.draw %in% sample(.draw, 100)) |&gt;\n  mutate(day = day + 1)\n\n\nggplot(gamma_nowcast_onsets, aes(x = day)) +\n  geom_col(data = reported_onset_df, mapping = aes(y = onsets), alpha = 0.6) +\n  geom_line(mapping = aes(y = .value, group = .draw), alpha = 0.1) +\n  geom_point(data = reported_onset_df, mapping = aes(y = reported_onsets))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTake 2 minute\n\n\n\nWhat do you think of the nowcast now? How would you know you had the wrong delay if you didn’t have the true delay distribution?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe nowcast now looks very different to the observed data. This is because the model is using the wrong delay distribution to nowcast the data.\nIf you didn’t have the true delay distribution you would not know that you had the wrong delay distribution. This is why it is important to estimate the delay distribution from the data.\nIn practice, you would likely compare the nowcast to the observed data and if they did not match you would consider whether the delay distribution was the cause.",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#methods-in-practice",
    "href": "sessions/nowcasting.html#methods-in-practice",
    "title": "Nowcasting concepts",
    "section": "Methods in practice",
    "text": "Methods in practice\n\nWolffram et al., Collaborative nowcasting of COVID-19 hospitalization incidences in Germany compares the performance of a range of methods that were used in a nowcasting hub and investigates what might explain performance differences.\nThe baselinenowcast R package provides implementations of simple multiplicative nowcasting methods that can serve as baselines for comparison with more complex approaches.",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/nowcasting.html#references",
    "href": "sessions/nowcasting.html#references",
    "title": "Nowcasting concepts",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Nowcasting concepts"
    ]
  },
  {
    "objectID": "sessions/slides/forecast-evaluation.html#your-turn",
    "href": "sessions/slides/forecast-evaluation.html#your-turn",
    "title": "Forecast evaluation",
    "section": " Your Turn",
    "text": "Your Turn\n\nBuild and visualize forecasts from multiple simple models.\nEvaluate the forecasts using proper scoring rules.\nDesign and run an experiment to compare forecasts made from multiple models.\nEvaluate forecasts across forecast dates, horizons, and other dimensions."
  },
  {
    "objectID": "sessions/slides/improving-forecasting-models.html#your-turn",
    "href": "sessions/slides/improving-forecasting-models.html#your-turn",
    "title": "Improving Forecasting Models",
    "section": " Your Turn",
    "text": "Your Turn\n\nExplore the influenza dataset, focusing on assessing observed patterns qualitatively and quantitatively.\nExplore different ways of improving modeling forecasts by incorporating patterns observed in data into our models."
  },
  {
    "objectID": "sessions/slides/introduction-to-ensembles.html#your-turn",
    "href": "sessions/slides/introduction-to-ensembles.html#your-turn",
    "title": "Multi-model ensembles",
    "section": " Your Turn",
    "text": "Your Turn\n\nCreate unweighted and weighted ensembles using forecasts from multiple models.\nEvaluate the forecasts from ensembles compared to their constituent models."
  },
  {
    "objectID": "sessions/slides/introduction-to-forecasting.html#your-turn",
    "href": "sessions/slides/introduction-to-forecasting.html#your-turn",
    "title": "Introduction to forecasting",
    "section": " Your Turn",
    "text": "Your Turn\n\nLearn about and explore seasonal influenza-like illness data from US CDC.\nBuild a simple forecast model and understand how it is represented in R."
  },
  {
    "objectID": "sessions/slides/introduction-to-nowcasting.html#your-turn",
    "href": "sessions/slides/introduction-to-nowcasting.html#your-turn",
    "title": "Introduction to nowcasting",
    "section": " Your Turn",
    "text": "Your Turn\n\nPerform nowcast with a known reporting delay distribution\nPerform a nowcast using a more realistic data generating process\nExplore the impact of getting the delay distribution wrong"
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#convolution-session",
    "href": "sessions/slides/introduction-to-reproduction-number.html#convolution-session",
    "title": "Introduction to the time-varying reproduction number",
    "section": "Convolution session",
    "text": "Convolution session\n\nfunctions {\n  #include \"functions/convolve_with_delay.stan\"\n}\n\ndata {\n  int n;            // number of time days\n  array[n] int obs; // observed onsets\n  int&lt;lower = 1&gt; ip_max; // max incubation period\n  // probability mass function of incubation period distribution (first index zero)\n  array[ip_max + 1] real ip_pmf;\n}\n\nparameters {\n  array[n] real&lt;lower = 0&gt; infections;\n}\n\ntransformed parameters {\n  array[n] real onsets = convolve_with_delay(infections, ip_pmf);\n}\n\nmodel {\n  // priors\n  infections ~ normal(0, 10) T[0, ];\n  obs ~ poisson(onsets);\n}\n\n\nPrior for infections at time \\(t\\) is independent from infections at all other time points. Is this reasonable?"
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#what-if-we-convolve-infections-with-infections",
    "href": "sessions/slides/introduction-to-reproduction-number.html#what-if-we-convolve-infections-with-infections",
    "title": "Introduction to the time-varying reproduction number",
    "section": "What if we convolve infections with infections?",
    "text": "What if we convolve infections with infections?\nWe can extend the convolution concept from the last session:\n\nPrevious session: infections → symptoms (via incubation period)\nThis session: infections → infections (via generation time)"
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#generation-time-infection-to-infection",
    "href": "sessions/slides/introduction-to-reproduction-number.html#generation-time-infection-to-infection",
    "title": "Introduction to the time-varying reproduction number",
    "section": "Generation time: infection to infection",
    "text": "Generation time: infection to infection\nHere rather than the time from infection (person A) to symptoms (person B, infected by A) we have the time from infection (person A) to infection (person B, infected by A).\nThis is known as the generation time distribution (\\(g(t)\\))."
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#generation-time-notation",
    "href": "sessions/slides/introduction-to-reproduction-number.html#generation-time-notation",
    "title": "Introduction to the time-varying reproduction number",
    "section": "Generation time: notation",
    "text": "Generation time: notation\nWe can write the convolution as:\n\\[\nI_t = \\mathrm{scaling} \\times \\sum_{t' &lt; t} I_t' g(t - t')\n\\]\nHowever, unlike the infection to symptoms case here we don’t assume that the two time series have the same magnitude so need to introduce a scaling.\n\nWhat is this scaling?"
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#the-renewal-equation-as-a-convolution",
    "href": "sessions/slides/introduction-to-reproduction-number.html#the-renewal-equation-as-a-convolution",
    "title": "Introduction to the time-varying reproduction number",
    "section": "The renewal equation as a convolution",
    "text": "The renewal equation as a convolution"
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#the-scaling-factor-reproduction-number",
    "href": "sessions/slides/introduction-to-reproduction-number.html#the-scaling-factor-reproduction-number",
    "title": "Introduction to the time-varying reproduction number",
    "section": "The scaling factor: reproduction number",
    "text": "The scaling factor: reproduction number\nLet’s assume we have \\(I_0\\) infections at time 0, and the scaling doesn’t change in time.\nHow many people will they go on to infect?\n\n\\[\nI = \\mathrm{scaling} \\times \\sum_{t=0}^\\infty I_0 g(t) = \\mathrm{scaling} * I_0\n\\]\nThe scaling can be interpreted as the reproduction number \\(R_0\\) (assuming a susceptible population)."
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#the-renewal-equation",
    "href": "sessions/slides/introduction-to-reproduction-number.html#the-renewal-equation",
    "title": "Introduction to the time-varying reproduction number",
    "section": "The renewal equation",
    "text": "The renewal equation\nIf \\(R_t\\) can change over time, it can be interpreted as the (“instantaneous”) reproduction number:\n\\[\nI_t = R_t \\times \\sum_{t' &lt; t} I_t' g(t - t')\n\\]\nWe can estimate \\(R_t\\) from a time series of infections using the renewal equation."
  },
  {
    "objectID": "sessions/slides/introduction-to-reproduction-number.html#your-turn",
    "href": "sessions/slides/introduction-to-reproduction-number.html#your-turn",
    "title": "Introduction to the time-varying reproduction number",
    "section": " Your Turn",
    "text": "Your Turn\n\nSimulate infections using the renewal equation\nEstimate reproduction numbers using a time series of infections\nCombine with delay distributions to jointly infer infections and R from a time series of outcomes"
  },
  {
    "objectID": "sessions/slides/introduction-to-the-spectrum-of-forecasting-models.html#different-types-of-models",
    "href": "sessions/slides/introduction-to-the-spectrum-of-forecasting-models.html#different-types-of-models",
    "title": "Introduction to the spectrum of forecasting models",
    "section": "Different types of models",
    "text": "Different types of models\n\n\n\n\nWe can classify models by the level of mechanism they include\nAll of the model types we will introduce in the next few slides have been used for COVID-19 forecasting (the US and/or European COVID-19 forecast hub)\n\nNOTE: level of mechanism \\(\\neq\\) model complexity\n\n\nCramer et al., Scientific Data, 2022"
  },
  {
    "objectID": "sessions/slides/introduction-to-the-spectrum-of-forecasting-models.html#your-turn",
    "href": "sessions/slides/introduction-to-the-spectrum-of-forecasting-models.html#your-turn",
    "title": "Introduction to the spectrum of forecasting models",
    "section": " Your Turn",
    "text": "Your Turn\n\nReview the performance of the random walk model\nMotivate a mechanism to order to address some of the issues\nMotivate a statistical approach aiming to address the same issues\nCompare the performance of these models for a single forecast\nEvaluate many forecast from these models and compare their performance"
  }
]